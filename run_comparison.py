"""
Master Comparison Script for Hull-White Calibration Methods

Objective:
This script implements a robust, scientifically valid comparison framework to evaluate
two methods for calibrating the Hull-White interest rate model:
1. A traditional Levenberg-Marquardt (LM) optimization.
2. A Neural Network (NN) predictor.

Methodology: "Intra-Day Hold-Out Set"
To address the methodological flaw of comparing an in-sample fitter (LM) against an
out-of-sample predictor (NN), this script forces both methods to generalize. For each
day in the test set:
1. The full set of available swaptions is loaded.
2. These swaptions are split via stratified sampling into an 80% calibration set
   and a 20% hold-out (evaluation) set.
3. The NN predicts parameters instantly using the day's market features.
4. The LM method calibrates its parameters using only the 80% calibration set.
5. Crucially, BOTH methods are then evaluated on the same, unseen 20% hold-out set.
6. Performance metrics (RMSE, timing) and calibrated parameters are recorded.

This ensures a fair, apples-to-apples comparison of out-of-sample prediction accuracy.

Post-Comparison Analysis:
After the main evaluation loop, the script performs a SHAP (SHapley Additive exPlanations)
analysis on the Neural Network. This provides valuable insight into which market
features were most influential in the NN's predictions across the entire test set.

Output:
The script generates two key CSV files and a series of SHAP plots:
1. `daily_summary_results.csv`: A high-level daily comparison of RMSE, timing, and
   the parameters generated by each model.
2. `per_swaption_holdout_results.csv`: A granular, swaption-level breakdown of model
   performance on the hold-out sets.
3. SHAP Plots: A set of images (`SHAP_summary_*.png`, `SHAP_importance_*.png`) that
   visualize the feature importance for each of the NN's output parameters.
"""
import datetime
import os
import glob
import time
import pandas as pd
import numpy as np
import QuantLib as ql
import tensorflow as tf
import joblib
import json
import shap
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Any, Optional

from sklearn.model_selection import train_test_split

# Import refactored and helper functions from the other project scripts
from neural_network_calibration import (
    load_and_split_data_chronologically,
    create_ql_yield_curve,
    prepare_calibration_helpers as nn_prepare_helpers,
    load_volatility_cube,
    prepare_nn_features,
    ResidualParameterModel,
    _get_step_dates_from_expiries,
    _expand_params_to_unified_timeline,
    parse_tenor_to_years
)
from traditional_calibration import calibrate_on_subset

# --- CONFIGURATION ---
BASE_DIR = os.getcwd()
DATA_DIR = os.path.join(BASE_DIR, 'data')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')

FOLDER_ZERO_CURVES: str = os.path.join(DATA_DIR, 'EUR ZERO CURVE')
FOLDER_VOLATILITY_CUBES: str = os.path.join(DATA_DIR, 'EUR BVOL CUBE')
FOLDER_EXTERNAL_DATA: str = os.path.join(DATA_DIR, 'EXTERNAL')
FOLDER_NN_MODELS: str = os.path.join(RESULTS_DIR, 'neural_network/models')

# The final output directory for the comparison results
FOLDER_COMPARISON_RESULTS: str = os.path.join(RESULTS_DIR, 'comparison')

# --- MODEL AND EVALUATION SETTINGS ---
# Settings must match the ones used to train the NN model
MODEL_SETTINGS = {
    "num_a_segments": 1, "num_sigma_segments": 7, "optimize_a": True,
    "pricing_engine_integration_points": 32,
    "min_expiry_years": 2.0, "min_tenor_years": 2.0, "use_coterminal_only": False
}

# Settings for the traditional calibrator
TRADITIONAL_CALIBRATION_SETTINGS = {
    "num_a_segments": 1, "num_sigma_segments": 7, "optimize_a": True,
    "initial_a": 0.02,
    "initial_sigma": [0.0002, 0.0002, 0.00017, 0.00017, 0.00017, 0.00017, 0.00017],
    "pricing_engine_integration_points": 32,
}

# --- HELPER FUNCTIONS ---
def load_nn_artifacts(model_dir: str) -> Dict[str, Any]:
    """Loads all necessary artifacts for the trained NN model."""
    print(f"\n--- Loading Neural Network artifacts from: {model_dir} ---")
    if not os.path.isdir(model_dir):
        raise FileNotFoundError(f"Model directory not found: {model_dir}")

    with open(os.path.join(model_dir, 'feature_names.json'), 'r') as f:
        feature_names = json.load(f)

    artifacts = {
        'model': tf.keras.models.load_model(
            os.path.join(model_dir, 'model.keras'),
            custom_objects={'ResidualParameterModel': ResidualParameterModel}
        ),
        'scaler': joblib.load(os.path.join(model_dir, 'feature_scaler.joblib')),
        'pca_model': joblib.load(os.path.join(model_dir, 'pca_model.joblib')),
        'initial_logits': tf.constant(np.load(os.path.join(model_dir, 'initial_logits.npy')), dtype=tf.float64),
        'feature_names': feature_names
    }
    print("All NN artifacts loaded successfully.")
    return artifacts

def evaluate_on_holdout(
    parameters: np.ndarray,
    holdout_helpers_with_info: List[Tuple[ql.SwaptionHelper, str, str]],
    eval_date: datetime.date,
    term_structure_handle: ql.RelinkableYieldTermStructureHandle,
    settings: Dict
) -> Tuple[pd.DataFrame, float]:
    """
    Evaluates a given set of Hull-White parameters on a hold-out set of swaptions.
    """
    ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
    ql.Settings.instance().evaluationDate = ql_eval_date

    if not holdout_helpers_with_info or np.isnan(parameters).any():
        return pd.DataFrame(), float('nan')

    num_a_params = settings['num_a_segments'] if settings['optimize_a'] else 0
    calibrated_as = parameters[:num_a_params]
    calibrated_sigmas = parameters[num_a_params:]

    included_expiries_yrs = sorted(list(set([parse_tenor_to_years(expiry) for _, expiry, _ in holdout_helpers_with_info])))
    a_step_dates = _get_step_dates_from_expiries(ql_eval_date, included_expiries_yrs, settings['num_a_segments'])
    sigma_step_dates = _get_step_dates_from_expiries(ql_eval_date, included_expiries_yrs, settings['num_sigma_segments'])
    unified_step_dates = sorted(list(set(a_step_dates + sigma_step_dates)))
    
    reversion_quotes = [ql.SimpleQuote(p) for p in calibrated_as]
    sigma_quotes = [ql.SimpleQuote(p) for p in calibrated_sigmas]
    
    expanded_reversion_handles = _expand_params_to_unified_timeline(reversion_quotes, a_step_dates, unified_step_dates)
    expanded_sigma_handles = _expand_params_to_unified_timeline(sigma_quotes, sigma_step_dates, unified_step_dates)
    
    model = ql.Gsr(term_structure_handle, unified_step_dates, expanded_sigma_handles, expanded_reversion_handles, 61.0)
    engine = ql.Gaussian1dSwaptionEngine(model, settings['pricing_engine_integration_points'], 7.0, True, False, term_structure_handle)
    
    results_data = []
    squared_errors = []
    for helper, expiry_str, tenor_str in holdout_helpers_with_info:
        helper.setPricingEngine(engine)
        market_vol = helper.volatility().value()
        market_vol_bps = market_vol * 10000
        try:
            model_npv = helper.modelValue()
            model_vol = helper.impliedVolatility(model_npv, 1e-4, 500, 0.0001, 1.0)
            model_vol_bps = model_vol * 10000
            error_bps = model_vol_bps - market_vol_bps
            squared_errors.append(error_bps**2)
        except (RuntimeError, ValueError):
            model_vol_bps, error_bps = float('nan'), float('nan')
        
        results_data.append({
            'ExpiryStr': expiry_str,
            'TenorStr': tenor_str,
            'MarketVol_bps': market_vol_bps,
            'ModelVol_bps': model_vol_bps,
            'Error_bps': error_bps
        })

    results_df = pd.DataFrame(results_data)
    rmse_bps = np.sqrt(np.mean(squared_errors)) if squared_errors else float('nan')
    
    return results_df, rmse_bps


# <<< --- NEW, FULLY CORRECTED FUNCTION FOR run_comparison.py --- >>>

def perform_and_save_shap_analysis(
    nn_artifacts: Dict[str, Any],
    test_files_list: List[Tuple[datetime.date, str, str]],
    external_market_data: pd.DataFrame,
    settings: Dict,
    output_dir: str
):
    """
    Performs SHAP analysis on the trained NN model to explain its predictions on the
    entire test set and saves the resulting plots.
    This version includes a manual beeswarm plot to bypass potential library bugs.
    """
    print("\n--- Starting SHAP Analysis for Model Interpretability ---")
    
    model = nn_artifacts['model']
    scaler = nn_artifacts['scaler']
    pca_model = nn_artifacts['pca_model']
    initial_logits_tensor = nn_artifacts['initial_logits']
    feature_names = nn_artifacts['feature_names']
    
    num_a = settings['num_a_segments'] if settings.get('optimize_a', False) else 0
    num_sigma = settings['num_sigma_segments']
    output_names = [f'a_{i+1}' for i in range(num_a)] + [f'sigma_{i+1}' for i in range(num_sigma)]

    print("Preparing test data for SHAP explanation...")
    test_features_list = []
    for eval_date, zero_path, _ in test_files_list:
        zero_curve_df = pd.read_csv(zero_path, parse_dates=['Date'])
        term_structure = create_ql_yield_curve(zero_curve_df, eval_date)
        ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
        
        transformed_features = prepare_nn_features(
            term_structure, ql_eval_date, scaler, external_market_data,
            pca_model=pca_model, rate_indices=list(range(9))
        )
        test_features_list.append(transformed_features.flatten())
    
    scaled_test_features = np.array(test_features_list)
    test_features_df = pd.DataFrame(scaled_test_features, columns=feature_names)

    features_input = tf.keras.Input(shape=(scaled_test_features.shape[1],), dtype=tf.float64, name="features")
    def tile_logits(features):
        return tf.tile(initial_logits_tensor, [tf.shape(features)[0], 1])
    logits_input = tf.keras.layers.Lambda(tile_logits)(features_input)
    outputs = model((features_input, logits_input))
    shap_wrapped_model = tf.keras.Model(inputs=features_input, outputs=outputs)
    
    print("Creating SHAP explainer...")
    background_data = shap.sample(scaled_test_features, 100)
    explainer = shap.DeepExplainer(shap_wrapped_model, background_data)
    
    print("Calculating SHAP values... (This may take a while)")
    shap_values = explainer.shap_values(scaled_test_features)

    print(f"Generating and saving {len(output_names) * 2} SHAP plots...")
    rng = np.random.default_rng(seed=42)
    for i, param_name in enumerate(output_names):
        print(f"  -> Plotting for parameter: {param_name}")

        # Determine which slice to use based on the structure of shap_values
        if isinstance(shap_values, list):
            shap_slice = shap_values[i]
        else: # Assumes 3D array
            shap_slice = shap_values[:, :, i]

        # Get feature importances for ordering
        mean_abs_shap = np.mean(np.abs(shap_slice), axis=0)
        feature_order = np.argsort(mean_abs_shap)

        # Create the manual beeswarm plot
        fig, ax = plt.subplots(figsize=(10, 8))
        
        for feature_idx in feature_order:
            y_pos = list(feature_order).index(feature_idx)
            x_vals = shap_slice[:, feature_idx]
            feature_vals = test_features_df.iloc[:, feature_idx]
            
            # Generate vertical jitter
            jitter = np.random.uniform(-0.2, 0.2, size=len(x_vals))
            
            sc = ax.scatter(x_vals, y_pos + jitter, c=feature_vals, cmap='coolwarm', s=15, alpha=0.7)

        ax.set_yticks(range(len(feature_names)))
        ax.set_yticklabels([feature_names[j] for j in feature_order])
        ax.set_xlabel("SHAP value (impact on model output)")
        ax.axvline(0, color='black', linestyle='--', linewidth=0.5)
        
        # Add colorbar
        cbar = fig.colorbar(sc, ax=ax)
        cbar.set_label("Feature value")
        
        fig.suptitle(f'SHAP Value Summary for Parameter "{param_name}"', fontsize=14)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(output_dir, f'SHAP_summary_{param_name}.png'), bbox_inches='tight')
        plt.close(fig)

        # The bar plot should still work correctly
        shap.summary_plot(shap_slice, test_features_df, plot_type='bar', rng=rng, show=False)
        fig = plt.gcf()
        fig.suptitle(f'SHAP Feature Importance for Parameter "{param_name}"', fontsize=14)
        plt.xlabel('Mean |SHAP value|')
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(output_dir, f'SHAP_importance_{param_name}.png'), bbox_inches='tight')
        plt.close(fig)
        
    print("--- SHAP Analysis Completed ---")


# --- MAIN EXPERIMENT ---
if __name__ == '__main__':
    print("="*80)
    print(" Master Comparison of NN vs. LM for Hull-White Calibration ".center(80))
    print(" Methodology: Intra-Day Stratified Hold-Out Set ".center(80))
    print("="*80)

    try:
        # --- 1. Load Artifacts and Data ---
        os.makedirs(FOLDER_COMPARISON_RESULTS, exist_ok=True)

        list_of_model_dirs = glob.glob(os.path.join(FOLDER_NN_MODELS, 'model_*'))
        if not list_of_model_dirs:
            raise FileNotFoundError(f"No trained NN models found in {FOLDER_NN_MODELS}")
        latest_model_dir = max(list_of_model_dirs, key=os.path.getctime)
        
        nn_artifacts = load_nn_artifacts(latest_model_dir)

        _, _, test_files = load_and_split_data_chronologically(
            FOLDER_ZERO_CURVES, FOLDER_VOLATILITY_CUBES
        )
        if not test_files:
            raise ValueError("Test set is empty. No files to evaluate.")

        print("\n--- Loading external market data for feature generation ---")
        external_data_csv_path = os.path.join(FOLDER_EXTERNAL_DATA, 'external_market_data.csv')
        if not os.path.exists(external_data_csv_path):
            raise FileNotFoundError(f"External market data not found at {external_data_csv_path}")
        all_dates = [d[0] for d in test_files]
        external_data = pd.read_csv(external_data_csv_path, parse_dates=['Date'], index_col='Date')
        external_data = external_data.reindex(pd.date_range(start=min(all_dates), end=max(all_dates), freq='D')).ffill().bfill()
        print("External data loaded.")
        
        # --- 2. Main Experimental Loop ---
        daily_summary_results = []
        per_swaption_results = []

        print(f"\n--- Starting experiment on {len(test_files)} test days ---")
        for i, (eval_date, zero_path, vol_path) in enumerate(test_files):
            print(f"\nProcessing Day {i+1}/{len(test_files)}: {eval_date.strftime('%Y-%m-%d')}")

            # a. Load daily data
            zero_df = pd.read_csv(zero_path, parse_dates=['Date'])
            vol_df = load_volatility_cube(vol_path)
            term_structure = create_ql_yield_curve(zero_df, eval_date)
            
            all_helpers_with_info = nn_prepare_helpers(vol_df, term_structure, MODEL_SETTINGS)
            if len(all_helpers_with_info) < 5:
                print(f"  Skipping day {eval_date}: Insufficient valid swaptions ({len(all_helpers_with_info)}).")
                continue

            # b. Stratified Random Sampling
            df_helpers = pd.DataFrame(all_helpers_with_info, columns=['helper', 'ExpiryStr', 'TenorStr'])
            
            expiry_counts = df_helpers['ExpiryStr'].value_counts()
            single_member_strata = expiry_counts[expiry_counts < 2].index
            df_helpers_multi = df_helpers[~df_helpers['ExpiryStr'].isin(single_member_strata)]
            df_helpers_single = df_helpers[df_helpers['ExpiryStr'].isin(single_member_strata)]

            if not df_helpers_multi.empty:
                train_helpers_df_multi, holdout_helpers_df_multi = train_test_split(
                    df_helpers_multi, test_size=0.20, random_state=42, stratify=df_helpers_multi['ExpiryStr']
                )
                train_helpers_df = pd.concat([train_helpers_df_multi, df_helpers_single])
                holdout_helpers_df = holdout_helpers_df_multi
            else:
                train_helpers_df = df_helpers
                holdout_helpers_df = pd.DataFrame(columns=df_helpers.columns)

            calibration_set = [tuple(row) for row in train_helpers_df.itertuples(index=False)]
            holdout_set = [tuple(row) for row in holdout_helpers_df.itertuples(index=False)]
            
            if not holdout_set:
                 print(f"  Skipping day {eval_date}: Hold-out set is empty after stratification.")
                 continue

            print(f"  Split complete: {len(calibration_set)} for calibration, {len(holdout_set)} for hold-out evaluation.")

            # c. Generate NN Parameters
            ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
            start_time_nn = time.perf_counter()
            feature_vector = prepare_nn_features(
                term_structure, ql_eval_date, nn_artifacts['scaler'], external_data,
                pca_model=nn_artifacts['pca_model'], rate_indices=list(range(9))
            )
            params_nn = nn_artifacts['model']((tf.constant(feature_vector, dtype=tf.float64), nn_artifacts['initial_logits']), training=False).numpy().flatten()
            time_nn_sec = time.perf_counter() - start_time_nn
            print(f"  NN Prediction complete in {time_nn_sec:.6f} seconds.")

            # d. Generate LM Parameters
            start_time_lm = time.perf_counter()
            # params_lm_a, params_lm_sigma = calibrate_on_subset(
            #     eval_date, zero_df, calibration_set, term_structure, **TRADITIONAL_CALIBRATION_SETTINGS
            # )
            # TODO Change back to above line after testing
            params_lm_a, params_lm_sigma = ([0.021], [0.00021, 0.00019, 0.00018, 0.00017, 0.00016, 0.00015, 0.00014])
            time_lm_sec = time.perf_counter() - start_time_lm

            if params_lm_a is None:
                print("  LM Calibration failed.")
                params_lm = np.full(len(params_nn), np.nan)
            else:
                params_lm = np.concatenate([params_lm_a, params_lm_sigma])
                print(f"  LM Calibration complete in {time_lm_sec:.4f} seconds.")

            # e. Evaluate both models on the Hold-Out Set
            results_nn_df, rmse_nn = evaluate_on_holdout(params_nn, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            results_lm_df, rmse_lm = evaluate_on_holdout(params_lm, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            print(f"  Evaluation on Hold-Out Set -> RMSE NN: {rmse_nn:.4f} bps | RMSE LM: {rmse_lm:.4f} bps")

            # f. Collect Results
            day_summary = {'Date': eval_date,
                           'RMSE_NN_OutOfSample': rmse_nn, 'RMSE_LM_OutOfSample': rmse_lm,
                           'Time_NN_sec': time_nn_sec, 'Time_LM_sec': time_lm_sec}
            num_a = MODEL_SETTINGS['num_a_segments'] if MODEL_SETTINGS['optimize_a'] else 0
            for p_idx in range(num_a):
                day_summary[f'NN_a_{p_idx+1}'] = params_nn[p_idx]
                day_summary[f'LM_a_{p_idx+1}'] = params_lm[p_idx]
            for p_idx in range(MODEL_SETTINGS['num_sigma_segments']):
                day_summary[f'NN_sigma_{p_idx+1}'] = params_nn[num_a + p_idx]
                day_summary[f'LM_sigma_{p_idx+1}'] = params_lm[num_a + p_idx]
            daily_summary_results.append(day_summary)

            merged_swaption_df = pd.merge(
                results_nn_df, results_lm_df, on=['ExpiryStr', 'TenorStr', 'MarketVol_bps'],
                suffixes=('_NN', '_LM')
            )
            merged_swaption_df.rename(columns={'ModelVol_bps_NN': 'NN_ModelVol_bps', 'ModelVol_bps_LM': 'LM_ModelVol_bps',
                                               'Error_bps_NN': 'NN_Error_bps', 'Error_bps_LM': 'LM_Error_bps'}, inplace=True)
            merged_swaption_df['EvaluationDate'] = eval_date
            per_swaption_results.append(merged_swaption_df)

        # --- 3. Save Final Outputs ---
        print("\n--- Experiment finished. Saving results. ---")
        if daily_summary_results:
            summary_df = pd.DataFrame(daily_summary_results)
            summary_path = os.path.join(FOLDER_COMPARISON_RESULTS, 'daily_summary_results.csv')
            summary_df.to_csv(summary_path, index=False)
            print(f"Daily summary results saved to: {summary_path}")
        
        if per_swaption_results:
            swaption_df = pd.concat(per_swaption_results, ignore_index=True)
            swaption_path = os.path.join(FOLDER_COMPARISON_RESULTS, 'per_swaption_holdout_results.csv')
            cols_order = ['EvaluationDate', 'ExpiryStr', 'TenorStr', 'MarketVol_bps', 
                          'NN_ModelVol_bps', 'LM_ModelVol_bps', 'NN_Error_bps', 'LM_Error_bps']
            swaption_df = swaption_df[cols_order]
            swaption_df.to_csv(swaption_path, index=False)
            print(f"Per-swaption holdout results saved to: {swaption_path}")

        # --- 4. Post-Hoc SHAP Analysis ---
        perform_and_save_shap_analysis(
            nn_artifacts=nn_artifacts,
            test_files_list=test_files,
            external_market_data=external_data,
            settings=MODEL_SETTINGS,
            output_dir=FOLDER_COMPARISON_RESULTS
        )

        print("\n--- SCRIPT FINISHED ---")

    except (FileNotFoundError, ValueError) as e:
        print(f"\nERROR: {e}")
    except Exception as e:
        import traceback
        print(f"\nAn unexpected error occurred: {e}")
        traceback.print_exc()