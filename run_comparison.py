"""
Master Comparison Script for Hull-White Calibration Methods

Objective:
This script implements a robust, scientifically valid comparison framework to evaluate
two methods for calibrating the Hull-White interest rate model:
1. A traditional Levenberg-Marquardt (LM) optimization.
2. A Neural Network (NN) predictor.

Methodology: "Intra-Day Hold-Out Set"
To address the methodological flaw of comparing an in-sample fitter (LM) against an
out-of-sample predictor (NN), this script forces both methods to generalize. For each
day in the test set:
1. The full set of available swaptions is loaded.
2. These swaptions are split via stratified sampling into an 80% calibration set
   and a 20% hold-out (evaluation) set.
3. The NN predicts parameters instantly using the day's market features.
4. The LM method calibrates its parameters using only the 80% calibration set.
5. Crucially, BOTH methods are then evaluated on the same, unseen 20% hold-out set.
6. Performance metrics (RMSE, timing) and calibrated parameters are recorded.

This ensures a fair, apples-to-apples comparison of out-of-sample prediction accuracy.

Post-Comparison Analysis:
After the main evaluation loop, the script performs a SHAP (SHapley Additive exPlanations)
analysis on the Neural Network. This provides valuable insight into which market
features were most influential in the NN's predictions across the entire test set.

Output:
The script generates two key CSV files and a series of SHAP plots:
1. `daily_summary_results.csv`: A high-level daily comparison of RMSE, timing, and
   the parameters generated by each model.
2. `per_swaption_holdout_results.csv`: A granular, swaption-level breakdown of model
   performance on the hold-out sets.
3. SHAP Plots: A set of images (`SHAP_summary_*.png`, `SHAP_importance_*.png`) that
   visualize the feature importance for each of the NN's output parameters.
"""
import datetime
import os
import glob
import time
import pandas as pd
import numpy as np
import QuantLib as ql
import tensorflow as tf
import joblib
import json
import shap
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Any, Optional

from sklearn.model_selection import train_test_split

# Import refactored and helper functions from the other project scripts
from neural_network_calibration import (
    load_and_split_data_chronologically,
    create_ql_yield_curve,
    prepare_calibration_helpers as nn_prepare_helpers,
    load_volatility_cube,
    prepare_nn_features,
    ResidualParameterModel,
    _get_step_dates_from_expiries,
    _expand_params_to_unified_timeline,
    parse_tenor_to_years
)
from traditional_calibration import calibrate_on_subset

# --- CONFIGURATION ---
BASE_DIR = os.getcwd()
DATA_DIR = os.path.join(BASE_DIR, 'data')
RESULTS_DIR = os.path.join(BASE_DIR, 'results')

FOLDER_ZERO_CURVES: str = os.path.join(DATA_DIR, 'EUR ZERO CURVE')
FOLDER_VOLATILITY_CUBES: str = os.path.join(DATA_DIR, 'EUR BVOL CUBE')
FOLDER_EXTERNAL_DATA: str = os.path.join(DATA_DIR, 'EXTERNAL')
FOLDER_NN_MODELS: str = os.path.join(RESULTS_DIR, 'neural_network/models')

# The final output directory for the comparison results
FOLDER_COMPARISON_RESULTS: str = os.path.join(RESULTS_DIR, 'comparison')

# --- MODEL AND EVALUATION SETTINGS ---
# Settings must match the ones used to train the NN model
MODEL_SETTINGS = {
    "num_a_segments": 1, "num_sigma_segments": 7, "optimize_a": True,
    "pricing_engine_integration_points": 32,
    "min_expiry_years": 2.0, "min_tenor_years": 2.0, "use_coterminal_only": False
}

# Settings for the traditional calibrator
TRADITIONAL_CALIBRATION_SETTINGS = {
    "num_a_segments": 1, "num_sigma_segments": 7, "optimize_a": True,
    "initial_a": 0.01821630830602023,
    "initial_sigma": [0.00021102917641460398, 0.00026161313022069355, 0.0002433782032732618, 0.00023207666803595517, 0.00019547828248440554, 0.00013296374054782453, 0.0001745673951321553],
    "pricing_engine_integration_points": 32,
}

# --- HELPER FUNCTIONS ---
def load_nn_artifacts(model_dir: str) -> Dict[str, Any]:
    """
    Loads all necessary artifacts from the trained Neural Network model.

    Parameters
    ----------
    model_dir : str
        The directory containing the trained model's artifacts.

    Returns
    -------
    Dict[str, Any]
        A dictionary containing all the necessary artifacts for the trained model.

    Raises
    ------
    FileNotFoundError
        If the model directory is not found.

    """
    print(f"\n--- Loading Neural Network artifacts from: {model_dir} ---")
    if not os.path.isdir(model_dir):
        raise FileNotFoundError(f"Model directory not found: {model_dir}")

    with open(os.path.join(model_dir, 'feature_names.json'), 'r') as f:
        feature_names = json.load(f)

    artifacts = {
        'model': tf.keras.models.load_model(
            os.path.join(model_dir, 'model.keras'),
            custom_objects={'ResidualParameterModel': ResidualParameterModel}
        ),
        'scaler': joblib.load(os.path.join(model_dir, 'feature_scaler.joblib')),
        'pca_model': joblib.load(os.path.join(model_dir, 'pca_model.joblib')),
        'initial_logits': tf.constant(np.load(os.path.join(model_dir, 'initial_logits.npy')), dtype=tf.float64),
        'feature_names': feature_names
    }
    print("All NN artifacts loaded successfully.")
    return artifacts

def evaluate_on_holdout(
    parameters: np.ndarray,
    holdout_helpers_with_info: List[Tuple[ql.SwaptionHelper, str, str]],
    eval_date: datetime.date,
    term_structure_handle: ql.RelinkableYieldTermStructureHandle,
    settings: Dict
) -> Tuple[pd.DataFrame, float]:
    """
    Evaluates the performance of the neural network on a holdout set of swaption helpers.

    Parameters
    ----------
    parameters : np.ndarray
        The calibrated Hull-White parameters to be used for evaluation.
    holdout_helpers_with_info : List[Tuple[ql.SwaptionHelper, str, str]]
        A list of (SwaptionHelper, expiry_str, tenor_str) tuples to be used for evaluation.
    eval_date : datetime.date
        The evaluation date for the calibration.
    term_structure_handle : ql.RelinkableYieldTermStructureHandle
        A handle to the QuantLib yield curve.
    settings : Dict
        A dictionary containing the necessary settings for the calibration.

    Returns
    -------
    Tuple[pd.DataFrame, float]
        A tuple containing a pandas DataFrame with the evaluation results and the root-mean-squared error in basis points.
    """
    ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
    ql.Settings.instance().evaluationDate = ql_eval_date

    if not holdout_helpers_with_info or np.isnan(parameters).any():
        return pd.DataFrame(), float('nan')

    num_a_params = settings['num_a_segments'] if settings['optimize_a'] else 0
    calibrated_as = parameters[:num_a_params]
    calibrated_sigmas = parameters[num_a_params:]

    included_expiries_yrs = sorted(list(set([parse_tenor_to_years(expiry) for _, expiry, _ in holdout_helpers_with_info])))
    a_step_dates = _get_step_dates_from_expiries(ql_eval_date, included_expiries_yrs, settings['num_a_segments'])
    sigma_step_dates = _get_step_dates_from_expiries(ql_eval_date, included_expiries_yrs, settings['num_sigma_segments'])
    unified_step_dates = sorted(list(set(a_step_dates + sigma_step_dates)))
    
    reversion_quotes = [ql.SimpleQuote(p) for p in calibrated_as]
    sigma_quotes = [ql.SimpleQuote(p) for p in calibrated_sigmas]
    
    expanded_reversion_handles = _expand_params_to_unified_timeline(reversion_quotes, a_step_dates, unified_step_dates)
    expanded_sigma_handles = _expand_params_to_unified_timeline(sigma_quotes, sigma_step_dates, unified_step_dates)
    
    model = ql.Gsr(term_structure_handle, unified_step_dates, expanded_sigma_handles, expanded_reversion_handles, 61.0)
    engine = ql.Gaussian1dSwaptionEngine(model, settings['pricing_engine_integration_points'], 7.0, True, False, term_structure_handle)
    
    results_data = []
    squared_errors = []
    for helper, expiry_str, tenor_str in holdout_helpers_with_info:
        helper.setPricingEngine(engine)
        market_vol = helper.volatility().value()
        market_vol_bps = market_vol * 10000
        try:
            model_npv = helper.modelValue()
            model_vol = helper.impliedVolatility(model_npv, 1e-4, 500, 0.0001, 1.0)
            model_vol_bps = model_vol * 10000
            error_bps = model_vol_bps - market_vol_bps
            squared_errors.append(error_bps**2)
        except (RuntimeError, ValueError):
            model_vol_bps, error_bps = float('nan'), float('nan')
        
        results_data.append({
            'ExpiryStr': expiry_str,
            'TenorStr': tenor_str,
            'MarketVol_bps': market_vol_bps,
            'ModelVol_bps': model_vol_bps,
            'Error_bps': error_bps
        })

    results_df = pd.DataFrame(results_data)
    rmse_bps = np.sqrt(np.mean(squared_errors)) if squared_errors else float('nan')
    
    return results_df, rmse_bps

def perform_and_save_shap_analysis(
    nn_artifacts: Dict[str, Any],
    test_files_list: List[Tuple[datetime.date, str, str]],
    external_market_data: pd.DataFrame,
    settings: Dict,
    output_dir: str
    ) -> None:
    """
    Performs SHAP analysis on a given neural network model to interpret its output.

    Given the model's artifacts, a list of test files, and external market data, this function
    creates a SHAP explainer and calculates the SHAP values for the given inputs. It then
    generates two plots for each parameter: (1) a manual beeswarm plot showing the SHAP values
    for each feature, and (2) a bar plot showing the feature importance based on the mean
    absolute SHAP values.

    The generated plots are saved to the specified output directory.

    Parameters:
        nn_artifacts (Dict[str, Any]): A dictionary containing the model's artifacts.
        test_files_list (List[Tuple[datetime.date, str, str]]): A list of tuples containing the evaluation date, zero curve path, and volatility cube path.
        external_market_data (pd.DataFrame): A pandas DataFrame containing the external market data used for feature extraction.
        settings (Dict): A dictionary containing the model's settings.
        output_dir (str): The directory where the SHAP plots will be saved.

    Returns:
        None
    """
    print("\n--- Starting SHAP Analysis for Model Interpretability ---")
    
    model = nn_artifacts['model']
    scaler = nn_artifacts['scaler']
    pca_model = nn_artifacts['pca_model']
    initial_logits_tensor = nn_artifacts['initial_logits']
    feature_names = nn_artifacts['feature_names']
    
    num_a = settings['num_a_segments'] if settings.get('optimize_a', False) else 0
    num_sigma = settings['num_sigma_segments']
    output_names = [f'a_{i+1}' for i in range(num_a)] + [f'sigma_{i+1}' for i in range(num_sigma)]

    print("Preparing test data for SHAP explanation...")
    test_features_list = []
    for eval_date, zero_path, _ in test_files_list:
        zero_curve_df = pd.read_csv(zero_path, parse_dates=['Date'])
        term_structure = create_ql_yield_curve(zero_curve_df, eval_date)
        ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
        
        transformed_features = prepare_nn_features(
            term_structure, ql_eval_date, scaler, external_market_data,
            pca_model=pca_model, rate_indices=list(range(9))
        )
        test_features_list.append(transformed_features.flatten())
    
    scaled_test_features = np.array(test_features_list)
    test_features_df = pd.DataFrame(scaled_test_features, columns=feature_names)

    features_input = tf.keras.Input(shape=(scaled_test_features.shape[1],), dtype=tf.float64, name="features")
    def tile_logits(features: tf.Tensor) -> tf.Tensor:
        """
        Tile the initial logits tensor to match the shape of the given features
        tensor.

        Parameters:
            features (tf.Tensor): The input features tensor.

        Returns:
            tf.Tensor: The tiled initial logits tensor.
        """
        return tf.tile(initial_logits_tensor, [tf.shape(features)[0], 1])
    logits_input = tf.keras.layers.Lambda(tile_logits)(features_input)
    outputs = model((features_input, logits_input))
    shap_wrapped_model = tf.keras.Model(inputs=features_input, outputs=outputs)
    
    print("Creating SHAP explainer...")
    background_data = shap.sample(scaled_test_features, 100)
    explainer = shap.DeepExplainer(shap_wrapped_model, background_data)
    
    print("Calculating SHAP values... (This may take a while)")
    shap_values = explainer.shap_values(scaled_test_features)

    print(f"Generating and saving {len(output_names) * 2} SHAP plots...")
    rng = np.random.default_rng(seed=42)
    for i, param_name in enumerate(output_names):
        print(f"  -> Plotting for parameter: {param_name}")

        # Determine which slice to use based on the structure of shap_values
        if isinstance(shap_values, list):
            shap_slice = shap_values[i]
        else: # Assumes 3D array
            shap_slice = shap_values[:, :, i]

        # Get feature importances for ordering
        mean_abs_shap = np.mean(np.abs(shap_slice), axis=0)
        feature_order = np.argsort(mean_abs_shap)

        # Create the manual beeswarm plot
        fig, ax = plt.subplots(figsize=(10, 8))
        
        for feature_idx in feature_order:
            y_pos = list(feature_order).index(feature_idx)
            x_vals = shap_slice[:, feature_idx]
            feature_vals = test_features_df.iloc[:, feature_idx]
            
            # Generate vertical jitter
            jitter = np.random.uniform(-0.2, 0.2, size=len(x_vals))
            
            sc = ax.scatter(x_vals, y_pos + jitter, c=feature_vals, cmap='coolwarm', s=15, alpha=0.7)

        ax.set_yticks(range(len(feature_names)))
        ax.set_yticklabels([feature_names[j] for j in feature_order])
        ax.set_xlabel("SHAP value (impact on model output)")
        ax.axvline(0, color='black', linestyle='--', linewidth=0.5)
        
        # Add colorbar
        cbar = fig.colorbar(sc, ax=ax)
        cbar.set_label("Feature value")
        
        fig.suptitle(f'SHAP Value Summary for Parameter "{param_name}"', fontsize=14)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(output_dir, f'SHAP_summary_{param_name}.png'), bbox_inches='tight')
        plt.close(fig)

        # The bar plot should still work correctly
        shap.summary_plot(shap_slice, test_features_df, plot_type='bar', rng=rng, show=False)
        fig = plt.gcf()
        fig.suptitle(f'SHAP Feature Importance for Parameter "{param_name}"', fontsize=14)
        plt.xlabel('Mean |SHAP value|')
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(output_dir, f'SHAP_importance_{param_name}.png'), bbox_inches='tight')
        plt.close(fig)
        
    print("--- SHAP Analysis Completed ---")


def calculate_dynamic_weight(
    previous_rmse: float,
    min_error: float,
    max_error: float,
    w_min: float,
    w_max: float
) -> float:
    """
    Calculates a dynamic weight 'w' based on the previous day's RMSE.

    A low RMSE results in a high weight (trusting the previous day's result),
    while a high RMSE results in a low weight (reverting to the stable guess).
    """
    if np.isnan(previous_rmse):
        # If the last run failed or is the first run, have minimum trust.
        return w_min

    # 1. Scale the error to a [0, 1] range
    scaled_error = (previous_rmse - min_error) / (max_error - min_error)

    # 2. Clamp the value to handle errors outside the defined range
    scaled_error = np.clip(scaled_error, 0.0, 1.0)

    # 3. Linearly interpolate to find the weight (inverse relationship)
    dynamic_w = w_max - scaled_error * (w_max - w_min)
    
    return dynamic_w

# --- MAIN EXPERIMENT ---
if __name__ == '__main__':
    print("="*80)
    print(" Master Comparison of NN vs. LM for Hull-White Calibration ".center(80))
    print(" Methodology: Comparing 3 Distinct LM Initial Guess Strategies ".center(80))
    print("="*80)

    try:
        # --- 1. Load Artifacts and Data ---
        os.makedirs(FOLDER_COMPARISON_RESULTS, exist_ok=True)

        list_of_model_dirs = glob.glob(os.path.join(FOLDER_NN_MODELS, 'model_*'))
        if not list_of_model_dirs:
            raise FileNotFoundError(f"No trained NN models found in {FOLDER_NN_MODELS}")
        latest_model_dir = max(list_of_model_dirs, key=os.path.getctime)
        
        nn_artifacts = load_nn_artifacts(latest_model_dir)

        _, _, test_files = load_and_split_data_chronologically(
            FOLDER_ZERO_CURVES, FOLDER_VOLATILITY_CUBES
        )
        if not test_files:
            raise ValueError("Test set is empty. No files to evaluate.")

        print("\n--- Loading external market data for feature generation ---")
        external_data_csv_path = os.path.join(FOLDER_EXTERNAL_DATA, 'external_market_data.csv')
        if not os.path.exists(external_data_csv_path):
            raise FileNotFoundError(f"External market data not found at {external_data_csv_path}")
        all_dates = [d[0] for d in test_files]
        external_data = pd.read_csv(external_data_csv_path, parse_dates=['Date'], index_col='Date')
        external_data = external_data.reindex(pd.date_range(start=min(all_dates), end=max(all_dates), freq='D')).ffill().bfill()
        print("External data loaded.")
        
        # --- 2. Main Experimental Loop ---
        daily_summary_results = []
        per_swaption_results = []
        
        # --- State Management for Strategies with Memory ---
        # Strategy 2: Pure Rolling (prone to drift)
        previous_pure_rolling_params = None
        # Strategy 3: Adaptive Anchor
        previous_adaptive_anchor_params = None
        previous_adaptive_anchor_rmse = np.nan

        # --- Hyperparameters for the Adaptive Anchor Strategy ---
        MIN_ERROR_THRESHOLD = 6.0  # RMSE below this is considered "good"
        MAX_ERROR_THRESHOLD = 12.0 # RMSE above this is considered "poor"
        W_MAX_TRUST = 0.95         # Weight for good performance
        W_MIN_TRUST = 0.15         # Weight for poor performance (strong anchor)

        print(f"\n--- Starting experiment on {len(test_files)} test days ---")
        for i, (eval_date, zero_path, vol_path) in enumerate(test_files):
            print(f"\nProcessing Day {i+1}/{len(test_files)}: {eval_date.strftime('%Y-%m-%d')}")

            # a. Load daily data and create hold-out set
            zero_df = pd.read_csv(zero_path, parse_dates=['Date'])
            vol_df = load_volatility_cube(vol_path)
            term_structure = create_ql_yield_curve(zero_df, eval_date)
            
            all_helpers_with_info = nn_prepare_helpers(vol_df, term_structure, MODEL_SETTINGS)
            if len(all_helpers_with_info) < 5:
                print(f"  Skipping day {eval_date}: Insufficient valid swaptions.")
                continue

            df_helpers = pd.DataFrame(all_helpers_with_info, columns=['helper', 'ExpiryStr', 'TenorStr'])
            train_helpers_df, holdout_helpers_df = train_test_split(df_helpers, test_size=0.20, random_state=42)
            calibration_set = [tuple(row) for row in train_helpers_df.itertuples(index=False)]
            holdout_set = [tuple(row) for row in holdout_helpers_df.itertuples(index=False)]
            
            if not holdout_set:
                 print(f"  Skipping day {eval_date}: Hold-out set is empty after split.")
                 continue

            print(f"  Split complete: {len(calibration_set)} for calibration, {len(holdout_set)} for hold-out evaluation.")

            # b. Generate NN Parameters
            ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)
            start_time_nn = time.perf_counter()
            feature_vector = prepare_nn_features(
                term_structure, ql_eval_date, nn_artifacts['scaler'], external_data,
                pca_model=nn_artifacts['pca_model'], rate_indices=list(range(9))
            )
            params_nn = nn_artifacts['model']((tf.constant(feature_vector, dtype=tf.float64), nn_artifacts['initial_logits']), training=False).numpy().flatten()
            time_nn_sec = time.perf_counter() - start_time_nn
            print(f"  NN Prediction complete in {time_nn_sec:.6f} seconds.")

            # --- c. Generate LM Parameters for all strategies ---

            # --- STRATEGY 1: STATIC COLD START (The Baseline) ---
            print("  Running LM Strategy 1: Static Cold Start...")
            static_lm_settings = TRADITIONAL_CALIBRATION_SETTINGS.copy()
            start_time_lm_static = time.perf_counter()
            params_lm_static_a, params_lm_static_sigma = calibrate_on_subset(
                eval_date, zero_df, calibration_set, term_structure, **static_lm_settings
            )
            time_lm_static_sec = time.perf_counter() - start_time_lm_static
            params_lm_static = np.concatenate([params_lm_static_a, params_lm_static_sigma]) if params_lm_static_a else np.full(len(params_nn), np.nan)

            # --- STRATEGY 2: PURE ROLLING WARM START (The Drifting Heuristic) ---
            print("  Running LM Strategy 2: Pure Rolling Warm Start...")
            pure_rolling_settings = TRADITIONAL_CALIBRATION_SETTINGS.copy()
            if previous_pure_rolling_params is not None:
                num_a = pure_rolling_settings['num_a_segments']
                pure_rolling_settings['initial_a'] = previous_pure_rolling_params[0]
                pure_rolling_settings['initial_sigma'] = list(previous_pure_rolling_params[num_a:])
            
            start_time_lm_pure_rolling = time.perf_counter()
            params_lm_pure_rolling_a, params_lm_pure_rolling_sigma = calibrate_on_subset(
                eval_date, zero_df, calibration_set, term_structure, **pure_rolling_settings
            )
            time_lm_pure_rolling_sec = time.perf_counter() - start_time_lm_pure_rolling
            params_lm_pure_rolling = np.concatenate([params_lm_pure_rolling_a, params_lm_pure_rolling_sigma]) if params_lm_pure_rolling_a else np.full(len(params_nn), np.nan)

            # --- STRATEGY 3: ADAPTIVE ANCHOR (The Optimized Solution) ---
            print("  Running LM Strategy 3: Adaptive Anchor...")
            static_guess_params = np.array([TRADITIONAL_CALIBRATION_SETTINGS['initial_a']] + TRADITIONAL_CALIBRATION_SETTINGS['initial_sigma'])
            rolling_guess_params = previous_adaptive_anchor_params if previous_adaptive_anchor_params is not None else static_guess_params
            
            dynamic_w = calculate_dynamic_weight(
                previous_adaptive_anchor_rmse, MIN_ERROR_THRESHOLD, MAX_ERROR_THRESHOLD, W_MIN_TRUST, W_MAX_TRUST
            )
            print(f"    -> Dynamic Anchor Weight: {dynamic_w:.4f} (based on previous RMSE of {previous_adaptive_anchor_rmse:.4f})")
            
            blended_guess_params = dynamic_w * rolling_guess_params + (1 - dynamic_w) * static_guess_params
            
            adaptive_anchor_settings = TRADITIONAL_CALIBRATION_SETTINGS.copy()
            num_a = adaptive_anchor_settings['num_a_segments']
            adaptive_anchor_settings['initial_a'] = blended_guess_params[0]
            adaptive_anchor_settings['initial_sigma'] = list(blended_guess_params[num_a:])

            start_time_lm_adaptive = time.perf_counter()
            params_lm_adaptive_a, params_lm_adaptive_sigma = calibrate_on_subset(
                eval_date, zero_df, calibration_set, term_structure, **adaptive_anchor_settings
            )
            time_lm_adaptive_sec = time.perf_counter() - start_time_lm_adaptive
            params_lm_adaptive_anchor = np.concatenate([params_lm_adaptive_a, params_lm_adaptive_sigma]) if params_lm_adaptive_a else np.full(len(params_nn), np.nan)

            # --- d. Evaluate all models on the Hold-Out Set ---
            results_nn_df, rmse_nn = evaluate_on_holdout(params_nn, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            results_static_df, rmse_lm_static = evaluate_on_holdout(params_lm_static, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            results_pure_rolling_df, rmse_lm_pure_rolling = evaluate_on_holdout(params_lm_pure_rolling, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            results_adaptive_df, rmse_lm_adaptive_anchor = evaluate_on_holdout(params_lm_adaptive_anchor, holdout_set, eval_date, term_structure, MODEL_SETTINGS)
            print(f"  Evaluation on Hold-Out Set -> RMSE NN: {rmse_nn:.4f} | LM-Static: {rmse_lm_static:.4f} | LM-PureRolling: {rmse_lm_pure_rolling:.4f} | LM-Adaptive: {rmse_lm_adaptive_anchor:.4f}")

            # --- e. Update State for Next Day's Loop ---
            previous_pure_rolling_params = params_lm_pure_rolling.copy() if not np.isnan(params_lm_pure_rolling).any() else None
            previous_adaptive_anchor_params = params_lm_adaptive_anchor.copy() if not np.isnan(params_lm_adaptive_anchor).any() else None
            previous_adaptive_anchor_rmse = rmse_lm_adaptive_anchor

            # --- f. Collect Results ---
            day_summary = {'Date': eval_date}
            param_sets = {
                'NN': (params_nn, rmse_nn, time_nn_sec),
                'LM_Static': (params_lm_static, rmse_lm_static, time_lm_static_sec),
                'LM_Pure_Rolling': (params_lm_pure_rolling, rmse_lm_pure_rolling, time_lm_pure_rolling_sec),
                'LM_Adaptive_Anchor': (params_lm_adaptive_anchor, rmse_lm_adaptive_anchor, time_lm_adaptive_sec)
            }
            num_a = MODEL_SETTINGS['num_a_segments']
            for name, (params, rmse, time_sec) in param_sets.items():
                day_summary[f'RMSE_{name}'] = rmse
                day_summary[f'Time_{name}_sec'] = time_sec
                for p_idx in range(num_a):
                    day_summary[f'{name}_a_{p_idx+1}'] = params[p_idx]
                for p_idx in range(MODEL_SETTINGS['num_sigma_segments']):
                    day_summary[f'{name}_sigma_{p_idx+1}'] = params[num_a + p_idx]
            daily_summary_results.append(day_summary)

            # Merge granular per-swaption results
            merged_df = results_nn_df.rename(columns={'ModelVol_bps': 'NN_ModelVol_bps', 'Error_bps': 'NN_Error_bps'})
            merged_df = pd.merge(merged_df, results_static_df.rename(columns={'ModelVol_bps': 'LM_Static_ModelVol_bps', 'Error_bps': 'LM_Static_Error_bps'}), on=['ExpiryStr', 'TenorStr', 'MarketVol_bps'])
            merged_df = pd.merge(merged_df, results_pure_rolling_df.rename(columns={'ModelVol_bps': 'LM_Pure_Rolling_ModelVol_bps', 'Error_bps': 'LM_Pure_Rolling_Error_bps'}), on=['ExpiryStr', 'TenorStr', 'MarketVol_bps'])
            merged_df = pd.merge(merged_df, results_adaptive_df.rename(columns={'ModelVol_bps': 'LM_Adaptive_Anchor_ModelVol_bps', 'Error_bps': 'LM_Adaptive_Anchor_Error_bps'}), on=['ExpiryStr', 'TenorStr', 'MarketVol_bps'])
            merged_df['EvaluationDate'] = eval_date
            per_swaption_results.append(merged_df)

        # --- 3. Save Final Outputs ---
        print("\n--- Experiment finished. Saving results. ---")
        if daily_summary_results:
            summary_df = pd.DataFrame(daily_summary_results)
            summary_path = os.path.join(FOLDER_COMPARISON_RESULTS, 'daily_summary_results.csv')
            summary_df.to_csv(summary_path, index=False)
            print(f"Daily summary results saved to: {summary_path}")
        
        if per_swaption_results:
            swaption_df = pd.concat(per_swaption_results, ignore_index=True)
            swaption_path = os.path.join(FOLDER_COMPARISON_RESULTS, 'per_swaption_holdout_results.csv')
            swaption_df.to_csv(swaption_path, index=False)
            print(f"Per-swaption holdout results saved to: {swaption_path}")

        # --- 4. Post-Hoc SHAP Analysis ---
        perform_and_save_shap_analysis(
            nn_artifacts=nn_artifacts,
            test_files_list=test_files,
            external_market_data=external_data,
            settings=MODEL_SETTINGS,
            output_dir=FOLDER_COMPARISON_RESULTS
        )

        print("\n--- SCRIPT FINISHED ---")

    except (FileNotFoundError, ValueError) as e:
        print(f"\nERROR: {e}")
    except Exception as e:
        import traceback
        print(f"\nAn unexpected error occurred: {e}")
        traceback.print_exc()