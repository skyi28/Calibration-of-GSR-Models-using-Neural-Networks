\subsection{Methodology of the Generalized Island Model}
\label{subsec:generalized_island_model}
The Generalized Island Model represents a parallelization scheme that extends the concept of isolated yet interacting populations, originally proposed within the framework of Genetic Algorithms, to a broader class of optimization algorithms. Its fundamental idea is to enhance both computational efficiency and search diversity through the coexistence of multiple, intercommunicating subpopulations referred to as islands. The approach not only accelerates computation via parallel execution but also enriches the optimization dynamics by enabling structured information exchange among subpopulations.

The model belongs to the family of coarse-grained parallelization schemes, where each island evolves largely independently, performing its own optimization process, while periodically exchanging individuals or solutions with other islands. Historically, this framework emerged in the 1980s as a natural extension of Genetic Algorithms, motivated by the need to mitigate premature convergence and improve solution diversity through distributed evolutionary processes. Subsequent research established the superiority of the island model over monolithic, global population schemes, both theoretically and empirically. Moreover, the paradigm has proven effective beyond evolutionary computation, finding applications in other metaheuristics such as Particle Swarm Optimization, where multiple swarms act as parallel islands exchanging particles at regular intervals.

The generalized island model can be applied to a broad spectrum of optimization algorithms as long as they share a compatible solution representation and can incorporate migration mechanisms. Its flexibility allows for the construction of heterogeneous archipelagos, where different islands employ distinct algorithms yet still participate in coordinated information exchange.

Formally, the model defines an archipelago \( A = \langle I, T \rangle \), where \( I = \{ I_1, I_2, \dots, I_n \} \) represents the set of islands (also known as demes), and \( T \) denotes the migration topology, modeled as a directed graph whose vertices correspond to the islands and whose edges specify permissible migration paths. Each island \( I_i \) is characterized by the quadruple \( \langle A_i, P_i, S_i, R_i \rangle \), where \( A_i \) is the optimization algorithm employed by the island, \( P_i \) denotes its local population associated with the problem under study, \( S_i \) defines the migration-selection policy determining which individuals are sent to other islands, and \( R_i \) specifies the migration-replacement policy governing the integration of received individuals into the local population.

The operational flow of the model alternates between independent evolution and coordinated migration phases. During evolution, each island applies its optimization operator \( P' \leftarrow A_i(P, \mu) \), where \( \mu \) represents the migration interval. At migration epochs, each island selects a subset of individuals according to its selection policy \( S_i \) and sends them to other islands following the connectivity defined by the topology \( T \). Upon receiving migrants, the destination islands incorporate them into their populations based on the replacement policy \( R_i \). This decoupling ensures that the internal workings of each optimization process remain independent of the migration mechanism, facilitating modularity and scalability.

The effectiveness of the generalized island model depends on the careful configuration of several parameters. The number of islands \( n \) influences both solution diversity and robustness, as a larger number of islands can explore different regions of the search space or employ distinct parameterizations. The migration topology \( T \) governs the communication structure, with common configurations including ring, torus, and fully connected networks, each characterized by distinct information diffusion properties. The migration interval \( \mu \) controls the frequency of exchanges, requiring a balance between exploration and convergence: overly frequent migrations can lead to homogenization, while infrequent ones may reduce cooperative benefits. Migration can be implemented synchronously, ensuring deterministic behavior at the cost of slower execution, or asynchronously, enhancing scalability at the expense of reproducibility. Finally, the migration-selection and migration-replacement policies \( S_i \) and \( R_i \) define which individuals are exchanged and how they are assimilated, often incorporating stochastic or elitist strategies and determining the overall migration rate.

\textit{Note:} This subsection is entirely based on the description provided in \parencite[pp.~151-169]{izzo2012generalislandmodel}.

This framework was implemented to calibrate the eight parameters of a Hull-White one-factor model with piecewise-constant volatility. The number of islands was set to equal the number of available CPU cores (16), with a total population of 512 individuals distributed among them (32 individuals per population). The migration policy was configured with a unidirectional ring topology, where, at an interval of every 15 generations, the two best-performing individuals from one island migrate to the next. The replacement strategy involved removing the two worst-performing individuals in the receiving population to accommodate the migrants, a common elitist approach. Each island independently executed a genetic algorithm for a total of 150 generations. Within each island, parent selection was managed by a tournament of size three. New individuals were generated through an averaging crossover operator, and a Gaussian mutation operator was applied with a 90\% probability. To balance exploration and exploitation, the mutation strength was dynamically adjusted, decaying exponentially from an initial value of 1.0 to a final value of 0.05 over the generations. Finally, an elitism count of one ensured that the best individual of each generation was preserved. The calibration was performed for a single day, 03.08.2025 (one day before the testing set starts) to determine the initial guess parameters for the \ac{lm} algorithm and concluded after 5576.09 seconds, achieving a final, not vega-weighted \ac{rmse} of 4.21~\ac{bps}.

\subsection{Detailed Implementation and Optimization Techniques}
\label{sec:appendix_implementation_details}
This section provides a detailed account of the algorithmic, numerical, and workflow-level optimizations which were employed in addition to the optimizations listed in seciton \ref{subsec:technical_setup} to ensure the computational feasibility and stability of the neural network training process.

\subsubsection{Algorithmic and Numerical Efficiency}
\label{appendix:algorithmic_and_numerical_efficiency}
Further acceleration was achieved through algorithmic refinements and numerical efficiency improvements. The numerical derivative used in the custom gradient computation can be approximated either by the forward difference or the central difference scheme. The forward difference method requires only one additional model evaluation per parameter and is therefore approximately twice as fast as the central difference method, albeit with potentially higher numerical noise. Given the high computational cost of each QuantLib evaluation, the forward difference scheme was selected as the default method for gradient approximation. Empirical tests confirmed that the increased noise did not significantly impair training stability or convergence, making this a worthwhile trade-off for speed. Eventhough the forward difference scheme is less computational intensive, the central difference scheme was implemented for completeness and potential future use cases.

Additionally, the precision of numerical integration within the QuantLib pricing engine was adjusted through the \texttt{pricing\_engine\_integration\_points} parameter, which controls the number of Gaussian quadrature points used by the \texttt{Gaussian1dSwaptionEngine}. Reducing the number of integration points lowers the computational cost per pricing call but may introduce marginally higher numerical error. The chosen configuration of 32 integration points represents a carefully selected compromise between computational tractability and pricing stability for the problem at hand.

\subsubsection{Optimization of the Training Process}
Several established machine learning strategies were applied to enhance convergence speed and stability. An \textit{early stopping} criterion was implemented to monitor the validation \ac{rmse} after each epoch. This mechanism automatically terminates the training process once no improvement is observed over a predefined patience period of 20 epochs, which both prevents overfitting to the training data and avoids unnecessary computation.

In addition, the concept of \textit{instrument batching} was introduced to reduce the computational burden per training iteration. Rather than evaluating all available swaptions at once, the loss function can be computed on randomly selected subsets of instruments. This stochasticity not only accelerates each iteration but also introduces beneficial noise, which can help the optimizer escape local minima and improve generalization. However, due to the relatively small size of the dataset, all available instruments were used in each batch (a batch size percentage of 100\%) for the final training runs to ensure maximum information utilization per gradient update.

Hyperparameter optimization was conducted using the Hyperband algorithm, as detailed in section~\ref{subsec:hyperband_algorithm}. Hyperband allocates computational resources adaptively by evaluating a large number of configurations for a limited number of epochs and promoting only the most promising candidates for further training. This method significantly reduces the computational time required for hyperparameter tuning while maintaining robustness in identifying high-performing parameter configurations.

\subsubsection{Workflow-Level Enhancements}
To further enhance efficiency, all input data, such as bootstrapped zero curves and volatility cubes, were preloaded into system memory prior to the commencement of model training. This design choice eliminates repeated disk input/output operations during the training loop, thereby minimizing latency and enabling smoother, faster training iterations.

Moreover, the data preprocessing pipeline was designed in a modular fashion, allowing for the selective execution of computationally expensive routines. Boolean flags such as \texttt{PREPROCESS\_CURVES} and \texttt{BOOTSTRAP\_CURVES} permit bypassing redundant computations once intermediate data has been generated and stored. This modularity proved highly beneficial for facilitating efficient experimentation and reproducibility, particularly during the resource-intensive hyperparameter search phase.

\subsection{Principal Component Analysis on the Yield Curve}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/pca_component_loadings.png}
	\caption{Component Loadings for the First Three Principal Components.}
	\label{fig:pca_loadings}
\end{figure}

Figure~\ref{fig:pca_loadings} presents the component loadings (eigenvectors) for the first three principal components obtained from the \ac{pca} applied to the time series of zero-coupon yield curves. Each panel in the figure displays the loading values on the vertical axis against the nine yield curve maturities on the horizontal axis. The shape and sign pattern of each component provide valuable insights into the underlying yield curve movements that each factor represents.

The first principal component (PC1) exhibits uniformly positive loadings across all maturities, indicating that changes in this factor lead to parallel shifts in the yield curve. This behavior corresponds to the so-called level factor, which reflects movements in the overall level of interest rates.

The second principal component (PC2) displays negative loadings for short-term maturities and positive loadings for long-term maturities. This transition from negative to positive values indicates that changes in this factor modify the steepness of the yield curve, capturing variations in the term spread between short- and long-term rates. This component therefore represents the slope factor.

The third principal component (PC3) is characterized by a distinct hump-shaped pattern, with positive loadings at the short and long ends of the maturity spectrum and negative loadings for intermediate maturities. This configuration reflects changes in the curvature of the yield curve, often referred to as butterfly movements, in which medium-term rates move differently from short- and long-term rates.

Taken together, these three loading structures provide strong visual evidence supporting the economic interpretation of the principal components as level, slope, and curvature factors. The figure thus offers a graphical confirmation of the empirical findings discussed in the main text and highlights the fundamental drivers of yield curve dynamics.

\subsection{Hyperparameter Tuning}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/error_distribution_histogram.png}
	\caption{Distribution of Final Validation \ac{rmse} for Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:error_distribution_histogram}
\end{figure}
The distribution of the final validation \ac{rmse} for the top 5\% of models evaluated during the hyperparameter search is shown in figure (\ref{fig:error_distribution_histogram}), with a kernel density estimate overlaid to illustrate the shape of the distribution. Validation errors within this elite subset range from approximately 4.6 to 5.5~\ac{bps}, and the distribution is roughly unimodal and bell-shaped. The highest frequency of models achieves an \ac{rmse} between 5.3 and 5.4~\ac{bps}, while a tail of lower-frequency, higher-performing models extends toward an \ac{rmse} of 4.6~\ac{bps}. This indicates that although multiple hyperparameter configurations can achieve high performance, the majority of well-performing models are concentrated in the 5.0-5.4~\ac{bps} \ac{rmse} range. The left-hand tail represents a small number of exceptionally well-configured models, highlighting that achieving top-tier performance is sensitive and requires a precise combination of hyperparameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_activation.png}
	\caption{Distribution of Activation Functions in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_activation}
\end{figure}
The distribution of activation functions across all hidden layers in the top-performing models is shown in figure \ref{fig:distribution_activation}, comparing the usage of the \ac{relu} and \ac{tanh} functions. Among the top 5\% of models, the choice of activation function is evenly split, with \ac{relu} and \ac{tanh} each employed in 25 of the top 50 models. This finding indicates that, for this specific modeling task, the selection between \ac{relu} and \ac{tanh} does not significantly impact model performance. Both functions appear equally capable of enabling the network to learn the complex mapping required, suggesting that performance is more sensitive to other architectural and training hyperparameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_dropout_rate.png}
	\caption{Distribution of Dropout Rate in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_dropout}
\end{figure}
The distribution of dropout rates among the top-performing models with dropout enabled is illustrated in figure \ref{fig:distribution_dropout}, with the x-axis representing the fraction of neurons deactivated during training. The distribution is multi-modal, exhibiting notable peaks in the ranges of 0.10-0.15, 0.25-0.30, and 0.40-0.45, with the highest frequency occurring around 0.25-0.30. This pattern suggests that the optimal level of regularization is highly dependent on other hyperparameters, such as network depth and width. Larger or deeper networks may require more aggressive dropout, whereas smaller networks may perform better with lower dropout rates. Although the peak near 0.30 indicates a commonly effective configuration, the overall spread demonstrates that no single dropout rate serves as a universally optimal setting.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_learning_rate.png}
	\caption{Distribution of Learning Rates in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_learning_rate}
\end{figure}
The distribution of learning rates among the top 5\% of models is shown in figure \ref{fig:distribution_learning_rate}, illustrating the frequency of different rates selected during the hyperparameter search. While learning rates are spread across the explored range, they are most heavily concentrated between 0.002 and 0.007, with a distinct peak in the 0.002-0.003 bin. Very high learning rates above 0.008 and very low rates below 0.002 occur infrequently. This pattern indicates that a moderately low learning rate is important for achieving optimal performance. Excessively high rates can cause unstable training and prevent convergence, whereas very low rates may result in excessively slow learning. The concentration within the moderate range suggests a well-defined "valley" in the loss landscape that can be effectively navigated with appropriately chosen learning rates.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_num_layers.png}
	\caption{Distribution of Number of Hidden Layers in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_num_layers}
\end{figure}
The distribution of the total number of hidden layers among the top 5\% of models is shown in figure \ref{fig:distribution_num_layers}, with the x-axis representing the number of hidden layers. The distribution is strongly skewed towards deeper networks, with the most frequent configurations comprising three, four, or five hidden layers. Shallow networks with only two layers occur significantly less often among the top-performing models. This pattern indicates that network depth is a critical factor for effectively modeling the problem, suggesting that the relationships between the market data and the Hull-White parameters are hierarchical and complex, and require multiple layers of non-linear transformations to be captured accurately.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_use_dropout.png}
	\caption{Distribution of Use of Dropout in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_use_dropout}
\end{figure}
The distribution of the "Use Dropout" hyperparameter among the top-performing models is presented in figure \ref{fig:distribution_use_dropout}, indicating whether a dropout layer was enabled or disabled for regularization. Among the top 50 models, a majority (30) employed dropout, while a substantial minority (20) did not. This suggests that dropout generally contributes to improved generalization on this dataset. However, the fact that a significant portion of top models performed best without dropout indicates that its effectiveness is not universal and likely depends on the specific architecture of the network.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_underestimation_penalty.png}
	\caption{Distribution of Underestimation Penalty in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_underestimation_penalty}
\end{figure}
The distribution of the custom underestimation penalty hyperparameter among the top-performing models is shown in figure \ref{fig:distribution_underestimation_penalty}, where a value of 1.0 represents no additional penalty. The distribution is strongly right-skewed, with the majority of the best models employing a low penalty between 1.0 and 1.5. Higher penalty values occur infrequently among top models. This indicates that imposing a large penalty for underestimation is detrimental to performance when evaluated with a symmetric metric such as \ac{rmse}. While the penalty is financially motivated, excessive conservatism introduces systematic overestimation, increasing overall error. The most effective models capture the underlying distribution accurately using a balanced or only slightly asymmetric loss function.

\subsection{Final Model Training}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/training_history.png}
	\caption{Training History of Final Model using Best Hyperparameters.}
	\label{fig:training_history}
\end{figure}
The training and validation loss history of the best-performing model over 59 epochs is illustrated in figure \ref{fig:training_history}, where the y-axis represents the \ac{rmse} in \ac{bps} and the x-axis represents the training epoch. The blue line denotes the weighted training \ac{rmse}, incorporating an asymmetric penalty for underestimation, while the red line shows the standard, unweighted validation \ac{rmse}. A vertical dashed line at epoch 39 indicates the point at which the model achieved its lowest validation error and was selected as the final model through early stopping.

The training process exhibits several distinct phases. An initial rapid convergence occurs within the first ten epochs, during which both training and validation \ac{rmse} decrease sharply from over 12~\ac{bps} to approximately 5-6~\ac{bps}. This is followed by a plateau phase characterized by noisy fluctuations without a significant long-term downward trend. Throughout this phase, the validation \ac{rmse} consistently remains lower than the weighted training \ac{rmse}, reaching its global minimum at epoch 39. After this point, the validation error does not improve over the subsequent twenty epochs, triggering early stopping and restoring the model weights from epoch 39.

The learning curve provides several insights into model behavior and the optimization landscape. The steep initial decrease indicates that the network architecture and learning rate effectively capture the principal patterns in the training data. The subsequent noisy yet stable plateau reflects the complex, non-convex loss surface typical of financial model calibration, which the stochastic optimizer successfully navigates without divergence. The persistent gap between training and validation \ac{rmse} is a direct consequence of the asymmetric loss function: the elevated training loss reflects the explicit penalty for underestimation, while the lower validation \ac{rmse} provides an unbiased measure of generalization. Finally, early stopping acts as an effective regularization mechanism, ensuring that the final model generalizes well to unseen data rather than overfitting, as evidenced by the stability of the validation curve and the model's robust predictive performance.

\subsection{Pricing Comparison}
\label{appendix:hypothesis_tests}
\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Shapiro-Wilk Normality Test for Each Strategy}
		\label{tab:shapiro_wilk}
		\begin{tabular}{lccccc}
			\toprule
			Strategy           & Statistic & p-value & $\alpha=0.01$ & $\alpha=0.05$ & $\alpha=0.10$ \\
			\midrule
			Neural Network     & 0.9076    & 0.0173  & Not Rejected  & Rejected      & Rejected      \\
			\ac{lm} Static          & 0.7788    & 0.0000  & Rejected      & Rejected      & Rejected      \\
			\ac{lm} Pure Rolling    & 0.9360    & 0.0877  & Not Rejected  & Not Rejected  & Rejected      \\
			\ac{lm} Adaptive Anchor & 0.8237    & 0.0003  & Rejected      & Rejected      & Rejected      \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The table shows Shapiro-Wilk test statistics, p-values, and decisions at three significance levels. Rejected indicates that the null hypothesis of normality is rejected.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Augmented Dickey-Fuller Test for Stationarity}
		\label{tab:adf_test}
		\begin{tabular}{lccccc}
			\toprule
			Strategy           & ADF Statistic & p-value & $\alpha=0.01$ & $\alpha=0.05$ & $\alpha=0.10$ \\
			\midrule
			Neural Network     & 1.1245        & 0.9954  & Not Rejected  & Not Rejected  & Not Rejected  \\
			\ac{lm} Static          & -4.2964       & 0.0005  & Rejected      & Rejected      & Rejected      \\
			\ac{lm} Pure Rolling    & -1.3795       & 0.5921  & Not Rejected  & Not Rejected  & Not Rejected  \\
			\ac{lm} Adaptive Anchor & -5.3023       & 0.0000  & Rejected      & Rejected      & Rejected      \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The null hypothesis of the ADF test is non-stationarity. “Rejected” indicates that the series is stationary at the given significance level.
		\end{tablenotes}
	\end{threeparttable}
\end{table}
Tables \ref{tab:shapiro_wilk} and \ref{tab:adf_test} summarize the results of the Shapiro-Wilk normality test and the Augmented Dickey-Fuller stationarity test, respectively, for the daily out-of-sample \ac{rmse} time series of each calibration strategy.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot2_rmse_distribution_combined.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm}.}
	\label{fig:distribution_rmse_violin}
\end{figure}
 Figure \ref{fig:distribution_rmse_violin} presents a comparative analysis of the daily out-of-sample \ac{rmse} distributions for the four calibration strategies under investigation. The visualization employs split violin plots to contrast the probability densities of both the unweighted and the vega-weighted \ac{rmse} for each model over the entire test period. This allows for a detailed examination of each method's accuracy, stability, and performance on financially significant instruments.

The \ac{nn} exhibits a distribution that is characterized by a low central tendency and minimal dispersion. The violin plot for the \ac{nn} is situated at the lowest level on the vertical axis, with a median \ac{rmse} around 4~\ac{bps}, and is substantially more compact than those of the other methods. This visual representation indicates a high degree of both accuracy and stability, suggesting that the \ac{nn} consistently generated low-error calibrations on a daily basis. The close alignment of its unweighted and vega-weighted distributions further implies that the model's performance is robust across instruments of varying price sensitivity to volatility.

In contrast, the \ac{lm} strategies display markedly different performance characteristics. The \ac{lm} (Pure Rolling) strategy yields the widest and most elongated error distribution, extending beyond 30~\ac{bps}. This demonstrates a high degree of instability and a significantly higher average error, which is consistent with the hypothesis of error propagation and parameter drift. The observation that its vega-weighted error distribution is shifted upwards relative to its unweighted counterpart indicates that this strategy's largest pricing errors are concentrated on the swaptions with the highest financial sensitivity.

The \ac{lm} (Static) approach shows a less extreme, yet still considerable, error distribution compared to the \ac{nn}. Its median error is approximately double that of the \ac{nn}, and its larger vertical span signifies lower day-to-day stability. Similar to the Pure Rolling strategy, the vega-weighted \ac{rmse} distribution is slightly higher than the unweighted one, suggesting a tendency to misprice more sensitive instruments.

The \ac{lm} (Adaptive Anchor) strategy represents the most effective of the traditional optimization methods. Its error distribution is located at a lower level than the other two \ac{lm} strategies, and its variance is visibly reduced, though it remains substantially larger than that of the \ac{nn}. An important distinction for this method is that its vega-weighted error distribution has a slightly lower median than its unweighted counterpart. This suggests that, unlike the other \ac{lm} methods, the Adaptive Anchor strategy achieves a comparatively better fit for the most vega-sensitive instruments.

In summary, the graphical evidence demonstrates a clear hierarchy in performance. The \ac{nn} provides the most accurate and stable calibrations. The \ac{lm} (Adaptive Anchor) method offers a substantial improvement over naive \ac{lm} initialization strategies but does not match the performance of the machine learning approach. The \ac{lm} (Pure Rolling) and \ac{lm} (Static) methods exhibit significant deficiencies in both stability and accuracy, particularly when evaluated using a financially meaningful, vega-weighted error metric.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot9_error_by_volatility.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm} by Swaption Volatility Quantiles.}
	\label{fig:distribution_rmse_by_swaption_volatility_quantile}
\end{figure}

Figure \ref{fig:distribution_rmse_by_swaption_volatility_quantile} presents a conditional analysis of model performance by displaying the distribution of unweighted prediction errors, stratified by the level of market volatility. The dataset is partitioned into three quantiles representing low (0-25\%), medium (25-75\%), and high (75-100\%) volatility regimes. This segmentation facilitates an assessment of each model's robustness and potential state-dependent biases.

The \ac{nn} model demonstrates a high degree of performance consistency across all three volatility environments. Its error distribution, represented by the blue boxplots, is characterized by a median that remains close to the zero-error line and a compact interquartile range. This indicates both low bias and low variance, irrespective of the prevailing market volatility level. The model exhibits a marginal positive bias, signifying a slight tendency to overpredict volatility, but the magnitude of this bias is minimal and stable across the quantiles.

The \ac{lm} strategies exhibit significant performance degradation and systematic biases that are persistent across the volatility regimes. The \ac{lm} (Static) method consistently produces a large positive error, with its median residing between approximately 10 and 15~\ac{bps} above zero. The substantial interquartile range of this method highlights a high degree of prediction uncertainty. This systematic overprediction suggests that the fixed initial parameter guess becomes progressively less suitable but does so in a manner that is not strongly dependent on the volatility level itself.

Conversely, the \ac{lm} (Pure Rolling) strategy displays a severe and consistent negative bias, with median errors centered around -20~\ac{bps}. The large interquartile range and extensive whiskers, particularly in the low and high volatility buckets, are indicative of extreme parameter instability. This result provides further evidence of the parameter drift phenomenon, where the model converges to a suboptimal state that systematically underestimates market volatility across all conditions.

The \ac{lm} (Adaptive Anchor) strategy provides a more controlled performance relative to the other \ac{lm} variants. Its error distribution is centered closer to zero, although it maintains a consistent negative bias across all quantiles. The interquartile range is considerably smaller than that of the Static and Pure Rolling methods, indicating improved stability. Nonetheless, its error distribution is wider and its bias is more pronounced than that of the \ac{nn} in all market regimes.

In conclusion, the analysis reveals that the \ac{nn}'s calibration performance is robust to changes in market volatility, consistently delivering the most accurate and stable predictions. The traditional \ac{lm} methods, in contrast, are prone to significant systematic biases that are not mitigated by shifts in the market environment. The choice of initialization strategy is paramount, with the Adaptive Anchor method effectively containing the extreme errors of the other \ac{lm} approaches, yet failing to achieve the level of accuracy and robustness demonstrated by the machine learning model.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot8_error_by_tenor.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm} by Swaption Tenor.}
	\label{fig:distribution_rmse_by_swaption_tenor}
\end{figure}

Figure \ref{fig:distribution_rmse_by_swaption_tenor} offers a granular analysis of the unweighted prediction error distributions, conditioned on the tenor of the underlying swaption. The data is segregated into three distinct buckets: Short (0-3 years), Medium (3-7 years), and Long (7+ years). This stratification allows for an examination of how each calibration model's accuracy varies across the term structure of the swaption volatility surface.

The \ac{nn} model demonstrates a structurally consistent and relatively balanced performance across all tenor buckets. For short-tenor swaptions, it exhibits a slight positive median error of approximately 4~\ac{bps}, indicating a tendency to overpredict volatility. As the tenor increases to the medium and long buckets, this bias shifts to a marginal underprediction, with medians slightly below the zero-error line. A key characteristic of the \ac{nn} is that its interquartile range remains compact and its median error stays proximate to zero across the entire term structure, signifying robust and stable pricing performance regardless of the instrument's tenor.

The performance of the \ac{lm} strategies reveals significant structural biases that are highly dependent on the swaption tenor. The \ac{lm} (Static) model's accuracy deteriorates monotonically with increasing tenor. While its median error is close to zero for short-tenor instruments, it develops a substantial positive bias in the medium-tenor bucket, which becomes even more pronounced for long-tenor swaptions, where the median error exceeds 5~\ac{bps} and the interquartile range is notably large.

The \ac{lm} (Pure Rolling) strategy is consistently the least accurate model, displaying a severe negative bias that worsens dramatically as the tenor lengthens. The median error begins at approximately -8~\ac{bps} for short tenors and declines to nearly -18 \ac{bps} for long tenors. The expansion of the interquartile range for longer tenors further underscores the model's instability and its fundamental failure to capture the term structure dynamics, a direct consequence of parameter drift.

The \ac{lm} (Adaptive Anchor) strategy performs best among the traditional optimizers, yet it is characterized by a persistent negative bias across all tenor buckets. The median error is approximately -3~\ac{bps} for short and medium tenors, increasing to -5~\ac{bps} for long-tenor swaptions. Although its bias is more controlled and its variance is smaller than the other \ac{lm} methods, its performance remains inferior to the \ac{nn} across all segments of the term structure.

In summary, the analysis demonstrates that the \ac{nn} provides the most accurate calibration across the full range of swaption tenors, exhibiting only minor, structured biases. In contrast, all \ac{lm} methods produce pricing errors with strong structural dependencies on the instrument tenor, indicating a failure to correctly model the entire volatility surface simultaneously.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot7_error_by_expiry.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm} by Swaption Expirty.}
	\label{fig:distribution_rmse_by_swaption_expiriy}
\end{figure}

Figure \ref{fig:distribution_rmse_by_swaption_expiriy} provides a conditional analysis of the unweighted daily pricing errors, segmented by the maturity of the swaption. The data is partitioned into Short (0-3 years), Medium (3-7 years), and Long (7+ years) expiry buckets to evaluate how each model's performance varies along the maturity dimension of the volatility surface.

The \ac{nn} model exhibits a distinct structural pattern in its pricing errors across the expiry buckets. For short-dated swaptions, the model displays a consistent underprediction bias, with a median error of approximately -8~\ac{bps}. As the swaption expiry increases into the medium bucket, the error distribution shifts upwards, becoming more symmetric with a median close to zero. For long-expiry swaptions, the bias reverses, resulting in a slight overprediction with a median error of approximately 2~\ac{bps}. Despite this structured bias, the interquartile range of the \ac{nn} remains relatively compact across all buckets, indicating a stable level of prediction variance.

The \ac{lm} strategies demonstrate more pronounced and systematic biases. The \ac{lm} (Static) model's performance degrades as the swaption expiry increases. It shows a marginal positive bias for short-dated swaptions, which escalates significantly for medium and long expiries, where the median error reaches approximately 7~\ac{bps}. The interquartile range for this model also expands considerably with longer maturities, signifying a loss of precision.

The \ac{lm} (Pure Rolling) strategy consistently yields the most inaccurate results, characterized by a severe underprediction bias. The median error is approximately -22~\ac{bps} for short-expiry instruments. While this bias lessens for medium-term expiries, it remains substantial, and the interquartile range becomes exceptionally large, reflecting high instability. For long-expiry swaptions, the underprediction bias intensifies again, highlighting a fundamental inability of the drifting parameters to accurately price instruments across the maturity spectrum.

The \ac{lm} (Adaptive Anchor) model shows a consistent underprediction bias across all expiry buckets, with median errors centered around -6~\ac{bps}. While its performance is notably more stable than the other \ac{lm} variants, especially for short-dated options where its interquartile range is the tightest, its accuracy diminishes for medium and long expiries as evidenced by an expanding interquartile range.

In conclusion, the analysis reveals that all calibration methods produce errors that are structurally dependent on the swaption expiry. The \ac{nn} exhibits the most balanced performance, with the smallest error magnitudes and a contained variance across the buckets, despite a clear shift in bias from underprediction to overprediction. The \ac{lm} methods, by contrast, are subject to larger and more systematic biases that suggest difficulties in simultaneously fitting the entire term structure of swaption volatilities.

\subsection{\ac{shap} Values}
\label{appendix:shap_values}
Presented below are the SHAP feature importance and summary plots for each of the eight output parameters of the Hull-White model. This comprehensive analysis covers the mean-reversion parameter, \(\alpha\), as well as the seven piecewise constant volatility parameters, from \(\sigma_1\) through \(\sigma_7\). The SHAP values were computed based on the network's performance on the hold-out test set, thereby providing insight into its out-of-sample decision-making process.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_importance_a_1.png}
	\caption{\ac{shap} Feature Importance for $\alpha$.}
	\label{fig:shap_importance_a_1}
\end{figure}
\foreach \i in {1,...,7}{
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_importance_sigma_\i.png}
			\caption{\ac{shap} Feature Importance for $\sigma_\i$.}
			\label{fig:shap_importance_sigma_\i}
		\end{figure}
	}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_summary_a_1.png}
	\caption{\ac{shap} Feature Summary for $\alpha$.}
	\label{fig:shap_summary_a_1}
\end{figure}
\foreach \i in {1,...,7}{
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_summary_sigma_\i.png}
			\caption{\ac{shap} Feature Importance for $\sigma_\i$.}
			\label{fig:shap_summary_sigma_\i}
		\end{figure}
	}

\subsection{Code and Data Availability}
For the purposes of transparency and to facilitate further research, the complete source code implementing the methodologies, experiments, and analyses detailed within this thesis has been made publicly available. The project repository is hosted on GitHub and can be accessed via the following URL: \linebreak \url{https://github.com/skyi28/Calibration-of-GSR-Models-using-Neural-Networks}

It is important to note that the repository contains only the source code and not the underlying market data utilized in the empirical study. The dataset, which was gathered from the Bloomberg Terminal, is not provided due to the proprietary nature of the information and the restrictive terms of the Bloomberg data license agreement, which expressly forbids the redistribution of its data.