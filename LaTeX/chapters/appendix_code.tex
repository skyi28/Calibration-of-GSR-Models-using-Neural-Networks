\subsection{Principal Component Analysis on the Yield Curve}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/pca_component_loadings.png}
	\caption{Component Loadings for the First Three Principal Components.}
	\label{fig:pca_loadings}
\end{figure}

Figure~\ref{fig:pca_loadings} presents the component loadings (eigenvectors) for the first three principal components obtained from the \ac{pca} applied to the time series of zero-coupon yield curves. Each panel in the figure displays the loading values on the vertical axis against the nine yield curve maturities on the horizontal axis. The shape and sign pattern of each component provide valuable insights into the underlying yield curve movements that each factor represents.

The first principal component (PC1) exhibits uniformly positive loadings across all maturities, indicating that changes in this factor lead to parallel shifts in the yield curve. This behavior corresponds to the so-called level factor, which reflects movements in the overall level of interest rates.

The second principal component (PC2) displays negative loadings for short-term maturities and positive loadings for long-term maturities. This transition from negative to positive values indicates that changes in this factor modify the steepness of the yield curve, capturing variations in the term spread between short- and long-term rates. This component therefore represents the slope factor.

The third principal component (PC3) is characterized by a distinct hump-shaped pattern, with positive loadings at the short and long ends of the maturity spectrum and negative loadings for intermediate maturities. This configuration reflects changes in the curvature of the yield curve, often referred to as butterfly movements, in which medium-term rates move differently from short- and long-term rates.

Taken together, these three loading structures provide strong visual evidence supporting the economic interpretation of the principal components as level, slope, and curvature factors. The figure thus offers a graphical confirmation of the empirical findings discussed in the main text and highlights the fundamental drivers of yield curve dynamics.

\subsection{Hyperparameter Tuning}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/error_distribution_histogram.png}
	\caption{Distribution of Final Validation \ac{rmse} for Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:error_distribution_histogram}
\end{figure}
The distribution of the final validation \ac{rmse} for the top 5\% of models evaluated during the hyperparameter search is shown in figure (\ref{fig:error_distribution_histogram}), with a kernel density estimate overlaid to illustrate the shape of the distribution. Validation errors within this elite subset range from approximately 4.6 to 5.5~\ac{bps}, and the distribution is roughly unimodal and bell-shaped. The highest frequency of models achieves an \ac{rmse} between 5.3 and 5.4~\ac{bps}, while a tail of lower-frequency, higher-performing models extends toward an \ac{rmse} of 4.6~\ac{bps}. This indicates that although multiple hyperparameter configurations can achieve high performance, the majority of well-performing models are concentrated in the 5.0-5.4~\ac{bps} \ac{rmse} range. The left-hand tail represents a small number of exceptionally well-configured models, highlighting that achieving top-tier performance is sensitive and requires a precise combination of hyperparameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_activation.png}
	\caption{Distribution of Activation Functions in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_activation}
\end{figure}
The distribution of activation functions across all hidden layers in the top-performing models is shown in figure \ref{fig:distribution_activation}, comparing the usage of the \ac{relu} and \ac{tanh} functions. Among the top 5\% of models, the choice of activation function is evenly split, with \ac{relu} and \ac{tanh} each employed in 25 of the top 50 models. This finding indicates that, for this specific modeling task, the selection between \ac{relu} and \ac{tanh} does not significantly impact model performance. Both functions appear equally capable of enabling the network to learn the complex mapping required, suggesting that performance is more sensitive to other architectural and training hyperparameters.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_dropout_rate.png}
	\caption{Distribution of Dropout Rate in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_dropout}
\end{figure}
The distribution of dropout rates among the top-performing models with dropout enabled is illustrated in figure \ref{fig:distribution_dropout}, with the x-axis representing the fraction of neurons deactivated during training. The distribution is multi-modal, exhibiting notable peaks in the ranges of 0.10-0.15, 0.25-0.30, and 0.40-0.45, with the highest frequency occurring around 0.25-0.30. This pattern suggests that the optimal level of regularization is highly dependent on other hyperparameters, such as network depth and width. Larger or deeper networks may require more aggressive dropout, whereas smaller networks may perform better with lower dropout rates. Although the peak near 0.30 indicates a commonly effective configuration, the overall spread demonstrates that no single dropout rate serves as a universally optimal setting.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_learning_rate.png}
	\caption{Distribution of Learning Rates in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_learning_rate}
\end{figure}
The distribution of learning rates among the top 5\% of models is shown in figure \ref{fig:distribution_learning_rate}, illustrating the frequency of different rates selected during the hyperparameter search. While learning rates are spread across the explored range, they are most heavily concentrated between 0.002 and 0.007, with a distinct peak in the 0.002-0.003 bin. Very high learning rates above 0.008 and very low rates below 0.002 occur infrequently. This pattern indicates that a moderately low learning rate is important for achieving optimal performance. Excessively high rates can cause unstable training and prevent convergence, whereas very low rates may result in excessively slow learning. The concentration within the moderate range suggests a well-defined "valley" in the loss landscape that can be effectively navigated with appropriately chosen learning rates.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_num_layers.png}
	\caption{Distribution of Number of Hidden Layers in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_num_layers}
\end{figure}
The distribution of the total number of hidden layers among the top 5\% of models is shown in figure \ref{fig:distribution_num_layers}, with the x-axis representing the number of hidden layers. The distribution is strongly skewed towards deeper networks, with the most frequent configurations comprising three, four, or five hidden layers. Shallow networks with only two layers occur significantly less often among the top-performing models. This pattern indicates that network depth is a critical factor for effectively modeling the problem, suggesting that the relationships between the market data and the Hull-White parameters are hierarchical and complex, and require multiple layers of non-linear transformations to be captured accurately.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_use_dropout.png}
	\caption{Distribution of Use of Dropout in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_use_dropout}
\end{figure}
The distribution of the "Use Dropout" hyperparameter among the top-performing models is presented in figure \ref{fig:distribution_use_dropout}, indicating whether a dropout layer was enabled or disabled for regularization. Among the top 50 models, a majority (30) employed dropout, while a substantial minority (20) did not. This suggests that dropout generally contributes to improved generalization on this dataset. However, the fact that a significant portion of top models performed best without dropout indicates that its effectiveness is not universal and likely depends on the specific architecture of the network.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/distribution_underestimation_penalty.png}
	\caption{Distribution of Underestimation Penalty in Top 5\% of \ac{nn} from Hyperparameter Search.}
	\label{fig:distribution_underestimation_penalty}
\end{figure}
The distribution of the custom underestimation penalty hyperparameter among the top-performing models is shown in figure \ref{fig:distribution_underestimation_penalty}, where a value of 1.0 represents no additional penalty. The distribution is strongly right-skewed, with the majority of the best models employing a low penalty between 1.0 and 1.5. Higher penalty values occur infrequently among top models. This indicates that imposing a large penalty for underestimation is detrimental to performance when evaluated with a symmetric metric such as \ac{rmse}. While the penalty is financially motivated, excessive conservatism introduces systematic overestimation, increasing overall error. The most effective models capture the underlying distribution accurately using a balanced or only slightly asymmetric loss function.

\subsection{Final Model Training}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/training_history.png}
	\caption{Training History of Final Model using Best Hyperparameters.}
	\label{fig:training_history}
\end{figure}
The training and validation loss history of the best-performing model over 59 epochs is illustrated in figure \ref{fig:training_history}, where the y-axis represents the \ac{rmse} in basis points and the x-axis represents the training epoch. The blue line denotes the weighted training \ac{rmse}, incorporating an asymmetric penalty for underestimation, while the red line shows the standard, unweighted validation \ac{rmse}. A vertical dashed line at epoch 39 indicates the point at which the model achieved its lowest validation error and was selected as the final model through early stopping.

The training process exhibits several distinct phases. An initial rapid convergence occurs within the first ten epochs, during which both training and validation \ac{rmse} decrease sharply from over 12~\ac{bps} to approximately 5-6~\ac{bps}. This is followed by a plateau phase characterized by noisy fluctuations without a significant long-term downward trend. Throughout this phase, the validation \ac{rmse} consistently remains lower than the weighted training \ac{rmse}, reaching its global minimum at epoch 39. After this point, the validation error does not improve over the subsequent twenty epochs, triggering early stopping and restoring the model weights from epoch 39.

The learning curve provides several insights into model behavior and the optimization landscape. The steep initial decrease indicates that the network architecture and learning rate effectively capture the principal patterns in the training data. The subsequent noisy yet stable plateau reflects the complex, non-convex loss surface typical of financial model calibration, which the stochastic optimizer successfully navigates without divergence. The persistent gap between training and validation \ac{rmse} is a direct consequence of the asymmetric loss function: the elevated training loss reflects the explicit penalty for underestimation, while the lower validation \ac{rmse} provides an unbiased measure of generalization. Finally, early stopping acts as an effective regularization mechanism, ensuring that the final model generalizes well to unseen data rather than overfitting, as evidenced by the stability of the validation curve and the model's robust predictive performance.

\subsection{Pricing Comparison}
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot2_rmse_distribution.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm}.}
	\label{fig:distribution_rmse_violin plot}
\end{figure}
The daily out-of-sample \ac{rmse} (no vega weighting was applied) for the \ac{nn} and \ac{lm} calibration methods is presented using a violin plot (figure \ref{fig:distribution_rmse_violin plot}), where the width represents the frequency of each daily error and the inner box plot indicates the median and interquartile range. A clear separation between the two methods is observed. The \ac{nn}'s error distribution is centered at a lower level, with a median of approximately 6.5~\ac{bps}, and is considerably more compact, with most daily errors between 5.5 and 7.5~\ac{bps}. In contrast, the \ac{lm} method exhibits a higher median of around 9.8~\ac{bps} and a substantially wider dispersion, ranging from approximately 8 to 12~\ac{bps}. Notably, the 75th percentile of the \ac{nn} distribution is lower than the 25th percentile of the \ac{lm} distribution.

These results provide strong evidence of both the superior accuracy of the \ac{nn} approach. The consistently lower \ac{rmse} indicates that the \ac{nn} generalizes more effectively to unseen market data, while the narrower distribution demonstrates more reliable and predictable performance, which is essential for practical risk management. The minimal overlap between the core distributions further emphasizes that the \ac{nn}'s worst-day performance typically exceeds the average performance of the \ac{lm} method.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/appendix/plot9_error_by_volatility.png}
	\caption{Distribution of \ac{rmse} between \ac{nn} and \ac{lm} by Swaption Volatility Quantiles.}
	\label{fig:distribution_rmse_by_swaption_volatility_quantile}
\end{figure}
The instrument-level pricing errors (no vega weighting was applied), defined as the difference between model-implied and market-observed volatilities, are analyzed across three market volatility quantiles—Low (0-25\%), Mid (25-75\%), and High (75-100\%)—using a grouped box plot (figure \ref{fig:distribution_rmse_by_swaption_volatility_quantile}), with a dashed line at zero indicating perfect predictions. The \ac{lm} method exhibits large, systematic, and offsetting biases. In the low-volatility regime, it shows a strong positive bias, with a median error exceeding 10~\ac{bps}, indicating overpricing, while in the high-volatility regime, the bias reverses to a median error of approximately -5~\ac{bps}, indicating underpricing. In contrast, the \ac{nn} maintains a consistent and less biased profile across all volatility regimes, with median errors closer to zero and substantially smaller interquartile ranges, reflecting lower prediction variance.

This analysis highlights a key limitation of the traditional \ac{lm} optimizer: its inability to simultaneously fit instruments across the full volatility surface results in severe, systematic mispricings, overcompensating in one regime to correct errors in another. The \ac{nn}, however, learns a more flexible and robust mapping that generalizes effectively across the volatility spectrum. Its consistently lower bias and variance demonstrate a superior capacity to capture the complex dynamics of the market, making it a more reliable tool for pricing and risk management.

\subsection{\ac{shap} Values}
The \ac{shap} feature importance and summary plots provide a detailed understanding of how individual features influence the model's predictions across different parameters. The \ac{shap} feature importance plot presents an aggregated view by ranking features according to their mean absolute \ac{shap} values across all predictions. The longer a bar appears in this plot, the greater the average magnitude of that feature's contribution to the model's output, irrespective of direction. This visualization therefore highlights the features with the strongest overall predictive power. In contrast, the \ac{shap} value summary plot (beeswarm plot) offers a granular perspective, showing the distribution of individual \ac{shap} values for each feature across all samples. Each dot represents a single prediction, positioned horizontally according to its \ac{shap} value, while the color encodes the original feature value from low (blue) to high (red). This dual encoding makes it possible to discern not only the importance but also the directionality and consistency of each feature's effect.

For the parameter \(a_1\), which governs mean reversion, the feature importance plot identifies \textit{curvature\_10y20y30y}, representing the long-end curvature of the yield curve, as the most influential feature, followed by slope measures such as \textit{slope\_3m10y} and \textit{slope\_5y30y}. The summary plot shows that high curvature and steep slope values, indicated by red dots, are associated with negative \ac{shap} values. This suggests that a pronounced long-end curvature and a steep yield curve reduce the predicted mean reversion parameter. Such a relationship aligns with financial intuition: when the market anticipates substantial future rate movements, the short rate becomes less anchored, leading to weaker mean reversion.

Across the parameters \(\sigma_1\) to \(\sigma_7\), which represent piecewise volatilities, a consistent pattern emerges. In nearly all cases, \textit{curvature\_10y20y30y} is the single most important predictor. This finding implies that the model perceives the \ac{shap}e of the far end of the yield curve as the key indicator for volatility across the entire term structure. The neural network thus interprets long-end curvature as a proxy for systemic uncertainty in interest rates. At the same time, the model demonstrates adaptive behavior in weighting secondary features. For short-term volatilities (\(\sigma_1, \sigma_2\)), features describing the immediate yield curve \ac{shap}e, such as \textit{PC\_Curvature} and slope measures, gain importance. In contrast, for long-term volatility (\(\sigma_7\)), the \textit{MOVE\_VIX\_Ratio} becomes a critical factor, reflecting the model's capacity to incorporate cross-asset market stress into its predictions.

The \ac{shap} beeswarm plot provides a comprehensive visual summary of these interactions. Features are ranked by their average impact, displayed along the y-axis, while the x-axis indicates how strongly a given feature value influences a specific prediction. Positive \ac{shap} values correspond to features that increase the predicted parameter, whereas negative values indicate a downward effect. The color gradient adds a third layer of interpretation by mapping original feature values, thereby revealing relationships such as positive or negative correlations between a feature's level and its effect on the output. The density and spread of the dots further illustrate the variability and context-dependence of each feature's influence, enabling the identification of stable versus conditional effects.

When examining all predicted parameters together, the beeswarm plots reveal a consistent and economically interpretable decision structure within the neural network. The long-end yield curve curvature emerges as the universal driver: high curvature values tend to decrease the mean reversion parameter \(a_1\) while simultaneously increasing volatility parameters \(\sigma_i\). This dual effect indicates that the model has learned to associate a more curved yield curve with heightened market uncertainty and weaker rate anchoring. Beyond this primary driver, the model differentiates between parameter-specific influences. For mean reversion, the dominant secondary features are yield curve slopes, whose high values lower \(a_1\) in line with the expectation that steep curves imply rising future rates. For short-term volatilities, the model prioritizes immediate yield curve factors, while for long-term volatility, it emphasizes inter-market stress measures such as the \textit{MOVE\_VIX\_Ratio}.

A particularly notable finding arises from the non-linear relationship between the \linebreak \textit{MOVE\_VIX\_Ratio} and long-term volatility \(\sigma_7\). High values of this ratio, indicative of bond market stress, are often associated with negative \ac{shap} values, implying that periods of extreme market tension can precede suppressed long-term volatility. This pattern reflects complex, regime-dependent market dynamics, such as the “flight-to-quality” phenomenon or central bank interventions following crises. Finally, the principal component features (\textit{PC\_Level}, \textit{PC\_Slope}, \textit{PC\_Curvature}) behave in accordance with financial expectations. For instance, higher \textit{PC\_Level} values, representing generally higher interest rates, correspond to higher predicted volatilities, consistent with empirical observations that volatility tends to increase in high-rate environments. These findings confirm that the model has successfully internalized financially coherent relationships and interprets engineered features in an economically meaningful manner.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_importance_a_1.png}
	\caption{\ac{shap} Feature Importance for $\alpha$.}
	\label{fig:shap_importance_a_1}
\end{figure}
\foreach \i in {1,...,7}{
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{images/appendix/SHAP_importance_sigma_\i.png}
        \caption{\ac{shap} Feature Importance for $\sigma_\i$.}
        \label{fig:shap_importance_sigma_\i}
    \end{figure}
}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{images/appendix/SHAP_summary_a_1.png}
	\caption{\ac{shap} Feature Summary for $\alpha$.}
	\label{fig:shap_summary_a_1}
\end{figure}
\foreach \i in {1,...,7}{
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{images/appendix/SHAP_summary_sigma_\i.png}
        \caption{\ac{shap} Feature Importance for $\sigma_\i$.}
        \label{fig:shap_summary_sigma_\i}
    \end{figure}
}

\subsection{Code and Data Availability}
For the purposes of transparency and to facilitate further research, the complete source code implementing the methodologies, experiments, and analyses detailed within this thesis has been made publicly available. The project repository is hosted on GitHub and can be accessed via the following URL: \linebreak \url{https://github.com/skyi28/Calibration-of-GSR-Models-using-Neural-Networks}

It is important to note that the repository contains only the source code and not the underlying market data utilized in the empirical study. The dataset, which was gathered from the Bloomberg Terminal, is not provided due to the proprietary nature of the information and the restrictive terms of the Bloomberg data license agreement, which expressly forbids the redistribution of its data.