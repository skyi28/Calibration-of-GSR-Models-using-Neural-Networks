The calibration of interest rate models is a cornerstone of quantitative finance, with traditional methods like the \ac{lm} algorithm representing a standard approach. While robust, these methods can be computationally intensive and may, under certain conditions, exhibit parameter instabilities. This thesis addresses these potential limitations by developing and evaluating a \ac{nn} for calibrating the one-factor \ac{hw} model. The research objective is to provide a comparison between this machine learning-based approach and traditional optimization techniques, focusing on out-of-sample pricing accuracy, computational efficiency, and parameter stability. Unlike prior studies that often train \ac{nn} to replicate the outputs of existing optimizers, this thesis implements a framework where the \ac{nn} is trained to directly minimize the pricing error between model-implied and market-observed swaption volatilities.

A \ac{nn} is trained on a dataset of European \ac{atm} swaptions from the \ac{eur} market, covering the period from June to August 2025. The feature set is engineered from yield curve data using \ac{pca} to derive level, slope, and curvature factors, and is augmented with external market indicators. A hold-out set is introduced, which enforces a strict out-of-sample evaluation for both the \ac{nn} and three variations of the \ac{lm} algorithm. Performance is quantified using the \ac{rmse}, calibration runtime, and parameter responses to market shocks.

The results indicate that the \ac{nn}-based approach and the \ac{lm} strategies exhibited distinct performance characteristics within the framework of this study. Regarding out-of-sample pricing accuracy, the \ac{nn} yielded a mean \ac{rmse} of 4.33~\ac{bps}, while the best-performing \ac{lm} strategy resulted in a mean of 6.05~\ac{bps}; the error distribution for the \ac{nn} also showed lower variance across the test period. A substantial difference in computational efficiency was observed. The \ac{nn}, operating as a pre-trained model, performed calibration in approximately 4 milliseconds. In contrast, the \ac{lm} methods, which conduct a full optimization for each market snapshot, required notably more time. In the analysis of parameter dynamics, the \ac{nn}-derived parameters remained stable and responded coherently to simulated market shocks. The parameters calibrated by the \ac{lm} algorithm were observed to be more sensitive to the choice of initialization strategy.

This study concludes that an end-to-end trained \ac{nn} provides a viable framework for the calibration of the one-factor \ac{hw} model, exhibiting favorable characteristics in terms of accuracy, stability, and computational speed under the conditions of this research. The findings suggest that direct error minimization is a promising training paradigm for financial surrogate models, potentially enabling them to identify robust parameter mappings through a global, data-driven learning process.

\textit{JEL Classification: G13, C45}