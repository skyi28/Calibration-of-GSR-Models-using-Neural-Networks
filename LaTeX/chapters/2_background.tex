Before delving into the comparative analysis that forms the core of this thesis, it is essential to establish a solid theoretical foundation. This chapter provides a comprehensive overview of the key financial instruments and mathematical models that underpin this research. The discussion begins with the fundamental characteristics of swaptions and their importance in the interest rate derivatives market. It then proceeds to detail the one-factor Hull-White model, explaining its mathematical specification, pricing solutions, and the conventional calibration process using numerical optimization methods. Finally, the chapter introduces the foundational concepts of neural networks, outlining the architectural and learning principles that enable their application as a modern calibration tool.

\subsection{Swaptions: Definition, Characteristics, and Financial Relevance} \label{swaptions}
A swaption, or swap option, is a derivative contract that grants the holder the right, but not the obligation, to enter into an interest rate swap at a predetermined date in the future and under predefined terms. Specifically, a payer swaption gives the holder the right to enter into a swap as the fixed-rate payer (and floating-rate receiver), while a receiver swaption gives the right to enter as the fixed-rate receiver (and floating-rate payer). Swaptions thus represent a combination of option and interest rate derivative characteristics and are a fundamental component of the interest rate derivatives market \parencite[pp.~19--20]{brigo2006interest}.

Swaptions can be classified by the style of exercise they permit, which significantly influences their valuation complexity and hedging strategies.

European swaptions are the most common type and allow exercise only at a single specified future date \parencite[p.~19]{brigo2006interest}, typically coinciding with the start of the underlying swap. Their valuation is relatively tractable and often performed using closed-form solutions such as Black’s formula \parencite{black1976pricing}.

American swaptions allow exercise at any time up to the expiry date \parencite{karlsson2016exercisepolicies}. This added flexibility increases the option’s value but also its computational complexity. Due to the path-dependency of optimal exercise, American swaptions are typically valued using numerical methods such as lattice models \parencite{gurrieri2009calibration} or finite-difference \parencite{longstaff2001valuingamericanoptions} schemes.

Bermudan swaptions occupy an intermediate position, allowing the holder to exercise the option on a set of predetermined dates, often corresponding to coupon dates of the underlying swap. Bermudan swaptions are widely traded \parencite{karlsson2016exercisepolicies} and are especially relevant for callable and cancellable structures in interest rate risk management \parencite[p.~22]{rebonato2004interest}. Valuation requires techniques that account for multiple early exercise opportunities, such as backward induction \parencite{karlsson2016exercisepolicies} or least-squares Monte Carlo methods \parencite{longstaff2001valuingamericanoptions}.

Asian swaptions are path-dependent options where the payoff depends on the average of the underlying swap rate over a certain period rather than a single rate observation at expiry. Because of this averaging feature, their valuation requires models that account for the entire path of interest rates rather than just their terminal value. Analytical or semi-analytical approximations—such as volatility expansion techniques can be used to price them \parencite{baaquie2010asianswaptions}.

Each of these exercise styles leads to distinct pricing and risk management considerations and determines the appropriate mathematical framework and numerical method for valuation. In addition to exercise style, a swaption is characterized by several structural and market parameters that determine its valuation and risk profile:

Option Maturity (Expiry): This denotes the time until the swaption can be exercised. It reflects the horizon over which uncertainty about future interest rate movements is resolved. Longer maturities generally imply greater sensitivity to the dynamics of the underlying yield curve and the volatility of interest rates.

Underlying Swap Tenor: The tenor is the length of the interest rate swap that begins upon swaption exercise \parencite[p.~19]{brigo2006interest}. For example, a 5yX10y swaption refers to an option that expires in five years and, if exercised, initiates a ten-year interest rate swap. The swap tenor affects the duration and convexity of the position, and thus its sensitivity to changes in the yield curve.

Strike Rate: The strike rate is the fixed interest rate agreed upon in the swaption contract. At expiry, the decision to exercise the swaption depends on the relationship between this strike and the prevailing forward swap rate for the corresponding maturity and tenor \parencite[p.~239]{brigo2006interest}.

Premium: The premium, or market price, of the swaption reflects the cost of acquiring the optionality embedded in the contract. It is influenced by all of the above parameters, as well as prevailing interest rates, the volatility environment, and the exercise style.

Implied Volatility: Swaptions are quoted in terms of implied volatility, which is extracted from market prices using inversion techniques based on models such as Black’s formula \parencite[p.~3]{hohmann2015newnormal}. Implied volatility encapsulates the market’s expectation of future fluctuations in interest rates and plays a central role in option pricing and risk management. Market practitioners often use a two-dimensional volatility cube, where implied volatilities vary with both option expiry and swap tenor \parencite[p.~1]{suo2009volatility}.

Swaptions serve multiple purposes in financial markets, ranging from risk management to model calibration and investment strategies. They are fundamental tools used by financial institutions, borrowers, and specialized agencies to manage future interest rate risk. In particular, Bermudan swaptions are highly relevant for hedging callable and cancellable structures in the over-the-counter (OTC) market. For example, government-sponsored mortgage agencies in the USD market often purchase European and Bermudan swaptions to hedge exposure created by callable debt funding programs \parencite[p.~22]{rebonato2004interest}. By granting the holder the right, but not the obligation, to enter into an interest rate swap at a predetermined future date, swaptions create optionality that protects against adverse movements in interest rates and makes sure that a company payments at each payment date have been capped to the fixed rate \parencite[p.~17]{brigo2006interest}. The payoff of a European swaption can also be replicated by a continuously rebalanced portfolio of zero-coupon bonds, which provides valuable insight for managing the risk of positions in the derivative itself \parencite[p.~241]{brigo2006interest}. Additionally, swaptions are used to manage volatility risk, which can be the largest component of risk in a derivative deal \parencite[p.~242]{brigo2006interest}.

Beyond risk management, swaptions are critical for market pricing, valuation, and calibration of interest rate models \parencite[pp.~132--136]{brigo2006interest}.  Swaption prices are essential inputs for calibrating models such as the Hull-White model \parencite{kladivko2023mlehullwhite} and the LIBOR Market Model (LMM)  for instance, calibrating the LMM to the swaption market allows parameters like instantaneous correlations to be determined \parencite[p.~290]{brigo2006interest}.

Swaptions are also used to adjust financing costs and optimize yields. Investors seeking yield enhancement and issuers in search of advantageous funding rates may sell swaption-type optionality, while swaptions or swaption-like features are frequently embedded in more complex financial products or callable structures offered to investors \parencite[p.~22]{rebonato2004interest}.

Swaptions represent highly versatile derivatives within interest rate markets, serving several critical functions. Their primary application lies in risk management, where they provide market participants with effective tools to hedge against adverse interest rate movements and manage volatility exposure. Moreover, swaptions play a pivotal role in the calibration of interest rate models; market-implied volatilities from swaption prices are essential inputs for sophisticated frameworks like the Hull-White and LIBOR Market Models. Finally, beyond their utility in hedging and valuation, these instruments are strategically employed by investors to enhance portfolio yields and by issuers to optimize the costs associated with debt financing.

\subsection{Bootstrapping the Zero-Coupon Yield Curve from Swap Rates}
\label{subsec:bootstrap_zero_curve}
The construction of a zero-coupon yield curve from market swap rates is an essential step in interest rate modeling. Since swap contracts are among the most liquid instruments across maturities, they serve as primary inputs for determining the risk-free term structure. The goal of the bootstrapping process is to derive discount factors $\{P(t_0, T_i)\}_{i=1}^N$ or equivalently continuously compounded zero rates $\{z(T_i)\}_{i=1}^N$ that are consistent with observed market swap rates $\{R_i\}_{i=1}^N$ \parencite[pp.~84--86]{hull2015optionsfutures}.

\subsubsection{Preliminaries and Definitions}
Let $t_0$ denote the valuation date and $\{T_i\}_{i=1}^N$ the maturities of the quoted par swap rates. Each swap with maturity $T_i$ exchanges a fixed rate $R_i$ for a floating rate indexed to a short-term reference rate (e.g., EURIBOR 6M) at payment dates $\{t_{i,k}\}_{k=1}^{n_i}$ on the fixed leg.

The discount factor $P(t_0, t)$ represents the present value at time $t_0$ of one unit of currency to be received at time $t$. The continuously compounded zero rate $z(t)$ is defined by
\begin{equation}
	P(t_0, t) = e^{-z(t)(t - t_0)},
	\label{eq:discount_zero_relation}
\end{equation}
which implies
\begin{equation}
	z(t) = -\frac{1}{t - t_0} \ln P(t_0, t).
	\label{eq:zero_rate_definition}
\end{equation}

\subsubsection{Valuation of a Par Swap}
A standard fixed-for-floating interest rate swap can be interpreted as the difference between a fixed-rate bond and a floating-rate note. The present value of the fixed leg at inception is given by
\begin{equation}
	PV_{\text{fixed}}(t_0) = R_i \sum_{k=1}^{n_i} \alpha_{i,k} P(t_0, t_{i,k}),
	\label{eq:fixed_leg_pv}
\end{equation}
where $\alpha_{i,k}$ denotes the accrual factor for the period $[t_{i,k-1}, t_{i,k}]$ under the fixed-leg day count convention.

The present value of the floating leg for a par swap (i.e., at inception) equals
\begin{equation}
	PV_{\text{float}}(t_0) = 1 - P(t_0, T_i),
	\label{eq:float_leg_pv}
\end{equation}
under the assumption that the first floating coupon is set at par.

At inception, the par swap has zero net present value, so that
\begin{equation}
	PV_{\text{fixed}}(t_0) = PV_{\text{float}}(t_0),
	\label{eq:par_swap_condition}
\end{equation}
which yields the par swap relation
\begin{equation}
	R_i \sum_{k=1}^{n_i} \alpha_{i,k} P(t_0, t_{i,k}) = 1 - P(t_0, T_i).
	\label{eq:par_swap_equation}
\end{equation}

\subsubsection{Recursive Bootstrapping Procedure}
Equation~\eqref{eq:par_swap_equation} establishes a nonlinear relationship between the par swap rate $R_i$ and the discount factors $\{P(t_0, t_{i,k})\}$. If discount factors up to $T_{i-1}$ are already known from previously bootstrapped maturities, one can rearrange \eqref{eq:par_swap_equation} to isolate the unknown discount factor $P(t_0, T_i)$ as
\begin{equation}
	P(t_0, T_i) = \frac{1 - R_i \sum_{k=1}^{n_i - 1} \alpha_{i,k} P(t_0, t_{i,k})}
	{1 + R_i \alpha_{i,n_i}}.
	\label{eq:bootstrap_equation}
\end{equation}

The procedure thus proceeds recursively:
\begin{enumerate}
	\item Initialize the curve with known short-term instruments (e.g., deposits or short-dated FRAs) to obtain discount factors for the first maturities.
	\item For each subsequent maturity $T_i$, solve \eqref{eq:bootstrap_equation} for $P(t_0, T_i)$ using previously determined discount factors.
	\item Continue until the entire maturity spectrum $\{T_i\}_{i=1}^N$ is covered.
\end{enumerate}

Once the discount factors are determined, the continuously compounded zero rates follow from \eqref{eq:zero_rate_definition}.

\subsubsection{Interpolation and Continuity of the Term Structure}
The market provides only a discrete set of maturities $\{T_i\}$. To obtain a continuous term structure, the discount factors between quoted maturities are interpolated. A common and arbitrage-free choice is to interpolate linearly in log-discount space, i.e.,
\begin{equation}
	\ln P(t_0, t) = (1 - \lambda) \ln P(t_0, T_j) + \lambda \ln P(t_0, T_{j+1}),
	\quad t \in [T_j, T_{j+1}],
	\label{eq:log_linear_interpolation}
\end{equation}
where
\begin{equation}
	\lambda = \frac{t - T_j}{T_{j+1} - T_j}.
\end{equation}
This ensures that the discount function $P(t_0, t)$ is continuous, positive, and monotonically decreasing, thereby avoiding arbitrage opportunities.

The bootstrapping method yields a set of discount factors $\{P(t_0, T_i)\}$ that are internally consistent with observed swap rates $\{R_i\}$. The corresponding zero-coupon yield curve $\{z(T_i)\}$ satisfies:
\begin{equation}
	z(T_i) = -\frac{1}{T_i - t_0} \ln P(t_0, T_i),
\end{equation}
providing a continuous, arbitrage-free representation of the term structure of interest rates suitable for valuation, risk management, and model calibration.


\subsection{Short Rate Models} \label{short_rate_models}
Short-rate models are a class of term structure models that describe the evolution of interest rates by modeling the dynamics of the instantaneous short-term interest rate, denoted by $r_{t}$. These models serve as the foundational framework for pricing interest rate derivatives, particularly those with path-dependent features or early exercise options such as American-style options where traditional models like Black's formula fall short due to their static assumptions. The short rate $r_{t}$ represents the risk-free rate applicable over an infinitesimally short time interval and evolves according to a stochastic differential equation (SDE), typically under the risk-neutral measure. Under this framework, the present value of future cash flows is derived by discounting at the short-rate path, allowing for a consistent valuation across a variety of financial instruments \parencite[pp.~706--735]{hull2015optionsfutures}.

Short-rate models are flexible in that they can be constructed either to derive the term structure as a model outcome (as in equilibrium models) or to fit the observed term structure exactly (as in no-arbitrage models). Regardless of the approach, these models typically require numerical methods for implementation, including lattice-based techniques (e.g., trinomial trees) or Monte Carlo simulation, especially in the case of American or exotic derivatives. A key aspect of the usability of the model is its calibration to market data, which involves estimating parameters - such as mean reversion speed ($a$) and volatility ($\sigma$)—from prices of liquid instruments such as caps and swaps. In the post-2008 financial environment, the shift towards OIS discounting has led to the need for multiple-curve modeling frameworks, often requiring the simultaneous calibration of separate short-rate processes for both the discounting curve (e.g., OIS) and the forwarding curve (e.g., LIBOR) \parencite[pp.~706--735]{hull2015optionsfutures}.

\subsubsection{Equilibrium Models} \label{equilibrium_models}
Equilibrium short-rate models are constructed by specifying a stochastic process for the short-term interest rate based on underlying economic principles, such as investor preferences and macroeconomic fundamentals. Unlike arbitrage-free models, equilibrium models derive the entire term structure of interest rates as an endogenous output of the model, rather than fitting it directly to observed market data. A distinguishing feature is that the drift term in the short rate's SDE is generally independent of time, reflecting long-term economic forces rather than market-implied expectations.

One of the earliest examples, the Rendleman–Bartter model, assumes that the short rate follows a geometric Brownian motion without mean reversion, which makes it analogous to equity price modeling, but unsuitable for interest rates due to its unbounded and potentially negative outcomes \parencite[p.~708]{hull2015optionsfutures}. In contrast, the Vasicek model introduces mean reversion, modeling the short rate as an Ornstein-Uhlenbeck process. Although analytically tractable and capable of capturing the tendency of interest rates to revert to a long-term mean, it allows for negative interest rates, a feature that may be undesirable in certain market environments \parencite[pp.~708--709]{hull2015optionsfutures}. The Cox–Ingersoll–Ross (CIR) model addresses this limitation by specifying that the diffusion term is proportional to the square root of the short rate, thereby ensuring non-negativity under suitable parameter conditions \parencite[p.~710]{hull2015optionsfutures}.

Despite their elegance and analytical appeal, equilibrium models are often limited in practical applications because they do not guarantee consistency with the observed initial term structure. As such, they are more suitable for theoretical analysis and long-term economic forecasting than for precise pricing and hedging of interest rate derivatives in the current market environment \parencite[pp.~707--714]{hull2015optionsfutures}.

\subsubsection{No Arbitrage Models} \label{no_arbitrage_models}
No-arbitrage short-rate models are designed to exactly match the observed term structure of interest rates at inception by construction. These models ensure that there are no arbitrage opportunities in the pricing of fixed-income securities and derivatives by embedding the initial yield curve as an explicit input. The drift component of the stochastic differential equation of the short rate in these models is typically time dependent, calibrated such that the models' generated bond prices align with market prices at time zero. As a result, no-arbitrage models are preferred for pricing and risk management in practice, especially in environments where accuracy in capturing the term structure is essential \parencite[pp.~714--715]{hull2015optionsfutures}.

The Ho–Lee model, introduced in 1986, was the first short -rate model to incorporate the no-arbitrage principle. It assumes constant volatility and introduces a time-dependent drift term to fit the initial yield curve, resulting in a normally distributed short rate. While simple and analytically tractable, the Ho–Lee model allows for negative interest rates and constant volatility across all maturities \parencite[pp.~715--716]{hull2015optionsfutures}. To address the limitations of both the Vasicek and Ho–Lee models, the Hull–White one-factor model combines mean reversion with time-dependent drift, allowing for greater flexibility in fitting the term structure and interest rate volatility \parencite[pp.~716--718]{hull2015optionsfutures}.

Further refinements include multi-factor extensions, such as the Hull–White two-factor model, which captures changes in both the level and slope of the yield curve through the interaction of two stochastic drivers. These enhancements are especially useful in capturing the empirical behavior of interest rates and in pricing complex derivatives such as Bermudan swaptions. The requirement to calibrate these models to market-observed instruments like caps, floors, and swaptions is central to their application, and they are widely used in practice for valuation, hedging, and risk management of interest rate products \parencite[p.~719]{hull2015optionsfutures}.

\subsubsection{Gaussian Short Rate Models} \label{gaussian_short_rate_models}
Gaussian short rate models form a specific subclass within the broader family of short rate models in which the evolution of the short-term interest rate—or a transformation thereof—is governed by a stochastic process with normally distributed increments. The defining feature of these models is the assumption that the short rate $r_t$ (or its change) evolves under a Brownian motion $dz_t$ with constant or time-dependent volatility, leading to normally distributed outcomes \parencite[p.~53]{brigo2006interest}. As such, these models offer analytical tractability and closed-form solutions for bond prices and some derivative instruments. However, they also imply the theoretical possibility of negative interest rates, which may or may not be consistent with the prevailing market environment or the model’s intended application \parencite[pp.~719--720]{hull2015optionsfutures}.

Formally, a Gaussian short rate model is characterized by a stochastic differential equation (SDE) of the general form:

\begin{equation}
	dr_t = \mu(t, r_t) dt + \sigma(t) dz_t,
\end{equation}


where $\mu(t, r_t)$ is the drift term, $\sigma(t)$ is the volatility function (possibly constant or time-dependent), and $dz_t$ denotes the increment of a standard Wiener process under the risk-neutral measure \parencite[p.~53]{brigo2006interest}. Because the increments of $dz_t$ are normally distributed, so too are the increments of $r_t$, provided that the transformation applied to the short rate is linear.

Prominent examples of Gaussian short rate models include the Vasicek, Ho–Lee, and Hull–White (one-factor) models, each of which was discussed in prior sections. While differing in their specification of the drift and volatility terms, all three share the core Gaussian property of allowing for symmetric, unbounded movements in the short rate. This feature enhances analytical tractability—particularly in deriving closed-form solutions for zero-coupon bond prices—but also permits negative interest rates, a theoretical artifact that gained practical relevance in post-crisis monetary regimes where nominal yields fell below zero in several developed markets.

Importantly, Gaussian short rate models stand in contrast to lognormal models such as the Black–Derman–Toy and Black–Karasinski models, which restrict the short rate to remain strictly positive by modeling either the logarithm of the rate or its multiplicative structure . While these alternative models circumvent the issue of negative rates, they often sacrifice analytical tractability in favor of numerical implementation \parencite[p.~718]{hull2015optionsfutures}.

This master thesis will focus in the following chapters on the one-factor Hull–White model, a prominent Gaussian short rate model that enhances the classical Vasicek formulation by introducing a time-dependent drift term to ensure an exact fit to the initial term structure of interest rates. Its balance between flexibility, analytical tractability, and market-consistency makes it a standard benchmark in both academic research and practical applications, particularly for the valuation and risk management of interest rate derivatives.

\subsection{The One-Factor Hull-White Model}
The one-factor Hull-White model represents a pivotal advancement in interest rate modeling, combining analytical tractability, empirical flexibility, and arbitrage-free consistency. As detailed in Hull and White’s seminal 1990 paper \parencite{hull1990pricing}, the model extends the classical Vasicek framework by introducing time-dependent parameters, enabling exact calibration to the observed initial term structure of interest rates. As explained in Section~\ref{gaussian_short_rate_models}, the Hull-White model falls within the class of Gaussian short rate models due to its assumption of normally distributed short rate dynamics. Furthermore, as discussed in Section~\ref{no_arbitrage_models}, it enforces market consistency by incorporating a time-varying drift term.

The Hull-White model offers a favorable balance between model realism and computational tractability. Its ability to fit the observed term structure exactly, model time-varying volatilities, and provide analytic solutions for a wide class of derivatives positions it as a standard tool in fixed income modeling. It is particularly well-suited for applications such as: Pricing of swaptions, caps, and callable bonds, risk management and Value-at-Risk (VaR) calculations and simulation of future interest rate scenarios for balance sheet modeling. The model’s allowance for negative interest rates, due to its Gaussian nature, has become a point of practical relevance in recent years, as monetary policy in several advanced economies has driven rates below zero.

\subsubsection{Mathematical Specification}
The Hull-White model is often referred to as the \textit{extended Vasicek model}, formalized as follows under the risk-neutral measure \( \mathbb{Q} \):

\begin{equation}
	dr_t = \left[ \theta(t) - a r_t \right] dt + \sigma(t) dz_t,
\end{equation}

where:
\begin{table}[H]
	\centering
	\caption{Notation for the Hull--White Short Rate Model}
	\begin{tabular}{ll}
		\toprule
		\textbf{Symbol} & \textbf{Meaning}                                                     \\
		\midrule
		$r_t$           & Instantaneous short rate at time $t$                                 \\
		$\theta(t)$     & Deterministic function fitted to match the initial term structure    \\
		$a$             & Constant mean reversion speed ($a > 0$)                              \\
		$\sigma(t)$     & Time-dependent volatility function                                   \\
		$dz_t$          & Standard Brownian motion under the risk-neutral measure $\mathbb{Q}$ \\
		\bottomrule
	\end{tabular}
\end{table}

This formulation allows the short rate to mean-revert toward a time-varying level \( \theta(t)/a \), ensuring flexibility in capturing the shape of the initial yield curve while retaining a parsimonious structure. The presence of \( \sigma(t) \) permits the model to reflect changes in the term structure of interest rate volatilities, which is crucial for accurately pricing interest rate derivatives.

\subsubsection{Bond Pricing and Functional Solutions}
One of the key strengths of the Hull-White model is its closed-form solution for zero-coupon bond prices. The price \( P(r_t, t, T) \) of a zero-coupon bond maturing at time \( T \), given the short rate at time \( t \), is given by:

\begin{equation}
	P(r_t, t, T) = \exp\left[ A(t, T) - B(t, T) r_t \right],
\end{equation}

where the deterministic functions \( A(t, T) \) and \( B(t, T) \) are defined as:
\begin{equation}
	B(t, T) = \frac{1}{a} \left(1 - e^{-a(T - t)}\right),
\end{equation}
\begin{equation}
	A(t, T) = \int_t^T \left[ \theta(s) B(s, T) - \frac{1}{2} \sigma(s)^2 B(s, T)^2 \right] ds.
\end{equation}

These solutions emerge from solving the associated partial differential equation for contingent claim prices, under the assumption of a risk-neutral market and bounded market price of risk. The functional form of \( A(t, T) \) links directly to the model's ability to replicate the observed yield curve.

\subsubsection{Option Pricing}
A notable advantage of the Hull-White model is its analytic tractability for European-style options on bonds, which makes it especially valuable for practical implementation. The lognormality of bond prices under the model’s Gaussian short rate dynamics allows for closed-form expressions for European bond option prices. For instance, a European call option on a discount bond with maturity \( T \) and exercise at \( t_1 < T \) has a pricing formula structurally analogous to the Black formula, with a volatility term given by:

\begin{equation}
	\sigma_P = \sigma(t_1) B(t_1, T),
\end{equation}

where the bond price volatility is independent of \( r_t \), enabling straightforward evaluation of the option price. Options on coupon-bearing instruments can be priced via decomposition into portfolios of zero-coupon bond options.

For American-style options or more complex interest-rate-contingent claims, numerical techniques such as finite-difference methods or lattice-based approaches (e.g., trinomial trees) are required to solve the underlying PDE.

\subsubsection{Swaption Pricing} \label{swaption_pricing}
In the one-factor Hull-White model, European swaptions - options that grant the right to enter into an interest rate swap at a predetermined fixed rate -can be priced analytically using a technique known as \textit{Jamshidian’s decomposition} \parencite{jamshidian1989decomposition}. This method is particularly powerful for one-factor models, where all coupon bond prices at a given time are deterministic functions of the short rate.

The fundamental insight of Jamshidian’s approach is that a payer (or receiver) swaption, which is an option on a portfolio of fixed payments, can be decomposed into a portfolio of options on individual zero-coupon bonds. Since the Hull-White model provides closed-form solutions for European options on zero-coupon bonds (as discussed in the previous subsection), this decomposition facilitates an analytical expression for the swaption price.

A European payer swaption gives the holder the right, but not the obligation, to enter at maturity \( T \) into a fixed-for-floating interest rate swap where fixed payments of \( K \) occur at payment dates \( T_1, T_2, \dots, T_n \). Let \( P(t, T_i) \) denote the price at time \( t \) of a zero-coupon bond maturing at \( T_i \), and let \( \alpha_i \) denote the accrual factors (e.g., 0.5 for semiannual payments). The value at time \( t \) of the fixed leg of the swap is:

\begin{equation}
	\text{FixedLeg}(t) = K \sum_{i=1}^n \alpha_i P(t, T_i).
\end{equation}

The value of the floating leg at time \( t \) equals one minus the price of a discount bond maturing at the swap start date \( T_0 \), assuming the swap is par at initiation. The swaption payoff at expiry \( T \) is therefore:

\begin{equation}
	\max\left( \sum_{i=1}^n \alpha_i P(T, T_i) (K - S(T)), 0 \right),
\end{equation}

where \( S(T) \) is the par swap rate at time \( T \).

Jamshidian's decomposition allows this multi-cash-flow option to be re-expressed as a portfolio of individual bond options. Specifically, there exists a unique short rate \( r^* \) such that the present value of the fixed leg equals the strike value \( K \sum_{i=1}^n \alpha_i P(T, T_i) \). Formally, we define \( r^* \) as the root of the equation:

\begin{equation}
	\sum_{i=1}^n \alpha_i P(r^*, T, T_i) = \sum_{i=1}^n \alpha_i P(T, T_i),
\end{equation}

where \( P(r^*, T, T_i) \) is the model-implied bond price expressed as a function of the short rate. Once \( r^* \) is found, the swaption payoff can be decomposed into a sum of bond option payoffs with the same exercise date \( T \):

\begin{equation}
	\text{Swaption}(t) = \sum_{i=1}^n \alpha_i \cdot \text{Option}(P(t, T_i), K_i),
\end{equation}

where each \( K_i = P(r^*, T, T_i) \) acts as the strike price for the bond option maturing at \( T_i \).

In the Hull-White model, the price of a European call option on a zero-coupon bond with maturity \( T_i \), strike \( K_i \), and exercise at \( T \) is given by the closed-form formula:

\begin{equation}
	C(t) = P(t, T_i) N(d_1) - K_i P(t, T) N(d_2),
\end{equation}

with
\begin{equation}
	d_1 = \frac{\ln\left( \frac{P(t, T_i)}{K_i P(t, T)} \right)}{\sigma_P} + \frac{1}{2} \sigma_P, \quad
	d_2 = d_1 - \sigma_P,
\end{equation}

and \( \sigma_P \) being the bond price volatility given by:

\begin{equation}
	\sigma_P^2 = \int_t^T \left( \sigma(s) B(s, T_i) - \sigma(s) B(s, T) \right)^2 ds.
\end{equation}

The full swaption price is obtained by summing the individual bond option prices across all cash flow dates.

\begin{equation}
	\text{Swaption}(t) = \sum_{i=1}^n \alpha_i \left[ P(t, T_i) N(d_1^{(i)}) - K_i P(t, T) N(d_2^{(i)}) \right],
\end{equation}

\begin{equation}
	\text{with} \quad d_1^{(i)} = \frac{\ln\left( \frac{P(t, T_i)}{K_i P(t, T)} \right)}{\sigma_P^{(i)}} + \frac{1}{2} \sigma_P^{(i)}, \quad d_2^{(i)} = d_1^{(i)} - \sigma_P^{(i)},
\end{equation}

\begin{equation}
	\text{and} \quad \left( \sigma_P^{(i)} \right)^2 = \int_t^T \left[ \sigma(s) B(s, T_i) - \sigma(s) B(s, T) \right]^2 ds,
\end{equation}

\begin{equation}
	\text{where} \quad K_i = P(r^*, T, T_i),
\end{equation}

\begin{table}[H]
	\centering
	\caption{Notation for Swaption Pricing via Jamshidian's Decomposition}
	\begin{tabular}{ll}
		\toprule
		\textbf{Symbol}        & \textbf{Meaning}                                                           \\
		\midrule
		$P(t, T)$              & Price at time $t$ of a zero-coupon bond maturing at $T$                    \\
		$T$                    & Expiry of the swaption                                                     \\
		$T_i$                  & Payment date of the $i$-th swap cash flow ($i = 1, \dots, n$)              \\
		$\alpha_i$             & Year fraction (accrual factor) between $T_{i-1}$ and $T_i$                 \\
		$r^*$                  & Unique short rate at expiry $T$ solving the bond portfolio equation        \\
		$K_i$                  & Strike price of the bond option for maturity $T_i$, i.e., $P(r^*, T, T_i)$ \\
		$\sigma(s)$            & Instantaneous volatility function of the Hull–White model                  \\
		$B(s, T)$              & Sensitivity function: $B(s, T) = \frac{1 - e^{-a(T - s)}}{a}$              \\
		$\sigma_P^{(i)}$       & Volatility of the bond price ratio $P(t, T_i)/P(t, T)$                     \\
		$d_1^{(i)}, d_2^{(i)}$ & Black-type variables for bond option $i$                                   \\
		$N(\cdot)$             & Standard normal cumulative distribution function                           \\
		\bottomrule
	\end{tabular}
\end{table}

Jamshidian’s decomposition offers significant computational efficiency for one-factor models by reducing a complex multidimensional option pricing problem to a sequence of univariate bond option valuations. Moreover, this approach retains full analytical tractability under the Hull-White framework, allowing for fast and accurate pricing of European swaptions, a critical requirement in both risk management and market calibration contexts.

\subsubsection{Calibration of the Hull-White Model}
The calibration of the one-factor Hull-White model plays a central role in ensuring its effectiveness for interest rate derivative pricing and risk management. As established in previous sections, the Hull-White model provides analytical tractability and the ability to fit the initial yield curve exactly. However, to make the model reflect market-implied prices of interest rate derivatives, particularly caplets and swaptions, its dynamic parameters-mean reversion $a(t)$ and volatility $\sigma(t)$-must be appropriately calibrated.

In practical applications, $a(t)$ and $\sigma(t)$ are commonly assumed to be piecewise constant to simplify the numerical implementation \parencite[p.~7]{gurrieri2009calibration}. However, unconstrained piecewise constant forms can lead to overfitting and parameter instability \parencite[p.~2]{gurrieri2009calibration}. To overcome this, the model is typically calibrated using functional parameterizations, such as logistic forms for $a(t)$ and cubic splines for $\sigma(t)$, to ensure smoothness and stability \parencite[p.~8]{gurrieri2009calibration}.

The calibration process relies on three key inputs: the initial yield curve, which determines $\theta(t)$, market-observed prices or implied volatilities of caplets and swaptions, and a well-defined objective function, often the sum of squared differences between model and market prices or implied volatilities. In particular, European swaptions serve as the primary calibration instruments. These are priced analytically via Jamshidian’s decomposition (see Section~\ref{swaption_pricing}), allowing the decomposition of the swaption payoff into a sum of bond options. The analytical tractability of this decomposition facilitates efficient model evaluation during optimization.

Three main strategies are typically employed to calibrate the Hull-White model parameters:

1. Fixed Mean Reversion: In this approach, the mean reversion parameter $a(t)$ is fixed to a pre-determined constant value, often based on empirical considerations or desired product sensitivities (e.g., Bermudan swaptions). The volatility parameter $\sigma(t)$ is then optimized to fit a subset of swaption prices or implied volatilities. While this method simplifies the calibration and allows for fast implementation, it can be sensitive to market regime changes and may offer poor global fit if the chosen $a$ does not reflect current market conditions \parencite[p.~13]{gurrieri2009calibration}.

2. Two-Step Estimation: This method separates the calibration of $a(t)$ and $\sigma(t)$. The mean reversion $a(t)$ is first estimated using an approximation method (e.g., the SMM approximation), which relates swaption implied volatilities in a manner independent of $\sigma(t)$. Once $a(t)$ is estimated and fixed, $\sigma(t)$ is calibrated to fit analytical swaption prices. This approach offers a balance between robustness and speed, making it suitable for environments requiring frequent recalibration \parencite[pp.~13--14]{gurrieri2009calibration}.

3. Simultaneous Optimization: This strategy involves a joint, multi-dimensional optimization of both $a(t)$ and $\sigma(t)$ to minimize the misfit between model prices and market prices of swaptions. While computationally more intensive, this method typically yields the highest fit quality across a broader range of instruments. Constraints and functional parameterizations are critical to ensure numerical stability and prevent overfitting \parencite[p.~14]{gurrieri2009calibration}.

The choice of calibration instruments significantly impacts the stability and accuracy of the resulting parameters. Two principal approaches are distinguished \parencite[p.~16]{gurrieri2009calibration}:

1. Local (Subset) Calibration: Calibration is restricted to a limited set of co-terminal swaptions (e.g., those maturing at 10Y or 20Y). This reduces the dimension of the optimization problem and allows for good fits on selected instruments, but can result in instability and poor performance on non-calibrated instruments, especially if time-dependent mean reversion is used \parencite[pp.~16--24]{gurrieri2009calibration}.

2. Global (Full Matrix) Calibration: The entire swaption matrix is used in the calibration procedure. This approach provides superior fitting quality across the full market surface and stabilizes the estimation of time-dependent parameters. When functional forms for $a(t)$ and $\sigma(t)$ are used in combination with global calibration, the model can achieve robust and accurate fits without evidence of overfitting. This thesis employs this global strategy for empirical analyses \parencite[pp.~24--29]{gurrieri2009calibration}.

The actual calibration of the Hull-White model is performed by minimizing the discrepancy between observed market data and model-implied quantities \parencite[p.~2]{alaya2021deep}. In practice, this is typically achieved by minimizing the squared differences between market-observed implied volatilities $\hat{\sigma}^{\text{market}}_{i,j}$ and the model-implied volatilities $\hat{\sigma}^{\text{model}}_{i,j}(\boldsymbol{\theta})$, where $(i,j)$ index the swaption expiry and tenor combinations, and $\boldsymbol{\theta}$ denotes the vector of model parameters (e.g., the values of $a(t)$ and $\sigma(t)$). The optimization problem can be formulated as

\begin{equation}
	\min_{\boldsymbol{\theta}} \; \sum_{(i,j) \in \mathcal{I}} w_{i,j} \left( \hat{\sigma}^{\text{model}}_{i,j}(\boldsymbol{\theta}) - \hat{\sigma}^{\text{market}}_{i,j} \right)^2,
\end{equation}

where $\mathcal{I}$ denotes the set of index pairs corresponding to available market swaptions and $w_{i,j} \geq 0$ are weighting factors that may be used to emphasize certain instruments (e.g., highly liquid swaptions or those near the money)  Alternatively, this objective can be defined in terms of option prices instead of implied volatilities, depending on the availability of reliable market data and numerical stability considerations. The choice of objective function should reflect both the intended use of the model and the data quality. This minimization problem can be solved by algorithms suitable for solving nonlinear least-squares problems such as the Levenberg-Marquardt algorithm (see Section~\ref{lm_algorithm}).

Understanding how the parameters affect model output is essential. The volatility function $\sigma(t)$ predominantly controls the \emph{level} of the implied volatility surface \parencite[p.~9]{gurrieri2009calibration}, while the mean reversion $a(t)$ influences its \emph{shape} \parencite[p.~9]{gurrieri2009calibration}. Specifically, increasing $a(t)$ generally leads to lower implied volatilities and a change in curvature. Accurately capturing market phenomena such as volatility humps necessitates time-dependent mean reversion.

\subsubsection{Levenberg-Marquardt Algorithm} \label{lm_algorithm}
The Levenberg-Marquardt algorithm is a widely used method for solving nonlinear least-squares problems, particularly effective when estimating model parameters in contexts such as the calibration of interest rate models. It aims to minimize the sum of squared residuals between observed data points and the corresponding model predictions \parencite{marquardt1963}.

Given a nonlinear model $f(x; \beta)$ with parameters $\beta = (\beta_1, \dots, \beta_k)$, and observations $\mathbf{y} = (y_1, \dots, y_n)$, the objective is to minimize the cost function

\begin{equation}
	\Phi(\beta) = \sum_{i=1}^{n} \left( y_i - f(x_i; \beta) \right)^2 = \| y - \hat{y}(\beta) \|^2.
\end{equation}

The algorithm interpolates between two classical optimization strategies:

1. Steepest descent method: Effective when far from the minimum but may converge slowly near it.

2. Gauss-Newton method: Efficient near the minimum but may diverge in highly nonlinear regions.

The Levenberg-Marquardt method combines these approaches by updating the parameter vector $\beta$ iteratively. In each iteration, the nonlinear model is linearized via a first-order Taylor expansion:

\begin{equation}
	\hat{y}(\beta + \delta) \approx \hat{y}(\beta) + J \delta,
\end{equation}

where $J$ is the Jacobian matrix of partial derivatives with elements $J_{ij} = \frac{\partial f(x_i; \beta)}{\partial \beta_j}$ evaluated at the current parameter vector. The correction vector $\delta$ is then determined by solving the following regularized normal equation:

\begin{equation}
	(J^\top J + \lambda I)\delta = J^\top (y - \hat{y}(\beta)),
\end{equation}

where $\lambda$ is a damping parameter, and $I$ is the identity matrix.

The damping parameter $\lambda$ controls the balance between the Gauss-Newton and steepest descent directions. For large $\lambda$, the algorithm behaves like a steepest descent method: $(\lambda I)$ dominates the system matrix, ensuring stability. For small $\lambda$, the method approaches the Gauss-Newton direction: $J^\top J$ dominates, allowing for faster convergence.

The value of $\lambda$ is adjusted dynamically during the iteration process:
If the new parameter vector $\beta + \delta$ results in a lower cost function $\Phi$, the step is accepted and $\lambda$ is decreased. If the step does not yield improvement, $\lambda$ is increased, making the algorithm more conservative.

To improve numerical stability and convergence behavior, especially when model parameters have different magnitudes, parameter scaling is often applied. This ensures that step sizes and gradients are appropriately balanced.

Due to its adaptive nature and robustness, the Levenberg-Marquardt algorithm is particularly well suited for the calibration of the Hull-White model, where the objective function is highly nonlinear and sensitive to changes in model parameters.

\subsection{Neural Networks}
Neural networks are computational models inspired by the structure and function of the human brain and its nervous system. Unlike traditional computing systems that operate through explicit programming and deterministic logic, neural networks are designed to learn from data by adjusting internal parameters in response to input stimuli. This adaptive capability enables them to approximate complex, non-linear functions and has led to their widespread use in fields such as image recognition, natural language processing, and financial forecasting.

\subsubsection{Types of Problems Solved by Neural Networks}
Neural networks have proven to be powerful and versatile tools for solving a wide variety of computational problems. While they are traditionally associated with regression and classification tasks, modern neural architectures are capable of addressing a broader spectrum of problems across supervised, unsupervised, and reinforcement learning paradigms. This section provides an overview of the principal problem types solvable by neural networks.

Regression Problems: In regression tasks, the goal is to predict continuous numerical values based on input features. Neural networks learn a mapping from input vectors to a real-valued output, typically by minimizing a loss function such as the mean squared error \parencite[p.~99]{goodfellow2016deeplearning}. Applications include time series forecasting, energy demand prediction, and asset price estimation.

Classification Problems: Classification involves assigning input data to one or more discrete categories \parencite[p.~98]{goodfellow2016deeplearning}. Neural networks trained for classification tasks typically use softmax activation in the output layer and categorical cross-entropy as a loss function. Common applications include image recognition, sentiment analysis, fraud detection, and medical diagnosis.

Clustering: Clustering is an unsupervised learning problem where the aim is to group similar data points based on inherent structure in the data \parencite{aljalbout2018clustering}. While traditional algorithms like $k$-means are commonly used, neural network-based methods—such as autoencoders, self-organizing maps, and deep clustering networks—can learn more flexible and data-adaptive representations for clustering tasks.

Dimensionality Reduction: Dimensionality reduction seeks to project high-dimensional data into a lower-dimensional space while preserving meaningful structure. Neural networks can perform this task using autoencoders, which compress the data into a latent representation and then reconstruct it \parencite{hinton2006dimensionalityreduction}. Such techniques are widely used in data visualization, noise reduction, and feature extraction.

Reinforcement Learning: In reinforcement learning (RL), agents learn to take actions in an environment to maximize a long-term reward signal \parencite[pp.~2--3]{sutton2015reinforcement}. Neural networks serve as function approximators for value functions or policies in deep reinforcement learning frameworks such as Deep Q-Networks (DQNs) and Actor-Critic methods. RL applications include robotics control, autonomous driving, game-playing agents, and resource optimization.

\subsubsection{Architecture}
At the fundamental level, the basic unit of a neural network is referred to as a \textit{neuron} or \textit{node}. Each neuron receives one or more input signals, which are combined using a set of learnable parameters called \textit{weights} \parencite[p.~4]{mienye2024nncomprehensivereview}. These weighted inputs are summed and passed through a non-linear transformation known as an \textit{activation function}, which determines the neuron's output \parencite[pp.~4--5]{mienye2024nncomprehensivereview}. The activation function introduces non-linearity into the model, enabling the network to learn complex patterns in the data. Commonly used activation functions in neural networks include the sigmoid function, the hyperbolic tangent (\textit{tanh}), and the Rectified Linear Unit (ReLU). Each of these functions introduces non-linearity into the model, enabling the network to capture complex patterns in the input data. The sigmoid and \textit{tanh} functions are smooth, bounded, and differentiable, making them suitable for problems requiring normalized outputs. In contrast, the ReLU function, defined as $f(x) = \max(0, x)$, is computationally efficient and mitigates the vanishing gradient problem, thereby facilitating the training of deep neural networks.

Neural networks are typically organized into a series of layers \parencite[p.~4]{mienye2024nncomprehensivereview}:

Input Layer: The input layer is the first layer of the neural network and serves solely as the interface for receiving external data. Each neuron in this layer corresponds to a single feature or variable in the input vector. Importantly, neurons in the input layer do not perform any computation; instead, they transmit the raw input signals to the subsequent layer in the network.

Hidden Layer(s): Hidden layers are situated between the input and output layers and consist of neurons that perform intermediate computations. These layers are termed "hidden" because they are not directly observable from the input or output. The primary function of hidden layers is to transform the input data into more abstract representations through successive linear and non-linear operations. Networks with more than one hidden layer are referred to as deep neural networks, which can model highly intricate data relationships. Each neuron in a hidden layer is typically fully connected to all neurons in the preceding and succeeding layers, although sparse (partially connected) architectures are also used.

Output Layer: The output layer is the final layer in the network and produces the model's predictions. The structure and activation function of this layer are typically chosen based on the nature of the problem—for example, a softmax activation function is used for multi-class classification, while a linear activation may be used for regression tasks.

The design of a neural network—such as the number of hidden layers, the number of neurons per layer, and the choice of activation functions—determines its capacity to learn and generalize from data. Proper configuration and training of these networks are essential to achieving optimal performance on a given task \parencite{sazli2006feedforwardnn}.

\subsubsection{Learning in Neural Networks: The Back-Propagation Algorithm}

A defining characteristic of artificial neural networks is their ability to \textit{learn} and \textit{adapt} based on interactions with their environment. In this context, learning is defined as the process through which the free parameters (i.e., synaptic weights) of the network are adjusted to minimize a discrepancy between the actual output and the desired target output. The nature of this adjustment process depends on the learning algorithm employed.

Among the various learning algorithms, the \textit{back-propagation algorithm} is the most widely used method for training feed-forward neural networks. It operates under the framework of supervised learning, where the correct output (label) is known for each training example. The algorithm iteratively minimizes a loss function by updating weights in the direction of the negative gradient of the error.

Let $y_j(n)$ denote the actual output of neuron $j$ at iteration $n$, and let $d_j$ be the corresponding desired output. The difference between the desired and actual output is quantified using a measure known as the \textit{error energy}. For each neuron \( j \) in the output layer, the error signal \( e_j(n) \) is defined as:
\begin{equation}
	e_j(n) = d_j - y_j(n)
\end{equation}

The \textit{instantaneous error energy} for neuron \( j \) at iteration \( n \), denoted by \( \epsilon_j(n) \), is calculated as half the square of the error signal:
\begin{equation}
	\epsilon_j(n) = \frac{1}{2} e_j^2(n)
\end{equation}

This squared formulation ensures that the error energy is always non-negative and penalizes larger deviations more strongly. To evaluate the total discrepancy for the entire output layer \( Q \), the \textit{total error energy} \( \epsilon(n) \) is computed as the sum of the individual error energies:
\begin{equation}
	\epsilon(n) = \sum_{j \in Q} \epsilon_j(n)
\end{equation}

Minimizing this total error energy is the central objective of the back-propagation learning algorithm, which iteratively adjusts the network’s synaptic weights to reduce the mismatch between predicted and target outputs. The error signal $e_j(n)$ for neuron $j$ is defined as:
\begin{equation}
	e_j(n) = d_j - y_j(n)
\end{equation}

The instantaneous error energy $\epsilon_j(n)$ associated with neuron $j$ is then given by:
\begin{equation}
	\epsilon_j(n) = \frac{1}{2} e_j^2(n)
\end{equation}

The total instantaneous error energy $\epsilon(n)$ over all neurons $j \in Q$ in the output layer $Q$ is computed as:
\begin{equation}
	\epsilon(n) = \sum_{j \in Q} \epsilon_j(n)
\end{equation}

To reduce the total error energy $\epsilon(n)$, the synaptic weights $w_{ji}(n)$ are updated using the gradient descent method. The weight update rule is expressed as:
\begin{equation} \label{weight_update_equation}
	\Delta w_{ji}(n) = -\eta \frac{\partial \epsilon(n)}{\partial w_{ji}(n)}
\end{equation}
where $\eta$ is the learning rate, a small positive constant controlling the step size.

To compute the partial derivative $\frac{\partial \epsilon(n)}{\partial w_{ji}(n)}$, the chain rule is applied:
\begin{equation} \label{nn_partial_derivative}
	\frac{\partial \epsilon(n)}{\partial w_{ji}(n)} =
	\frac{\partial \epsilon(n)}{\partial e_j(n)} \cdot
	\frac{\partial e_j(n)}{\partial y_j(n)} \cdot
	\frac{\partial y_j(n)}{\partial w_{ji}(n)}
\end{equation}

Each term in this product is derived as follows:

\begin{equation} \label{nn_partial_derivative_term1}
	\frac{\partial \epsilon(n)}{\partial e_j(n)} = e_j(n)
\end{equation}
\begin{equation} \label{nn_partial_derivative_term2}
	\frac{\partial e_j(n)}{\partial y_j(n)} = -1
\end{equation}
\begin{equation} \label{nn_partial_derivative_term3}
	\frac{\partial y_j(n)}{\partial w_{ji}(n)} =
	f'\left(\sum_{i=0}^{m} w_{ji}(n) y_i(n)\right) \cdot y_i(n)
\end{equation}
where $f'(\cdot)$ is the derivative of the activation function of neuron $j$, and $y_i(n)$ is the output of neuron $i$ in the preceding layer.

Substituting Equations~\ref{nn_partial_derivative_term1},~\ref{nn_partial_derivative_term2}, and~\ref{nn_partial_derivative_term3} into Equation~\ref{nn_partial_derivative} yields:
\begin{equation}
	\frac{\partial \epsilon(n)}{\partial w_{ji}(n)} =
	-e_j(n) \cdot f'\left(\sum_{i=0}^{m} w_{ji}(n) y_i(n)\right) \cdot y_i(n)
\end{equation}

Inserting this result into the weight update rule (Equation~\ref{weight_update_equation}) gives the final expression for the synaptic weight adjustment:
\begin{equation}
	\Delta w_{ji}(n) = \eta \cdot e_j(n) \cdot f'\left(\sum_{i=0}^{m} w_{ji}(n) y_i(n)\right) \cdot y_i(n)
\end{equation}

This process of computing the output error, back-propagating it through the network, and updating the weights is performed iteratively for each training sample. Over time, the network's parameters converge such that the output increasingly approximates the desired targets, thereby enabling the network to generalize and make accurate predictions on unseen data. This iterative optimization process is typically continued until the total error $\epsilon(n)$ reaches an acceptably low threshold or a maximum number of training epochs is completed \parencite{sazli2006feedforwardnn}.

\subsubsection{The Hyperband Algorithm for Hyperparameter Optimization}
\label{subsec:hyperband_algorithm}

The performance of neural networks depends critically on the choice of hyperparameters, such as learning rate, batch size, number of layers, or regularization strength. Since these hyperparameters are not learned from data, their optimal configuration must be determined externally through hyperparameter optimization. Traditional methods such as grid search or random search are computationally inefficient, particularly when training deep neural networks. The \textit{Hyperband} algorithm, introduced by \textcite{li2017hyperband}, provides an efficient, theoretically grounded framework for hyperparameter selection based on the principle of adaptive resource allocation and early stopping.

Let $\mathcal{H}$ denote the hyperparameter space, and let $f : \mathcal{H} \rightarrow \mathbb{R}$ be a performance function (e.g., validation loss or error) that maps a hyperparameter configuration $h \in \mathcal{H}$ to its corresponding validation loss after training with a finite computational budget. The goal is to find
\begin{equation}
	h^* = \arg\min_{h \in \mathcal{H}} f(h),
	\label{eq:hyperband_objective}
\end{equation}
subject to a total computational budget $B_{\text{total}}$.

Hyperband frames this optimization as a \emph{multi-armed bandit} problem, in which each configuration corresponds to an arm, and computational resources correspond to the number of pulls allocated to each arm. The algorithm dynamically balances \textit{exploration} (testing many configurations) and \textit{exploitation} (allocating more resources to promising configurations).

The core component of Hyperband is the \textit{Successive Halving} (SH) procedure. Suppose $n$ configurations are sampled uniformly at random from $\mathcal{H}$ and each is trained for an initial budget $r$ (e.g., number of epochs or training samples). Their performances $\{f_i\}_{i=1}^{n}$ are evaluated, and only the top fraction $\frac{1}{\eta}$ is retained, where $\eta > 1$ is the \textit{reduction factor}. The surviving configurations are then trained for $\eta$ times the previous budget, and the process repeats. Formally, the iterative process for SH can be summarized as:
\begin{align}
	n_j & = \left\lfloor n \cdot \eta^{-j} \right\rfloor, \\
	r_j & = r \cdot \eta^{j},
	\label{eq:successive_halving}
\end{align}
for stages $j = 0, 1, \ldots, s_{\max}$, where $n_j$ is the number of configurations and $r_j$ is the resource budget per configuration at stage $j$. At each stage, the best $\lfloor n_j / \eta \rfloor$ configurations are promoted.

While Successive Halving requires a predefined $(n, r)$ pair, Hyperband systematically explores different trade-offs between the number of configurations $n$ and the allocated resources $r$. Specifically, it defines multiple \textit{brackets} indexed by $s \in \{0, 1, \ldots, s_{\max}\}$, where each bracket represents a different allocation strategy.

For each bracket $s$, Hyperband computes:
\begin{align}
	s_{\max} & = \left\lfloor \log_{\eta} R \right\rfloor,                   \\
	n_s      & = \left\lceil \frac{s_{\max} + 1}{s + 1} \eta^s \right\rceil, \\
	r_s      & = \frac{R}{\eta^s},
	\label{eq:hyperband_parameters}
\end{align}
where $R$ denotes the maximum allowable resource per configuration (e.g., maximum number of training epochs).

Each bracket executes a full Successive Halving run with its respective $(n_s, r_s)$ values. This design enables Hyperband to balance between two extremes:
\begin{itemize}
	\item Wide exploration (large $n_s$, small $r_s$): testing many configurations briefly.
	\item Deep exploitation (small $n_s$, large $r_s$): fully training fewer configurations.
\end{itemize}

The total computational cost per bracket is approximately constant, ensuring that the overall budget $B_{\text{total}}$ is respected:
\begin{equation}
	B_{\text{total}} \approx (s_{\max} + 1) R.
\end{equation}

The Hyperband procedure can be summarized as follows:
\begin{enumerate}
	\item For each bracket $s = s_{\max}, s_{\max}-1, \ldots, 0$:
	      \begin{enumerate}
		      \item Sample $n_s$ configurations $\{h_{s,i}\}_{i=1}^{n_s}$ uniformly from $\mathcal{H}$.
		      \item Run Successive Halving with initial resource $r_s$ and reduction factor $\eta$.
		      \item Record the best-performing configuration $h_s^*$ in each bracket.
	      \end{enumerate}
	\item Select the globally best configuration
	      \begin{equation}
		      h^* = \arg\min_{s} f(h_s^*).
	      \end{equation}
\end{enumerate}

The Hyperband algorithm enjoys strong theoretical guarantees under mild regularity assumptions. Li et al.\ (2018) show that it achieves an asymptotically optimal trade-off between the number of configurations and resources allocated, with expected simple regret bounded by
\begin{equation}
	\mathbb{E}\left[f(h^*) - f^*\right] = \mathcal{O}\left(\sqrt{\frac{\log n}{B_{\text{total}}}}\right),
	\label{eq:hyperband_regret}
\end{equation}
where $f^* = \min_{h \in \mathcal{H}} f(h)$ denotes the true optimal performance. This result demonstrates that Hyperband is provably more efficient than random search and grid search in expectation.

Hyperband provides a principled and computationally efficient framework for neural network hyperparameter optimization by combining random search with adaptive resource allocation. Its key strength lies in automatically balancing exploration and exploitation through multiple parallel Successive Halving procedures, resulting in near-optimal use of available computational resources without requiring manual tuning of training budgets.


Include descriptions of the following points:
Layers
Neurons
Weights \& Weights Updates
Activation Functions
Bias
Kernel


\subsection{Related Work}
\textcite{alvarez2022hullwhite} demonstrated that the prices generated by a one-factor Hull-White model which uses parameters calibrated by a NN which was trained to learn the inverse relationship between swaption prices and the model parameters $\alpha$ and $\sigma$ were close to the observed market prices, suggesting that the NN is an effective method for accurately calibrating the model parameters.

\textcite{hernandez2016model} examines the use of artificial NNs to replace traditional global optimization methods in the calibration of the two-factor Hull-White model. While global optimization methods are better at avoiding local minima, they are slower than local optimizers. The proposed approach uses a NN to approximate the calibration function, shifting the computational effort to an offline training phase. This enables the actual calibration process to be performed in a fraction of the time compared to global optimizers. The results show that using NNs for calibration offers significant time savings, with the process being much faster and more consistent than traditional methods. However, the performance of the NN degrades over time, though this can be mitigated by periodically retraining the model.

\textcite{alaya2021deep} also investigate the use of NNs for the calibration of interest rate models, focusing on the G2++ model and the CIR intensity model. The paper proposes using Deep Calibration (DC) to improve this process, making calibration faster, more accurate, and easier to handle. Two main approaches for DC are presented: Indirect Deep Calibration (Indirect DC), which uses intermediate quantities like covariance matrices derived from market data, and Direct Deep Calibration (Direct DC), which uses market-observable zero-coupon (ZC) rate curves directly. Both methods demonstrate significant time savings, with Direct DC being particularly efficient due to its simplicity and lower computational costs. The paper also highlights the advantages of training on synthetic data, which allows for large datasets, stress-testing scenarios, and direct error measurement. The results show that DC significantly reduces calibration time compared to classical methods, with Direct DC being up to 7,600 times faster. Moreover, DC achieves good accuracy and is more resilient to noise in the data, outperforming traditional calibration methods in terms of global error across all parameters. The approaches also work well when applied to other models, such as the CIR intensity model. Overall, the study demonstrates that DL-based DC methods offer a practical, efficient alternative for calibrating financial models, with potential applications in various financial contexts.

\textcite{moysiadis2019calibrating} focussed on the calibration of the mean reversion speed parameter in the one-factor Hull-White model and proposed a method in which a NN is utilized to learn the future movement of interest rates based on historical data. The primary aim was to extract the mean reversion parameter from the derivative of the function learned by the NN which approximates the future rate. The approach emphasizes the robustness of the model, particularly its ability to handle market turbulence. The results demonstrate that the NN-based method delivers mean reversion values comparable to those obtained using a rolling linear regression, a standard method.