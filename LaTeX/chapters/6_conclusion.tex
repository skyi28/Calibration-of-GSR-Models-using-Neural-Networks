This thesis embarked on a comprehensive investigation into the efficacy of modern machine learning techniques for the calibration of the one-factor \ac{hw} interest rate model. The central objective was to move beyond traditional iterative optimization and determine whether a \ac{nn}, trained end-to-end, could provide a more accurate, stable, and computationally efficient alternative to the well-established \ac{lm} algorithm. This research was guided by three central questions aimed at evaluating the efficacy of a machine learning-based approach against traditional methods for the calibration of the one-factor Hull-White model. The empirical findings provide the following answers:

How does the accuracy of a \ac{nn}-based calibration, measured by the model's ability to replicate market swaption prices, compare to that of the traditional \ac{lm} algorithm?

The empirical results show a superior out-of-sample pricing accuracy for the \ac{nn}-based calibration method. Over the test period, the \ac{nn} achieved a mean unweighted \ac{rmse} of 4.33 \ac{bps}, compared to 6.05 \ac{bps} for the best-performing traditional method, the \ac{lm} algorithm with an adaptive anchor initialization. The performance of the \ac{nn} also exhibited significantly higher stability, evidenced by a standard deviation of its daily error (0.33 \ac{bps}) that was approximately 4.6 times lower than that of the most stable \ac{lm} strategy (1.53 \ac{bps}). Statistical tests confirmed that these differences in both mean error and variance are statistically significant. Further analysis of error distributions revealed that the \ac{nn} produces smaller and less systematic biases across different market volatility regimes and instrument characteristics (tenor and expiry), indicating a more robust and consistent calibration performance.

What is the quantitative difference in computational speed between the two methods?

A substantial difference in computational efficiency was observed. The primary computational cost of the \ac{nn} approach is incurred during a one-off, offline training phase. Once trained, the ongoing application (inference) for a daily calibration is nearly instantaneous, requiring an average of approximately 4 milliseconds. In contrast, the \ac{lm} algorithm performs a full, iterative optimization for each new market snapshot. The most efficient \ac{lm} strategy required an average of 72.25 seconds per calibration. This represents a speed-up factor of approximately 17,621 for daily, operational calibration tasks and is consistent with the results observed in related literature, with \textcite{alaya2021deep} reporting a speedup of 7,600 times when applying a \ac{nn} to the calibration of the G2++ model. While the \ac{nn} requires a significant upfront investment for training, this cost is amortized over its operational lifetime, making it highly efficient for applications requiring high-frequency or real-time calibration.

How stable are the estimated model parameters over time?

The analysis of parameter trajectories demonstrates that the \ac{nn} generates temporally stable and economically coherent model parameters. The parameters produced by the \ac{nn} exhibit minor daily adjustments consistent with market fluctuations but maintain a high degree of overall stability, with standard deviations an order of magnitude smaller than those of the \ac{lm} methods. Conversely, the traditional \ac{lm} optimization strategies proved susceptible to parameter instability. The strategy using the previous day's solution as an initial guess (Pure Rolling Warm Start) suffered from error propagation and significant parameter drift. The strategy using a fixed initial guess (Static Cold Start) produced highly erratic parameters, particularly under changing market conditions. Even the most robust \ac{lm} approach (Adaptive Anchor) yielded parameters that were substantially more volatile than those of the \ac{nn}. Stress tests further confirmed this distinction, showing that the \ac{nn} responds to market shocks with stable and economically interpretable parameter adjustments, whereas the \ac{lm} optimizer exhibited severe numerical instability.

The findings of this thesis may have implications for financial modeling. The observed performance difference suggests a potential advantage in the end-to-end training approach, which, unlike the benchmark study by \textcite{hernandez2016model}, allows the \ac{nn} to directly minimize the pricing error. This may permit the \ac{nn} to identify parameter mappings that are not constrained by the local minima of traditional optimizers. Furthermore, the \ac{shap} analysis pointed to the importance of the economically-motivated feature engineering, as the model learned to prioritize yield curve curvature and systemic risk indicators in a manner consistent with financial theory. From a practical standpoint, the results indicate that the sub-second calibration time of the \ac{nn} could make applications such as real-time risk management and intraday recalibration operationally feasible. The observed accuracy and parameter stability could provide a more reliable foundation for pricing other derivatives.

Nonetheless, this research is subject to several limitations. The analysis was restricted to a one-factor \ac{hw} model calibrated exclusively to European \ac{atm} swaptions, omitting multi-factor models, other derivative instruments, and the volatility smile or skew. The dataset covers only a short, three-month period in a single currency market, limiting generalizability across different economic cycles and the assessment of long-term performance stability. Feature selection was not systematically performed, potentially constraining model performance. Furthermore, the \ac{nn} approach entails a substantial upfront investment in training and hyperparameter tuning, and despite \ac{shap}-based interpretability, its black box nature presents challenges for model validation, governance, and regulatory approval.

The empirical evidence from this thesis leads to several considerations. For users of the traditional \ac{lm} method, the results show that the algorithm's performance is sensitive to its initial guess, as evidenced by the performance degradation of the Pure Rolling strategy. Furthermore, more robust initialization strategies, such as the Adaptive Anchor approach, are beneficial for mitigating parameter drift. For developers of machine learning models in finance, the results outline the importance of integrating domain knowledge through feature engineering, as the \ac{nn}'s use of curated features was a notable component of its performance. The direct optimization of the pricing error also appeared to be an effective training paradigm in this study, suggesting that such end-to-end learning frameworks could be a valuable approach for developing surrogate calibration models. 

Future research should extend the framework to multi-factor interest rate models like the G2++ model to capture more complex yield curve dynamics and incorporate \ac{otm} and \ac{itm} swaptions to account for the volatility smile. Evaluating hedge performance through the accuracy, stability, and P\&L impact of derived hedge parameters would provide a more comprehensive assessment. Validation over longer time horizons and across diverse market conditions would provide deeper insights into the \ac{nn}'s behavior under varying market regimes. Systematic feature selection using techniques such as backward elimitation, could further improve model robustness. Exploring advanced \ac{nn} architectures, such as recurrent networks or transformers, may enhance the capture of temporal dependencies in market data. A significant enhancement to the residual learning framework could be the adoption of a dynamic initial guess, leveraging the parameters from the previous day, to focus the learning task on modeling daily parameter dynamics rather than absolute levels, potentially improving both stability and convergence speed. Finally, re-implementing the \ac{hw} swaption pricing logic in a fully GPU-accelerated and differentiable framework, eliminating numerical gradient approximations, could dramatically reduce training times, enable more extensive hyperparameter exploration and thereby potentially enhance model performance. Should a fully differentiable pricing engine not be feasible, an intermediate step could involve altering the training target from volatility to price. This would entail converting the market swaption volatility surface to a corresponding price surface in a one-time preprocessing step. The \ac{nn} would then be trained to minimize the error between the model-generated prices and these market prices. Such an approach would circumvent the computationally expensive, iterative root-finding algorithm required to invert prices back to implied volatilities within the training loop. This would significantly accelerate both the hyperparameter optimization and final training phases, while the model's core task, learning the parameters of the \ac{hw} model, would remain unchanged.