This chapter synthesizes the key findings of the study, comparing the accuracy, speed, and robustness of the machine learning and traditional calibration methods. It discusses the implications for practitioners and researchers, assesses the study's limitations, and outlines directions for future research in machine learning-based financial model calibration.

The primary objective of this research was to conduct a comprehensive comparison between a traditional optimization-based calibration method, specifically the \ac{lm} algorithm, and a novel, end-to-end machine learning approach for the one-factor \ac{hw} interest rate model. The study demonstrates that the \ac{nn}-based calibration is orders of magnitude faster than the traditional optimizer, performing in milliseconds compared to over ninety seconds for \ac{lm}, representing a computational speed-up of approximately 22,555 times. This shift moves the main computational burden from ongoing, real-time application to a one-time, offline training phase.  

In terms of accuracy, the \ac{nn} consistently outperforms the traditional method in out-of-sample pricing, achieving a substantially lower average root mean squared error of 6.53 basis points compared to 10.11 basis points for the \ac{lm} approach. The network's error profile is more balanced across the volatility surface, while the traditional method exhibits systematic biases, particularly for short-tenor and short-maturity swaptions. Moreover, the \ac{nn} produces economically sensible and stable parameters that react logically to simulated market shocks, whereas the traditional optimizer frequently suffers from numerical instability, with the mean-reversion parameter collapsing or exploding under stress scenarios. Compared to the approach of \textcite{hernandez2016model}, in which the network is trained on a predetermined set of optimal parameters, the end-to-end framework integrating the network directly with QuantLib's pricing engine and minimizing pricing errors proves, once trained, superior in both speed and pricing ability.  

The implications of these findings are substantial. The machine learning approach is particularly well-suited for applications requiring high-frequency recalibration, including real-time risk management, pre-trade pricing for large derivative portfolios, and intraday adjustments. Its robust performance in sensitivity analyses suggests resilience to market shocks and changing regimes, whereas the \ac{lm} method is more appropriate for less time-sensitive, batch-processing tasks. Practically, the speed, accuracy, and robustness of the \ac{nn} enable more frequent and comprehensive risk assessments, more stable hedge ratios, and reliable pricing of complex, path-dependent derivatives.  

Nonetheless, this research is subject to several limitations. The analysis was restricted to a one-factor \ac{hw} model calibrated exclusively to European \ac{atm} swaptions, omitting multi-factor models, other derivative instruments, and the volatility smile or skew. The dataset covers only a short, three-month period in a single currency market, limiting generalizability across different economic cycles and the assessment of long-term performance stability. Feature selection was not systematically performed, potentially constraining model performance. The evaluation focused solely on pricing accuracy (\ac{rmse}), without assessing the stability or reliability of hedge parameters, which are critical for practical risk management. Furthermore, the \ac{nn} approach entails a substantial upfront investment in training and hyperparameter tuning, and despite \ac{shap}-based interpretability, its "black box" nature presents challenges for model validation, governance, and regulatory approval.  

Future research should extend the framework to multi-factor interest rate models like the G2++ model to capture more complex yield curve dynamics and incorporate \ac{otm} and \ac{itm} swaptions to account for the volatility smile. Evaluating hedge performance through the accuracy, stability, and P\&L impact of derived hedge parameters would provide a more comprehensive assessment. Validation over longer time horizons and across diverse market conditions would provide deeper insights into the \ac{nn}'s behavior under varying market regimes. Systematic feature selection using techniques such as backward elimitation, could further improve model robustness. Exploring advanced \ac{nn} architectures, such as recurrent networks or transformers, may enhance the capture of temporal dependencies in market data. A significant enhancement to the residual learning framework would be the adoption of a dynamic initial guess, leveraging the parameters from the previous day, to focus the learning task on modeling daily parameter dynamics rather than absolute levels, potentially improving both stability and convergence speed. Finally, re-implementing the \ac{hw} swaption pricing logic in a fully GPU-accelerated and differentiable framework, eliminating numerical gradient approximations, could dramatically reduce training times, enable more extensive hyperparameter exploration, and facilitate near-real-time high-frequency recalibration on large derivative portfolios.
