This study is subject to several limitations arising from the model choice, dataset, methodology, evaluation framework, and practical implementation. The research exclusively focuses on the one-factor \ac{hw} model. While this model serves as a standard benchmark, it has inherent limitations in capturing complex yield curve dynamics, such as twists and butterfly movements, and in fitting the complete swaption volatility surface. Consequently, the findings may not directly generalize to more sophisticated multi-factor interest rate models. Additionally, the model is calibrated solely to European \ac{atm}, leaving its ability to price \ac{otm} or \ac{itm} options and to reproduce the volatility smile or skew unevaluated.

The dataset employed introduces several constraints. The analysis covers only a short, recent period from 01.06.2025, to 31.08.2025, characterized by generally moderately decreasing volatility and rising rates. The performance across other market regimes, including periods of high volatility, financial crises, or near-zero interest rates, remains untested. Data were carefully extracted from screenshots using image-to-Excel software due to restrictions of the university's Bloomberg licence, and a subsequent manual validation was conducted; nevertheless, this procedure may introduce minor transcription errors that would not occur with a direct \ac{api} feed. Moreover, the use of mid-market quotes for swap rates and swaption volatilities ignores bid-ask spreads, which reflect transaction costs and liquidity, potentially affecting practical profitability and risk assessment. The \ac{nn} is trained on daily snapshots, which limits its ability to capture intraday movements, restricting applicability for real-time pricing or high-frequency trading. The research is confined to the \ac{eur} market, so the conclusions may not extend to other currency markets with different structures, liquidity profiles, and central bank policies.

The findings and performance metrics reported herein are contingent upon the specific historical dataset utilized, which reflects a particular set of market conditions and volatility dynamics. Consequently, the generalizability of these results to different time periods, alternative datasets, or divergent market regimes cannot be assumed. The relative performance of the calibration methods may vary under different market structures. Furthermore, it must be emphasized that the documented performance is a historical assessment. In line with established principles of financial modeling, past performance is not a reliable indicator of future results, as underlying market structures and dynamics are inherently non-stationary and subject to change.

Another limitation regarding the evaluation framework used from a practical risk management perspective is the study's exclusive focus on pricing accuracy, while an examination of the resulting hedge parameters remain unevaluated. A model can exhibit a low pricing error \ac{rmse} yet produce highly volatile hedge ratios if its calibrated parameters, and thus its derivatives with respect to market variables, fluctuate erratically over time. Such instability would render the model impractical for active hedging, as it would necessitate frequent and costly portfolio rebalancing that could erode any pricing advantages. Furthermore, the focus on \ac{rmse} as a single error metric does not capture tail risks or the distribution of errors, and the artificial hold-out set used for the \ac{lm} method may slightly handicap its in-sample performance relative to real-world usage.

From a technical and practical perspective, the end-to-end training approach is computationally intensive due to numerical gradient approximation and would further increase if a larger dataset would be utilized, which may have limited hyperparameter tuning and model complexity. Static hyperparameters were used, although optimal settings may vary over time, and no feature selection algorithm was employed. \ac{pca}, as a linear technique, may not capture non-linear relationships fully. While the Hyperband algorithm is highly efficient, the search for optimal hyperparameters was still confined to a predefined range for each parameter (e.g., number of neurons, learning rate). It is possible that the globally optimal configuration lies outside of these selected ranges. The search identifies the best model within the explored space, which is not necessarily the best possible model in the absolute sense. Moreover, the \ac{nn} approach requires substantial upfront investment in data collection, feature engineering, and initial training. The long-term performance and need for retraining could not be fully assessed, and certain aspects of model risk, governance, and regulatory considerations were not addressed. These limitations collectively suggest that while the proposed approach demonstrates promising results, its applicability and robustness in broader, real-world settings require further investigation.