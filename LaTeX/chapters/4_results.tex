Building upon the comprehensive methodology detailed previously, this chapter presents the empirical findings of the comparative analysis. The discussion is structured to build from foundational model validation to the core performance comparison. It begins by interpreting the results of the \ac{pca} on the yield curve, validating the feature engineering process. This is followed by a detailed analysis of the hyperparameter tuning phase, which identified the optimal architecture for the \ac{nn} model.

With the models established, the chapter proceeds to the central, multi-faceted comparison between the predictive \ac{nn} and the traditional \ac{lm} optimizer. This evaluation is structured around the key criteria of out-of-sample pricing accuracy, the stability and economic interpretability of the derived model parameters under simulated market shocks, and the computational efficiency measured by calibration runtime.

To address the common criticism of machine learning models as "black boxes", a dedicated analysis of the \ac{nn}'s interpretability is presented using \ac{shap} values, revealing the key market drivers behind its predictions. Finally, the results are contextualized through a direct comparison with the benchmark study by \textcite{hernandez2016model}, positioning this thesis's findings and contributions within the existing body of research.

\subsection{Interpretation of Principal Component Analysis on the Yield Curve}
The application of \ac{pca} to the yield curve provides a powerful framework for decomposing the complex dynamics of interest rate movements into a small number of uncorrelated factors. The empirical results of the analysis indicate that the first three principal components collectively explain 99.84\% of the total variance observed in daily yield curve changes. This finding confirms that the term structure of interest rates can be effectively described by a limited set of underlying factors, each representing a distinct pattern of yield curve movement.

\begin{table}[H]
	\centering
	\caption{Explained Variance of the Principal Components}
	\label{tab:pca_variance}
	\begin{tabular}{lcc}
		\toprule
		Principal Component & Explained Variance (\%) & Cumulative Variance (\%) \\
		\midrule
		PC1 (Level)         & 91.08                   & 91.08                    \\
		PC2 (Slope)         & 8.16                    & 99.24                    \\
		PC3 (Curvature)     & 0.59                    & 99.84                    \\
		\bottomrule
	\end{tabular}
\end{table}

The first principal component (PC1) accounts for 91.08\% of the total variance and clearly emerges as the dominant driver of yield curve fluctuations. The loading plot of PC1 reveals positive loadings across all maturities, indicating that variations in this factor correspond to parallel shifts in the yield curve. In other words, changes in PC1 lead to simultaneous increases or decreases in yields across all tenors, representing the so-called level factor. This component captures broad-based movements in the general level of interest rates and is thus associated with macroeconomic influences such as monetary policy shifts and long-term inflation expectations.

The second principal component (PC2), explaining 8.16\% of the total variance, can be interpreted as the slope factor. Its loading structure is characterized by negative values for short maturities and positive values for longer maturities, reflecting an inverse relationship between short- and long-term rates. When this factor increases, the yield curve steepens-short-term yields decline while long-term yields rise. Conversely, a decrease in this component leads to a flattening of the curve. The slope factor therefore captures non-parallel shifts in the term structure and is closely related to expectations regarding future economic growth and central bank policy adjustments.

The third principal component (PC3) explains 0.59\% of the total variance and represents the curvature factor. Its loading pattern exhibits a distinct hump shape, with positive loadings at the short and long ends of the maturity spectrum and negative loadings in the intermediate maturities. This configuration indicates that variations in PC3 alter the concavity of the yield curve, producing so-called butterfly movements. Such changes often occur when medium-term interest rates move differently from short- and long-term rates, providing information about market expectations of medium-term monetary policy and term premia.

The empirical findings are consistent with the theoretical and empirical literature on interest rate modeling, particularly the work of \parencite[pp.~98--107]{Rebonato_2018}. The identification of level, slope, and curvature as the three principal components of the yield curve is a well-established result in fixed-income research. Their hierarchical importance-where the level factor dominates, followed by the slope and curvature factors-reflects a universal characteristic of yield curve dynamics across different markets and time periods.

The concentration of explanatory power within the first three components demonstrates the suitability of PCA for dimensionality reduction in yield curve modeling. Instead of modeling the dynamics of nine highly correlated interest rates, the analysis can be simplified to three orthogonal factors that capture nearly all relevant variation. This dimensionality reduction not only mitigates model complexity and overfitting risk but also enhances interpretability by linking statistical factors to economically meaningful concepts. Furthermore, such a factor-based representation supports practical applications in risk management, hedging, and scenario analysis, as the identified components correspond directly to the main sources of interest rate risk in the term structure.

\subsection{Hyperparameter Tuning}
To identify the most suitable model configuration, a comprehensive hyperparameter search was conducted using the Hyperband algorithm with a total of 1000 trials. Model performance was evaluated based on the unweighted \ac{rmse} computed on the validation dataset (\textit{val\_rmse}). For subsequent analysis, only the top 5\% of all tested configurations were retained, resulting in a subset of 50 models that minimized the validation \ac{rmse}. Within this subset, Pearson correlation coefficients were calculated to quantify the linear dependencies among the numerical hyperparameters, as well as their individual relationships with the validation error. The resulting correlation matrix and correlation vector serve as diagnostic tools for assessing parameter interactions and their effects on model performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/hyperparameters/hyperparameter_correlation_heatmap.png}
	\caption{Correlation between the optimized numerical hyperparameters.}
	\label{fig:hyperparameter_correlation}
\end{figure}

The correlation matrix of numerical hyperparameters in figure \ref{fig:hyperparameter_correlation} reveals that most pairwise correlations are weak, as indicated by predominantly light blue and grey cells in the respective heatmap. This finding suggests that the search space of the best-performing models is not strongly restricted by interdependencies among parameters. Nevertheless, a moderate negative correlation of -0.34 between \textit{neurons\_2} and \textit{neurons\_3} can be observed, indicating that an increase in the capacity of the third hidden layer is often accompanied by a reduction in the fourth layer's size. This pattern may reflect an implicit constraint on the overall capacity distribution across deeper layers. Furthermore, weak positive correlations were detected between \textit{neurons\_0} and both \textit{dropout\_rate} (0.28) and \textit{learning\_rate} (0.26), implying that larger initial layers might benefit from slightly higher regularization and more aggressive learning rates to achieve optimal performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/hyperparameters/error_feature_correlation_heatmap.png}
	\caption{Correlation between the optimized numerical hyperparameters and the validation error.}
	\label{fig:error_feature_correlation_heatmap}
\end{figure}

The correlation analysis provided in figure \ref{fig:error_feature_correlation_heatmap} between hyperparameters and model error provides additional insights into performance sensitivity. Since the objective is to minimize \textit{val\_rmse}, a negative correlation indicates an improvement in performance with increasing hyperparameter values, while a positive correlation suggests performance deterioration. The strongest observed relationship is a positive correlation of 0.46 for the \textit{underestimation\_penalty}, indicating that higher values of this penalty consistently degrade performance. Consequently, the optimal configuration for this parameter likely lies near zero. In contrast, a moderate negative correlation of -0.29 between \textit{num\_layers} and error suggests that deeper network architectures tend to yield superior performance within the top-tier models. Similarly, the parameters \textit{neurons\_0} (-0.22) and \textit{neurons\_3} (-0.15) exhibit weak negative correlations with error, indicating a slight preference for wider layers at specific network depths. The regularization term \textit{dropout\_rate} shows a mild positive correlation (0.13) with error, implying that excessive dropout may hinder learning even among the best configurations. Lastly, the parameter \textit{tuner/epochs} is negatively correlated with error (-0.14), confirming that extended training durations within the Hyperband framework generally improve convergence and validation performance.

The architecture of the best-performing model reflects these findings. It consists of five hidden layers with 112, 48, 32, 112, and 48 neurons, respectively. All layers employ the \ac{relu} activation function. Although a dropout rate of 0.4 was specified in the search space, dropout was disabled in this configuration. The model uses a learning rate of approximately 0.0033 and applies an underestimation penalty of 1.5, indicating the use of a customized loss formulation to control prediction asymmetry. The corresponding hyperparameter configuration is summarized in table~\ref{tab:best_hyperparameters}.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Hyperparameter Configuration of the Best-Performing Model}
		\label{tab:best_hyperparameters}
		\begin{tabular}{lcc}
			\toprule
			\textbf{Hyperparameter}  & \textbf{Description}        & \textbf{Value} \\
			\midrule
			num\_layers              & Number of hidden layers     & 5              \\
			neurons\_0               & Neurons in layer 1          & 112            \\
			neurons\_1               & Neurons in layer 2          & 48             \\
			neurons\_2               & Neurons in layer 3          & 32             \\
			neurons\_3               & Neurons in layer 4          & 112            \\
			neurons\_4               & Neurons in layer 5          & 48             \\
			activation               & Activation function         & \ac{relu}      \\
			use\_dropout             & Dropout enabled             & False          \\
			dropout\_rate            & Dropout rate (inactive)     & 0.4            \\
			learning\_rate           & Optimizer learning rate     & 0.0033         \\
			underestimation\_penalty & Penalty for underestimation & 1.5            \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item The table reports the optimal hyperparameter values obtained from the Hyperband search. Dropout was disabled in the final configuration, rendering the specified dropout rate inactive. The underestimation penalty indicates the inclusion of a custom asymmetric loss component.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\subsection{Out-of-Sample Performance}
\subsubsection{Pricing Ability}
Figure~\ref{fig:daily_rmse_comparison} depicts the daily out-of-sample \ac{rmse} in \ac{bps} for the \ac{nn} and the traditional \ac{lm} calibration strategies over the test period from 04.08.2025 to 31.08.2025. The evaluation is conducted on a hold-out set of swaptions that were excluded from the \ac{lm} optimization on each respective day, ensuring an unbiased comparison between the \ac{nn}'s predictive capability and the \ac{lm}'s in-sample fitting performance as described in section~\ref{subsec:comparison_of_nn_and_lm}.

To provide a metric that reflects the financial significance of the calibration errors, rather than just their statistical magnitude, the subsequent analysis additionally utilizes vega-weighted values. The formal procedure for this vega-based aggregation, which translates volatility errors into their approximate price impact, is specified in section~\ref{subsec:conversion_from_normal_to_lognormal_errors}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot1_daily_rmse_combined.png}
	\caption{Comparison of daily out-of-sample \ac{rmse} between \ac{nn} and \ac{lm} calibration methods over the test period.}
	\label{fig:daily_rmse_comparison}
\end{figure}

The results, summarized in figure \ref{fig:daily_rmse_comparison} and table \ref{tab:rmse_statistics}, indicate distinct performance profiles for each calibration method. The \ac{nn} consistently exhibits the lowest error, achieving a mean unweighted \ac{rmse} of 4~\ac{bps}bps with a standard deviation of only 0.33~\ac{bps}. This low standard deviation underscores the model's high degree of stability across the test period. Among the \ac{lm} implementations, the Adaptive Anchor strategy yields the most favorable results, with a mean unweighted \ac{rmse} of 6.05~\ac{bps}. However, its performance is more variable than the \ac{nn}'s, as reflected by a higher standard deviation of 1.53~\ac{bps}. The Static \ac{lm} approach produces a higher mean unweighted error of 9.72~\ac{bps} and a standard deviation of 2.93~\ac{bps}. The performance of this strategy, which utilizes a fixed initial parameter guess for all days, was initially competitive because the guess was calibrated on market data immediately preceding the test period. However, as market conditions evolved, this static guess became progressively less representative, leading to an increase in calibration error over time. The Pure Rolling \ac{lm} strategy demonstrates the least effective performance, with a mean unweighted \ac{rmse} of 17.97~ac{bps}. This strategy's deteriorating performance can be attributed to error propagation; by using the previous day's calibrated parameters as the initial guess for the current day, any small estimation error from one day is carried forward to the next. Over the test period, these incremental errors accumulate, causing the parameter estimates to drift away from the optimal values and resulting in a compounding increase in the daily \ac{rmse}. Its instability is further highlighted by a large standard deviation of 6.72~\ac{bps} and a wide error range, with a maximum daily \ac{rmse} reaching 27.68~\ac{bps}.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Out-of-Sample \ac{rmse} Statistics (Unweighted vs. Vega-Weighted)}
		\label{tab:rmse_statistics}
		\begin{tabular}{lccccc}
			\toprule
			\textbf{Method}                     & \textbf{Mean} & \textbf{Std Dev} & \textbf{Median} & \textbf{Min} & \textbf{Max} \\
			\midrule
			Neural Network (Unweighted)         & 4.33          & 0.33             & 4.25            & 3.91         & 5.32         \\
			Neural Network (Vega-Weighted)      & 4.01          & 0.31             & 3.92            & 3.61         & 4.96         \\
			\midrule
			LM (Adaptive Anchor, Unweighted)    & 6.05          & 1.53             & 5.79            & 4.60         & 10.49        \\
			LM (Adaptive Anchor, Vega-Weighted) & 5.60          & 1.96             & 5.19            & 3.80         & 11.30        \\
			\midrule
			LM (Pure Rolling, Unweighted)       & 17.97         & 6.72             & 17.46           & 4.98         & 27.68        \\
			LM (Pure Rolling, Vega-Weighted)    & 20.08         & 7.86             & 19.58           & 4.55         & 31.35        \\
			\midrule
			LM (Static, Unweighted)             & 9.72          & 2.93             & 11.51           & 4.26         & 12.65        \\
			LM (Static, Vega-Weighted)          & 11.35         & 3.96             & 13.74           & 4.13         & 15.10        \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The table reports summary statistics of daily out-of-sample \ac{rmse} values over the test period for both unweighted and vega-weighted metrics. All values are expressed in basis points.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

A comparison between the two error metrics provides further insights into model behavior. For both the \ac{nn} and the \ac{lm} (Adaptive Anchor) strategy, the mean vega-weighted \ac{rmse} is lower than the unweighted \ac{rmse}, 4.01~\ac{bps} versus 4.33~\ac{bps}, for the \ac{nn}, and 5.60~\ac{bps} versus 6.05~\ac{bps} for the Adaptive Anchor. This suggests that these two models achieve a more accurate calibration for the swaptions that have the most significant financial sensitivity. Conversely, for the \ac{lm} (Static) and \ac{lm} (Pure Rolling) strategies, the mean vega-weighted \ac{rmse} is higher than its unweighted counterpart. The mean error for the Static method increases from 9.72~\ac{bps} to 11.35~\ac{bps}, and for the Pure Rolling method, it increases from 17.97~\ac{bps} to 20.08~\ac{bps}. This indicates that the mispricings for these latter two methods are more pronounced on the most vega-sensitive instruments. The performance degradation of the Pure Rolling strategy is particularly notable in the vega-weighted metric, suggesting that its parameter drift leads to substantial errors on financially critical instruments. In summary, the quantitative evidence from the out-of-sample test confirms that the \ac{nn} framework provides a more accurate and stable calibration than the traditional optimization techniques, and that the choice of initial guess strategy significantly influences the robustness of the \ac{lm} optimizer.

To provide a more formal statistical foundation for these observations, a series of hypothesis tests were conducted on the unweighted RMSE time series of each method. These tests were designed to assess the underlying properties of the error distributions and to quantify the statistical significance of the performance differences between the calibration strategies.

The Shapiro-Wilk test was first applied to assess the normality of the error distributions. The null hypothesis of normality was rejected at a 1\% significance level for the \ac{lm} Static (p < 0.0001) and \ac{lm} Adaptive Anchor (p = 0.0003) strategies, indicating that their errors do not follow a normal distribution. The evidence against normality was less conclusive for the \ac{nn} and the LM Pure Rolling strategy, with the null hypothesis being rejected at the 5\% level for the former (p = 0.0173) but not for the latter (p = 0.0877). Given the deviation from normality in the majority of the series, the non-parametric Mann-Whitney U test was selected for subsequent pairwise comparisons of central tendency. Next, the stationarity of the error series was examined using the Augmented Dickey-Fuller test, where the null hypothesis is that the time series is non-stationary. The test revealed that the error series for the \ac{lm} Static (p = 0.0005) and \ac{lm} Adaptive Anchor (p < 0.0001) strategies are stationary. This suggests that their errors are mean-reverting and do not contain a unit root. In contrast, the null hypothesis of non-stationarity could not be rejected for the \ac{nn} (p = 0.9954) or the \ac{lm} Pure Rolling (p = 0.5921) error series. For the Pure Rolling strategy, this result is consistent with the visually observed error accumulation, providing statistical evidence of its parameter drift. For the \ac{nn}, the non-stationarity finding may reflect the fact that its highly stable and low-variance error closely tracks underlying non-stationary market factors. The tables containg the results can be found in the appendix~\ref{appendix:hypothesis_tests}.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Pairwise Comparison Tests Between Strategies}
		\label{tab:pairwise_tests}
		\begin{tabular}{lcccccc}
			\toprule
			Comparison               & Test           & Statistic & p-value & $\alpha=0.01$ & $\alpha=0.05$ & $\alpha=0.10$ \\
			\midrule
			NN|LM Static          & Mann-Whitney U & 31.00   & 0.0000  & Rejected      & Rejected      & Rejected      \\
			                      & Levene         & 14.67   & 0.0003  & Rejected      & Rejected      & Rejected      \\
			NN|LM Pure Rolling    & Mann-Whitney U & 1.00    & 0.0000  & Rejected      & Rejected      & Rejected      \\
			                      & Levene         & 97.40   & 0.0000  & Rejected      & Rejected      & Rejected      \\
			NN|LM Adaptive Anchor & Mann-Whitney U & 28.00   & 0.0000  & Rejected      & Rejected      & Rejected      \\
			                      & Levene         & 13.78   & 0.0005  & Rejected      & Rejected      & Rejected      \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} Mann-Whitney U tests differences in distribution medians; Levene tests differences in variance. Rejected indicates significant difference at the specified $\alpha$ level.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

Pairwise comparisons confirm the performance hierarchy with high statistical confidence. The Mann-Whitney U test indicates that the \ac{nn}'s mean error is statistically significantly lower than that of all three LM strategies (p < 0.0001 in all cases). Furthermore, Levene's test for equality of variances shows that the \ac{nn}'s error variance is also statistically significantly lower than that of every \ac{lm} method (p < 0.001 in all cases). These results provide formal statistical validation that the \ac{nn} is not only more accurate but also produces significantly more stable and consistent calibrations. Among the LM strategies, the Adaptive Anchor is shown to be statistically superior to the Static approach in terms of mean error (p < 0.0001), although the difference in their variances is less pronounced and only significant at the 5\% level. Collectively, these statistical tests reinforce the initial interpretation of the performance data, formally quantifying the superior accuracy and stability of the \ac{nn} approach.

The mean prediction errors, defined as the difference between model-implied volatilities and observed market volatilities (\(\text{Model Volatility} - \text{Market Volatility}\)) in basis points (\ac{bps}), were analyzed for the \ac{nn} and \ac{lm} calibration methods across various swaption expiry and tenor bins. The corresponding heatmaps in figure~\ref{fig:error_heatmaps} visualize these unweighted statistical errors, where red cells indicate overestimation and blue cells indicate underestimation.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot6_error_heatmaps.png}
	\caption{Heatmaps of Mean Prediction Errors for \ac{nn} and \ac{lm} Calibration Methods, and Their Difference.}
	\label{fig:error_heatmaps}
\end{figure}

The heatmaps demonstrate that the different calibration methods produce structurally distinct average error patterns. The \ac{nn} exhibits the most balanced profile. While it shows a time-averaged tendency to overpredict volatility for short-expiry instruments (e.g., a 9.97~\ac{bps} average overprediction in the [0, 5) expiry and tenor bin) and underpredict for mid-to-long expiries, the magnitude of these average errors is generally the smallest among all methods. This indicates that its calibration is the most consistent across the entire swaption matrix when averaged over the test period. In stark contrast, the Levenberg-Marquardt strategies display significant and systematic biases. The \ac{lm} Static model shows a pervasive positive bias, meaning it consistently overpredicts volatility across most of the surface, with average errors exceeding 19~\ac{bps} for some long-dated instruments. This systematic overprediction explains its higher average \ac{rmse}. The \ac{lm} Pure Rolling strategy exhibits the opposite and most extreme bias, with a severe and widespread underprediction across nearly the entire surface. Many error bins show average underpredictions greater than -15~\ac{bps}, with some exceeding -22~\ac{bps}. This is the direct consequence of the parameter drift identified earlier, where the model has diverged to a state that fundamentally misrepresents the market on average. The \ac{lm} Adaptive Anchor strategy presents a more controlled, yet still biased, profile. It tends to overpredict volatility for short-expiry swaptions (e.g., an average of 13.85~\ac{bps} in the shortest-dated bin) while underpredicting for mid- and long-expiry instruments. Although it avoids the drift of the Pure Rolling method, its average biases are structurally more pronounced than those of the \ac{nn}. Collectively, these heatmaps illustrate that the \ac{nn} learns a more flexible and accurate representation of the volatility surface, resulting in smaller and less systematic errors when evaluated over time. The traditional optimizers, by comparison, struggle to fit the entire surface simultaneously, leading to structural mispricings.

To deepen this analysis from a financial risk perspective, a second set of heatmaps was generated, as shown in figure~\ref{fig:error_heatmaps_weighted}. These plots visualize the \textit{vega-weighted} mean errors, which represent the financial impact of the calibration inaccuracies. This metric highlights the regions of the volatility surface where the model's errors would cause the largest potential P\&L discrepancies.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot6b_error_heatmaps_weighted.png}
	\caption{Heatmaps of Vega-Weighted Mean Errors for \ac{nn} and \ac{lm} Calibration, and Their Difference.}
	\label{fig:error_heatmaps_weighted}
\end{figure}

For the \ac{nn}, the error contributions are modest across the entire surface, with the largest positive contributions (indicating the biggest impact on the total error) appearing in the mid-to-long expiry and long tenor region (e.g., 10.97~\ac{bps} in the [10, 15) expiry, [25, 30) tenor bin). However, the overall low magnitude and lack of a strong structural pattern suggest that the model does not have a specific area of the market where it systematically fails in a financially significant way. The \ac{lm} strategies, in contrast, show that their previously identified biases translate into substantial financial mispricings. The \ac{lm} Static model's largest error contributions are heavily concentrated in the mid-to-long expiry and long tenor sections of the curve, with values reaching as high as 35.45~\ac{bps}. This indicates that its systematic overprediction has the most severe financial consequences for long-dated, high-vega instruments. The \ac{lm} Pure Rolling strategy again shows the most extreme behavior; its heatmap is dominated by large negative contributions, with values frequently below -30~\ac{bps} (e.g., -35.92 and -43.78~\ac{bps}). This confirms that its systemic underprediction results in a financially critical failure to price the most sensitive instruments correctly. The \ac{lm} Adaptive Anchor model mitigates these extremes but still shows larger and more structured error contributions than the \ac{nn}, with a mixed pattern of negative contributions in the mid-section and positive contributions at the long end. This vega-weighted analysis reinforces the conclusion that the \ac{nn}'s calibration is not only statistically more accurate but also financially more robust, as it avoids the large, concentrated mispricings that characterize the traditional optimization methods. it avoids the large, concentrated mispricings that characterize the traditional optimization methods.

The relationship between model-implied volatilities and observed market volatilities was examined on an instrument-by-instrument basis for all swaptions in the hold-out sets over the entire test period. Scatter plots (figure~\ref{fig:scatter_comparison}) were constructed for the \ac{nn} and \ac{lm} strategies, with the dashed \(y=x\) line representing a perfect fit, where model predictions exactly match market values. The proximity of the data points to this line provides a direct measure of each model's out-of-sample accuracy and predictive reliability.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot10_scatter_comparison.png}
	\caption{Scatter Plot Comparison of Model-Implied vs.\ Market Swaption Volatilities for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:scatter_comparison}
\end{figure}

The \ac{nn}'s predictions are clustered most closely around the reference line, demonstrating the tightest fit and lowest variance among the four models. This visual evidence aligns with the low mean error and low standard deviation reported in the statistical analysis. The distribution of its points along the diagonal indicates that the model's predictions maintain a strong linear relationship with market volatilities across the observed range.

In contrast, the \ac{lm} strategies exhibit significant deviations. The \ac{lm} Static plot shows a wide dispersion of points, with a tendency for model-implied volatilities to exceed market values, particularly at higher volatility levels. This pattern visually corroborates the positive bias identified in the heatmap analysis. The \ac{lm} Pure Rolling plot reveals a systematic underprediction, with the vast majority of data points falling well below the perfect fit line. The model's predictions are compressed into a narrow, lower range and appear largely unresponsive to the true variation in market volatility, a direct visual consequence of the parameter drift discussed previously. The \ac{lm} Adaptive Anchor strategy demonstrates an improved fit compared to the other \ac{lm} methods, with its data points being more centered around the reference line. However, a greater degree of scatter and a slight positive bias persist relative to the \ac{nn}. In aggregate, these plots indicate at the instrument level that the \ac{nn} produces more accurate and less biased predictions.

\subsubsection{Parameter Stability}
This section examines the temporal behavior of the calibrated \ac{hw} model parameters-specifically the mean-reversion rate (\(\alpha\)) and the piecewise constant volatility term structure (\(\sigma_i\))-for both the \ac{nn} and \ac{lm} (\ac{lm}) calibration methods. The analysis is based on daily recalibrations (\ac{lm} approach) and the predicted parameters (\ac{nn}) over the test period, with a focus on the stability  of the parameter trajectories. Parameter stability is of central importance, as erratic fluctuations can result in unstable hedging ratios, inconsistent pricing, and unreliable risk metrics. As described in section \ref{fig:yield_curve_selected_tenors} a moderate upwards shift of the yield curve could be observered during the testing period.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot3_parameter_evolution.png}
	\caption{Parameter Evolution for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:parameter_evolution}
\end{figure}

At first inspection, both calibration techniques yield parameters that evolve smoothly over time without abrupt jumps or discontinuities. The magnitudes of the parameters appear economically plausible, with no extreme outliers observed. This visual stability suggests that both methods achieve a baseline level of temporal consistency in the estimated \ac{hw} parameters.

The time-series plots of the calibrated parameters reveal fundamentally different behaviors across the four models within this stable market environment. The \ac{nn}'s parameters exhibit dynamic adjustments that are responsive to minor daily market fluctuations while maintaining a high degree of overall stability. The mean-reversion parameter, $\alpha$, remains within a tight and stable range around 0.01 for the entire test period. The volatility parameters, $\sigma_i$, show clear day-to-day movements but do so smoothly and without extreme fluctuations. This behavior suggests the network has learned a stable representation of the mean-reversion process while adaptively modeling the term structure of volatility, which corresponds to its consistently low calibration error.

The \ac{lm} strategies display parameter paths that directly explain their respective error characteristics. Despite the stable market, the \ac{lm} Pure Rolling strategy exhibits a pronounced and persistent upward drift in the mean-reversion parameter $\alpha$ and in several of the short-term volatility parameters (e.g., $\sigma_1$ and $\sigma_2$). This monotonic drift in a non-trending market underscores the method's susceptibility to error propagation, where even small daily calibration noises accumulate over time, causing the parameters to systematically diverge. Other volatility parameters for this method, such as $\sigma_5$
, show extremely high day-to-day volatility, indicative of numerical instability. The \ac{lm} Static strategy's parameters are also erratic; its $\alpha$ parameter exhibits sharp, day-to-day fluctuations, including a collapse towards zero mid-period. This instability, occurring even in a stable market, suggests that the optimization problem is highly sensitive and that the optimizer is frequently trapped in suboptimal local minima, struggling to converge reliably from a fixed starting point that becomes less suitable over time. The \ac{lm} Adaptive Anchor strategy successfully mitigates these issues. Its parameters are considerably more stable, avoiding the monotonic drift of the Pure Rolling approach and the extreme volatility of the Static method. This stability is why it performs best among the \ac{lm} optimizers. In summary, the time-series analysis of the model parameters confirms that the \ac{nn} produces economically sensible and stable outputs, whereas the \ac{lm} optimizers are susceptible to instability and drift, with the severity depending heavily on the initial guess strategy, even in the absence of significant market regime changes.

\subsubsection{Parameter Sensitivity Analysis}
To evaluate the economic coherence and dynamic stability of the calibrated parameters beyond their static out-of-sample performance, a series of stress tests were conducted. This analysis involved applying predefined, plausible shocks to the initial yield curve and observing the resultant parameter adjustments from both the \ac{nn} and the \ac{lm} calibration model. The objective is to assess whether the models' reactions are not only numerically stable but also consistent with established financial theory and empirical observations under different market regimes. The stress tests were performed using market data from 04.08.2025, the first day of the test period, to ensure that both approaches were evaluated from a relevant baseline where the \ac{nn} was recently calibrated and the initial guesses for the \ac{lm} approach were most applicable. The following scenarios were investigated: a parallel upward shift in the yield curve, a parallel downward shift, and a steepening twist.

\paragraph{Scenario 1: Parallel Yield Curve Shift Up (+50 \ac{bps})}
This scenario reflects a market environment characterized by rising interest rates. In response to such a shift, the \ac{nn} adjusts the mean-reversion parameter $a_1$ upward by 20.65\%, while the volatility parameters ($\sigma_i$) exhibit minor and mixed adjustments, with most decreasing slightly. This suggests the model interprets the shift as a move toward a more certain, higher-rate environment, primarily strengthening the speed of reversion to the new term structure without a corresponding increase in overall market uncertainty. In contrast, the \ac{lm} model demonstrates severe numerical instability. The mean-reversion parameter $a_1$ collapses to zero, a 100\% decrease, which implies a breakdown of the model's mean-reverting property. Simultaneously, the volatility parameters undergo large and inconsistent adjustments, such as a 15.00\% increase in $\sigma_2$ and a 15.00\% decrease in $\sigma_6$. This behavior indicates that the optimizer fails to converge to a stable or economically coherent parameter set under the stress condition.

\paragraph{Scenario 2: Parallel Yield Curve Shift Down (-50 \ac{bps})}
A parallel downward shift of 50 \ac{bps} captures an environment of monetary easing or a flight-to-quality. The \ac{nn}'s response is consistent with the previous scenario, with the mean-reversion parameter $a_1$ increasing by 13.32\% while the volatility structure remains largely unchanged. This suggests the model's learned behavior is to increase the anchoring effect of mean reversion in response to large parallel shifts, irrespective of their direction. The \ac{lm} model again displays an unstable reaction. The mean-reversion parameter increases by an extreme 319.16\%, while all volatility parameters ($\sigma_i$) decrease significantly, by as much as -17.66\% for $\sigma_7$. This combination of an exceptionally high mean-reversion speed with uniformly lower volatility is indicative of the optimizer forcing a fit with a parameter set that may not be economically justifiable, further highlighting its numerical instability.

\paragraph{Scenario 3: Yield Curve Twist (Steepening)}
The final scenario models a steepening of the yield curve, which typically signals changing expectations about future economic growth. The \ac{nn} reacts with a moderate 7.51\% increase in the mean-reversion parameter $a_1$ and minimal changes to the volatility parameters. This response is economically coherent, as a steeper curve implies a higher long-term rate anchor, which would justify a stronger mean-reverting pull. The stability of the volatility parameters suggests the model does not interpret this change in shape as a major increase in overall market uncertainty. The \ac{lm} model, however, fails to produce a robust calibration. As in the upward shift scenario, the mean-reversion parameter $a_1$ collapses to zero. The volatility parameters show large, offsetting adjustments, such as a 13.19\% increase in $\sigma_2$ and a 6.85\% decrease in $\sigma_6$. This demonstrates the optimizer's inability to robustly handle a change in the yield curve's shape, again resulting in an unstable parameterization.

Across all three scenarios, a consistent pattern emerges: the \ac{nn} produces stable, nuanced, and economically interpretable parameter adjustments. Its primary response to market shocks is to adjust the strength of the mean reversion, while maintaining a stable volatility structure. In contrast, the \ac{lm} optimizer demonstrates severe numerical instability. The mean-reversion parameter either collapses to zero or increases to extreme levels, indicating a failure to find a robust minimum in the error surface. A potential explanation for the observed instability of the \ac{lm} optimizer lies in the sensitivity of iterative optimization algorithms to their starting conditions. The stress tests were conducted using the same initial parameter guess that was determined under base-case market conditions. When a significant shock is applied to the yield curve, such as a 50 basis point parallel shift or a steepening twist, this fixed initial guess may no longer reside in a region of the error surface that is conducive to finding a stable and economically meaningful solution. An initial guess that is far from the new optimal parameter set can cause the optimizer to struggle during its iterative search, potentially leading it to converge to a suboptimal local minimum or, as observed in the collapse of the mean-reversion parameter, to fail to converge to a robust solution entirely.

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\caption{Scenario Analysis of \ac{nn} Model Parameters}
	\label{tab:scenario_analysis_part_nn}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			\toprule
			           & \multicolumn{2}{c}{$a_1$} & \multicolumn{2}{c}{$\sigma_1$} & \multicolumn{2}{c}{$\sigma_2$} & \multicolumn{2}{c}{$\sigma_3$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$               & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                           & 0.01220                        &                                & 0.00021                        &             & 0.00027    &             & 0.00026    \\
			Shift Up   & 20.65                     & 0.01471                        & -0.82                          & 0.00021                        & -2.85       & 0.00026    & -2.24       & 0.00026    \\
			Shift Down & 13.32                     & 0.01382                        & -0.62                          & 0.00021                        & -1.69       & 0.00026    & -2.04       & 0.00026    \\
			Twist      & 7.51                      & 0.01311                        & -0.37                          & 0.00021                        & -0.95       & 0.00026    & -1.26       & 0.00026    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			           & \multicolumn{2}{c}{$\sigma_4$} & \multicolumn{2}{c}{$\sigma_5$} & \multicolumn{2}{c}{$\sigma_6$} & \multicolumn{2}{c}{$\sigma_7$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$                    & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                                & 0.00025                        &                                & 0.00018                        &             & 0.00014    &             & 0.00016    \\
			Shift Up   & -6.58                          & 0.00024                        & -2.25                          & 0.00018                        & 3.16        & 0.00014    & -1.29       & 0.00015    \\
			Shift Down & -4.51                          & 0.00024                        & 0.01                           & 0.00018                        & 2.57        & 0.00014    & 0.59        & 0.00016    \\
			Twist      & -2.64                          & 0.00025                        & 0.19                           & 0.00018                        & 1.54        & 0.00014    & 0.52        & 0.00016    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

% Levenberg-Marquardt
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\caption{Scenario Analysis of \ac{lm} Model Parameters}
	\label{tab:scenario_analysis_part_lm}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			\toprule
			           & \multicolumn{2}{c}{$a_1$} & \multicolumn{2}{c}{$\sigma_1$} & \multicolumn{2}{c}{$\sigma_2$} & \multicolumn{2}{c}{$\sigma_3$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$               & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                           & 0.01250                        &                                & 0.00022                        &             & 0.00024    &             & 0.00023    \\
			Shift Up   & -100.00                   & 0.00000                        & 1.25                           & 0.00022                        & 15.00       & 0.00027    & 12.88       & 0.00026    \\
			Shift Down & 319.16                    & 0.05241                        & -6.92                          & 0.00020                        & -13.23      & 0.00021    & -14.57      & 0.00020    \\
			Twist      & -100.00                   & 0.00000                        & 1.10                           & 0.00022                        & 13.19       & 0.00027    & 10.93       & 0.00025    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			           & \multicolumn{2}{c}{$\sigma_4$} & \multicolumn{2}{c}{$\sigma_5$} & \multicolumn{2}{c}{$\sigma_6$} & \multicolumn{2}{c}{$\sigma_7$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$                    & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                                & 0.00022                        &                                & 0.00020                        &             & 0.00017    &             & 0.00018    \\
			Shift Up   & 12.64                          & 0.00025                        & 6.15                           & 0.00021                        & -15.00      & 0.00015    & 1.22        & 0.00018    \\
			Shift Down & -15.59                         & 0.00019                        & -15.70                         & 0.00017                        & -17.08      & 0.00014    & -17.66      & 0.00015    \\
			Twist      & 11.63                          & 0.00025                        & 8.90                           & 0.00022                        & -6.85       & 0.00016    & 6.97        & 0.00019    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

\subsubsection{Analysis of Computational Efficiency and Practical Applicability}
The computational performance of the \ac{nn} and \ac{lm} (\ac{lm}) calibration methods was evaluated to assess their practical suitability for real-world applications. Table~\ref{tab:comp_efficiency} summarizes the measured runtimes and variability for each approach, highlighting the substantial differences in computational efficiency.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Computational Efficiency of \ac{nn} and \ac{lm} Calibration Methods}
		\label{tab:comp_efficiency}
		\begin{tabular}{lcc}
			\toprule
			Method                       & Mean Time (s) & Std Dev Time (s) \\
			\midrule
			\ac{nn}                      & 0.0041        & 0.0019           \\
			\ac{lm} (Static)             & 80.34         & 18.48            \\
			\ac{lm} (Pure Rolling)       & 80.48         & 16.36            \\
			\ac{lm} (Adaptive Anchor) & 72.25         & 13.37            \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The table reports average runtime and standard deviation for daily calibrations. The \ac{nn} achieves an approximate speed-up of 17,621x compared to the fastest \ac{lm} approach.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

The \ac{nn} exhibits a significant advantage in computational speed for daily calibration tasks. With an average runtime of approximately 4 milliseconds per calibration, it is approximately 17,621 times faster than the most efficient traditional optimizer, the LM (Adaptive Anchor), which requires an average of 72.25 seconds. The other \ac{lm} strategies are even slower, with the Static and Pure Rolling methods requiring 80.34 and 80.44 seconds on average, respectively. This multi-order-of-magnitude improvement fundamentally transforms the operational feasibility of high-frequency calibration workflows. This observed acceleration is consistent with findings in related literature. For instance, \textcite{alaya2021deep} documented a speedup of approximately 7,600 times when applying a \ac{nn} to the calibration of the G2++ model. The acceleration factor of approximately 17,621, observed when comparing the \ac{nn} to the most efficient \ac{lm} strategy in this study, is therefore of a comparable order of magnitude, reinforcing the conclusion that deep learning methods can offer substantial efficiency gains for complex financial model calibration tasks.

This computational advantage arises from the methodological distinction between the two approaches. The \ac{nn} incurs the majority of its computational cost during an offline training phase. Once trained, \ac{nn} performs calibration via a near-instantaneous forward pass (inference). In contrast, each of the \ac{lm} algorithms performs a fully iterative numerical optimization online for each new market snapshot, making them substantially more resource-intensive in day-to-day operations.

The initial offline cost of training the \ac{nn} is efficiently amortized over its operational lifetime. Given that optimal hyperparameters are expected to remain stable, the model would likely require only periodic retraining (e.g., nightly or weekly) to maintain its accuracy. This allows a high volume of real-time calibrations to be executed with minimal computational overhead, rendering the upfront investment highly cost-effective in a production environment.

The sub-second runtime of the \ac{nn} makes it suitable for applications that are infeasible with the \ac{lm} methods, including real-time risk management, pre-trade pricing of extensive derivative portfolios, and intraday recalibration in response to evolving market conditions.

\subsection{Interpretability of the Neural Network via SHAP Values}
The interpretability of the \ac{nn} was examined through \ac{shap} values, which quantify the contribution of each input feature to the model's predictions for the \ac{hw} parameters. This analysis provides insight into the internal decision-making process of the \ac{nn}, allowing for a scientific understanding of how it generates parameter estimates from market data.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/SHAP_summary_a_1.png}
	\caption{\ac{shap} Summary Plot for the Mean-Reversion Parameter $\alpha_1$.}
	\label{fig:shap_summary_alpha_1}
\end{figure}

For the mean-reversion parameter, $\alpha_1$, the feature importance plot identifies 'curvature\_10y20y30y', which represents the curvature of the long end of the yield curve, as having the single largest impact. The corresponding summary plot shows that high values of this feature (indicated by red dots), corresponding to a more pronounced curvature, consistently produce negative SHAP values. This signifies that the model has learned to associate a more curved long-end of the term structure with a decrease in the predicted mean-reversion speed. This relationship aligns with financial theory, where a highly curved long end suggests greater market uncertainty about the long-term anchor for interest rates, which in turn leads to a weaker reverting tendency for the short rate. The next most influential features for $\alpha_1$ are the slope measures, 'slope\_3m10y' and 'slope\_5y30y'. The summary plot indicates that high values for these features, representing a steeper yield curve, also contribute to a lower mean-reversion parameter. This is also economically coherent, as a steep curve implies market expectations of rising future rates, a dynamic that is contrary to the concept of reversion to a stable, long-term mean.

Across the volatility parameters ($\sigma_i$), a different but equally consistent pattern emerges. The 'curvature\_10y20y30y' feature again stands out as the most dominant predictor for nearly the entire volatility term structure, from $\sigma_1$ through $\sigma_5$ and again for $\sigma_7$. However, its directional impact is the inverse of its effect on mean reversion. The summary plots for these parameters consistently show that high values of long-end curvature generate positive SHAP values, thereby increasing the predicted volatility. This dual effect is a key insight into the model's learned logic: the same market condition of high long-end curvature that weakens the mean-reversion anchor is interpreted as simultaneously signaling greater market uncertainty, which correctly translates to higher calibrated volatility.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/SHAP_summary_sigma_7.png}
	\caption{\ac{shap} Summary Plot for the Long-Term Volatility Parameter \(\sigma_7\).}
	\label{fig:shap_summary_sigma_7}
\end{figure}

A characteristic of the model is its adaptive re-weighting of feature importance for different parts of the volatility term structure. This is most evident when comparing the drivers of short-term volatility ($\sigma_1$) with those of long-term volatility ($\sigma_7$). For predicting short-term volatility, the model prioritizes features describing the immediate yield curve environment; after the dominant long-end curvature, the broad 'slope\_3m10y' is the second most important feature, while the systemic risk measure 'MOVE\_VIX\_Ratio' is ranked near the bottom (12th out of 14). In stark contrast, for predicting the longest-term volatility ($\sigma_7$), the feature hierarchy undergoes a significant transformation. The 'MOVE\_VIX\_Ratio' elevates to become the second most important feature, indicating the model has learned that long-horizon uncertainty is heavily influenced by systemic, cross-asset risk sentiment. Concurrently, the model's focus shifts from broad slopes to the specific shape of the long end, with 'slope\_5y30y' gaining substantial importance. Furthermore, the model also assigns high importance to shorter-end curvature measures like 'curvature\_1y2y5y' when predicting $\sigma_7$, suggesting it is capturing the complex interaction or 'twist' between the shape of the short/medium end and the long end as a key indicator of long-term market expectations.

The \ac{shap} summary plots also reveal complex, non-linear relationships. The impact of the 'MOVE\_VIX\_Ratio' on $\sigma_7$ provides a clear example. High values of this ratio, which indicate heightened stress in the bond market relative to the equity market, do not have a simple monotonic effect. Instead, the red dots are spread across both positive and negative SHAP values. This suggests the model has captured a nuanced, regime-dependent relationship. It may be distinguishing between general market stress that elevates long-term volatility and extreme "flight-to-quality" events where severe market stress could paradoxically suppress long-term interest rate volatility due to massive inflows into safe-haven assets. This complex pattern is a feature that a linear model would be unable to capture.

Furthermore, the analysis consistently shows that the 'EURUSD\_Open' feature has a minimal impact across all parameter predictions, regularly appearing as one of the least important variables. This finding is economically sound. While the FX rate is influenced by relative interest rate differentials and macroeconomic conditions, the primary drivers for a single-currency interest rate model like the \ac{hw} are more directly captured by the yield curve's own structure (level, slope, curvature) and interest rate volatility metrics. The \ac{nn} appears to have correctly identified that the FX rate provides largely redundant or second-order information for this specific calibration task, as its predictive signal is already contained within the more direct market features. This demonstrates the model's ability to distinguish between primary drivers and less informative, correlated variables. In aggregate, the \ac{shap} analysis demonstrates that the \ac{nn} has internalized a set of complex yet financially coherent relationships to arrive at its predictions.

Note: All other plots regarding the \ac{shap} values can be found at the appendix \ref{appendix:shap_values}.

\subsection{Comparison with \textcite{hernandez2016model}}
\textcite{hernandez2016model} investigated the calibration of a simplified one-factor \ac{hw} model characterized by a single mean-reversion parameter $\alpha$ and a single volatility parameter $\sigma$. His study was based on 156 GBP \ac{atm} swaptions observed between 2013 and 2016. For the estimation task, he employed a feed-forward \ac{nn} with four hidden layers, trained to directly predict the parameters $(\alpha, \sigma)$ of the \ac{hw} model. In contrast, the \ac{nn} developed in the present thesis consists of five layers and utilizes a residual parameterization approach, in which the model learns to predict corrections to an initial guess of the parameters rather than the parameters themselves. This design choice aims to enhance numerical stability and convergence properties, particularly in non-linear calibration problems.

The methodological distinction between both approaches is substantial. \textcite{hernandez2016model} \ac{nn} was trained using pre-calibrated parameter pairs obtained from a \ac{lm} optimization as target variables. The \ac{lm} optimizer represented the traditional calibration benchmark in his study, providing a set of optimal parameters for each observation date. Conversely, the approach presented in this thesis does not rely on such precomputed targets. Instead, the predicted parameters are directly inserted into the QuantLib pricing engine, and the \ac{nn} is trained by minimizing the deviation between the model-implied and market-observed swaption volatilities. Formally, this corresponds to minimizing the loss function
\[
	L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left( \sigma^{\text{model}}_i(\alpha(\theta), \sigma(\theta)) - \sigma^{\text{market}}_i \right)^2,
\]
where $\sigma^{\text{model}}_i$ and $\sigma^{\text{market}}_i$ denote model-implied and market-observed volatilities, respectively, and $\theta$ represents the \ac{nn} parameters (see sections \ref{calibration} and \ref{subsec:loss_function_and_error_metric}). This direct loss formulation allows the network to learn parameter mappings that minimize actual pricing errors rather than reproducing the outcomes of an external optimizer. However, the approach entails higher computational costs because QuantLib's pricing routines are implemented in C++ and do not expose analytical gradients to TensorFlow's automatic differentiation, making backpropagation computationally demanding (see section \ref{appendix:algorithmic_and_numerical_efficiency}).

Regarding input representation, \textcite{hernandez2016model} incorporated both yield curve information and swaption data into the \ac{nn}'s feature vector. Specifically, he employed a 6-month tenor \ac{libor} curve discretized at 44 maturity points, ranging from 0 days to 50 years, including tenors of 0, 1, 2, 7, and 14 days; 1--24 months; 3--10 years; and 12, 15, 20, 25, 30, and 50 years. \ac{pca} was subsequently applied to retain 99.5\% of the variance. Additionally, the 156 swaption volatilities formed a major component of the input vector. In contrast, the this thesis did not use swaption volatilities as input features but instead relied on alternative market information to ensure that the model's predictive power stemmed from broader market dynamics rather than direct encoding of the target variable.

Both studies calibrated their respective models to the full swaption volatility surface. To ensure comparability, the results obtained in this thesis, originally expressed in Bachelier (normal) volatilities, were converted to Black (lognormal) volatilities and then weighted by the corresponding vegas using the approach described in section \ref{subsec:conversion_from_normal_to_lognormal_errors}, consistent with the convention used by \textcite{hernandez2016model}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot4_daily_rmse_black_vol.png}
	\caption{Average Daily \ac{rmse} in Black Volatilities for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:daily_rmse_black_vol}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/hernandez_calibration_results.png}
	\caption{Hernandez's: Average Daily \ac{rmse} in Black Volatilities for \ac{nn} and \ac{lm} Calibration Methods. Source: \parencite[figure~3]{hernandez2016model}}
	\label{fig:daily_rmse_black_vol_herandez}
\end{figure}


A direct comparison of the empirical results from both studies reveals significant differences in the performance of the respective \ac{nn} models relative to their traditional benchmarks. The focus of this comparison is placed on the initial month of the out-of-sample period in both analyses. This approach ensures a methodologically sound comparison, as the time elapsed since the training of the respective \ac{nn} and the calibration of the initial guess for the static benchmark models is analogous. While the absolute error magnitudes are not directly comparable due to differences in the underlying datasets, currencies, and market conditions, the relative performance of each \ac{nn} against its own set of traditional benchmarks provides a valid basis for comparison.

The \ac{nn} developed in this thesis maintains a consistent and substantial performance advantage over all three \ac{lm} strategies throughout the test period. Its vega-weighted \ac{rmse} remains stable at approximately 1.5\%, which is quantitatively lower than the approximately 2.0\% to 4.0\% error of the best-performing benchmark, the LM (Adaptive Anchor). In contrast, the results from \textcite{hernandez2016model} show a different dynamic. The performance of his feed-forward \ac{nn} is closely aligned with that of the benchmark methods. Specifically, the \ac{nn}, the 'Historical Starting Point' strategy (which corresponds to the 'Pure Rolling' strategy in this work), and the 'Default Starting Point' strategy (which corresponds to the '\ac{lm} (Static)' strategy) all exhibit a comparable error level of approximately 4-5\% and track each other closely during this initial period.

This disparity in the relative performance gap may be explained by the fundamental difference in the training objectives of the two \ac{nn}. The \ac{nn} in \textcite{hernandez2016model} was trained to replicate the parameters generated by the \ac{lm} optimizer, which inherently limits its potential performance to, at best, the accuracy of the optimizer it is mimicking. In contrast, the \ac{nn} in this thesis is trained to directly minimize the pricing error between model-implied and market-observed volatilities. This direct optimization framework allows \ac{nn} to learn a parameter mapping that is not constrained by the limitations of the \ac{lm} method and may discover solutions that yield a lower out-of-sample pricing error than the traditional optimizer can consistently achieve. A further contributing factor to the enhanced relative performance of the \ac{nn} in this study is the strategic incorporation of economically motivated, engineered features. Unlike the \textcite{hernandez2016model} approach, which relied primarily on \ac{pca}-compressed yield curve data, this thesis constructed a richer feature set that included explicit measures of the yield curve's shape—such as various slope and curvature metrics—and external market indicators like the \ac{vix} and \ac{move} indices. The significance of this feature engineering is validated by the \ac{shap} analysis, which revealed that these engineered features were not peripheral but central to the model's decision-making process. For instance, the long-end curvature ('curvature\_10y20y30y') was identified as the single most dominant driver for both the mean-reversion parameter and most of the volatility term structure. Similarly, the engineered 'MOVE\_VIX\_Ratio' was found to be a critical predictor for long-term volatility, demonstrating the model's ability to learn from higher-order, cross-market risk signals. By providing the \ac{nn} with these more direct and interpretable signals about the underlying market state, the model was likely better equipped to learn the complex, non-linear relationships governing the \ac{hw} parameters. This contrasts with a model that must infer such dynamics implicitly from a less processed set of inputs, and might have contributed to the superior generalization capabilities and the wider performance gap observed relative to the traditional benchmarks.

A further significant difference is observed in the behavior of the benchmark that utilizes historical parameters as an initial guess. In this thesis, the 'Pure Rolling' strategy was found to be highly unstable, exhibiting significant error propagation and performance degradation over time, making it the worst-performing method. Conversely, \textcite{hernandez2016model} identifies the 'Historical Starting Point' as his most robust benchmark, which performs comparably to his \ac{nn} throughout the out-of-sample period and does not show evidence of catastrophic drift. This discrepancy may be attributable to several methodological differences, including the higher parameter dimensionality of the model used in this thesis (eight parameters versus two).

A further difference between the studies lies in the temporal stability of the models. \textcite{hernandez2016model} reported a degradation in performance of his \ac{nn} approximately six to twelve months after training, implying limited temporal generalization. Due to the restricted testing period of only one month in this thesis, no such degradation could be observed, and further analysis over a longer horizon would be required to assess long-term stability.

It is important to note several limitations that constrain the comparability of both studies. First, \textcite{hernandez2016model} employed a simplified version of the \ac{hw} model with only one mean-reversion and one volatility parameter, inherently reducing its flexibility to fit complex swaption surfaces. Second, his \ac{nn} benefited from a substantially larger training dataset, as he synthetically generated approximately 150,000 samples, whereas the present thesis relied exclusively on empirical data, limiting both the temporal coverage and sample size. Furthermore, a crucial distinction lies in the evaluation protocol; this thesis enforced a strict out-of-sample test for both the \ac{nn} and the \ac{lm} optimizer using an intra-day hold-out set, ensuring a direct comparison of their generalization capabilities. In contrast, the benchmark in \textcite{hernandez2016model} work was the in-sample fit of the traditional optimizer, which does not measure predictive performance on unseen data. Consequently, while the lower calibration errors observed in this work indicate improved model fit, they must be interpreted cautiously when compared to the results of \textcite{hernandez2016model} given the differences in model complexity, data volume, and testing horizon.

In summary, while both studies demonstrate the viability of using \ac{nn} for model calibration, the results of this thesis indicate a larger performance gain relative to traditional methods than was observed by \textcite{hernandez2016model}. This enhanced relative performance is likely attributable to key methodological advancements in the training paradigm and feature engineering. Specifically, the direct error minimization framework likely empowers the \ac{nn} to discover parameter mappings that may be superior to those found by the \ac{lm} optimizer, rather than being constrained to merely replicating them. Furthermore, the reliance on a curated set of economically motivated features likely fostered the learning of a more robust and generalizable model. These factors, taken together, suggest that the specific design choices of end-to-end training and informed feature engineering are beneficial to further increase the potential of \ac{nn} for complex financial calibration tasks, leading to models that not only match but can outperform their traditional counterparts in out-of-sample generalization, especially if their initial guess has not been chosen carefully.