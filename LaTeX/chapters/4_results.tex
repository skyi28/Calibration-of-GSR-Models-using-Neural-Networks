Building upon the comprehensive methodology detailed previously, this chapter presents the empirical findings of the comparative analysis. The discussion is structured to build from foundational model validation to the core performance comparison. It begins by interpreting the results of the \ac{pca} on the yield curve, validating the feature engineering process. This is followed by a detailed analysis of the hyperparameter tuning phase, which identified the optimal architecture for the \ac{nn} model.

With the models established, the chapter proceeds to the central, multi-faceted comparison between the predictive \ac{nn} and the traditional \ac{lm} optimizer. This evaluation is structured around the key criteria of out-of-sample pricing accuracy, the stability and economic interpretability of the derived model parameters under simulated market shocks, and the computational efficiency measured by calibration runtime.

To address the common criticism of machine learning models as "black boxes", a dedicated analysis of the \ac{nn}'s interpretability is presented using \ac{shap} values, revealing the key market drivers behind its predictions. Finally, the results are contextualized through a direct comparison with the benchmark study by \textcite{hernandez2016model}, positioning this thesis's findings and contributions within the existing body of research.

\subsection{Interpretation of Principal Component Analysis on the Yield Curve}
The application of \ac{pca} to the yield curve provides a powerful framework for decomposing the complex dynamics of interest rate movements into a small number of uncorrelated factors. The empirical results of the analysis indicate that the first three principal components collectively explain 99.84\% of the total variance observed in daily yield curve changes. This finding confirms that the term structure of interest rates can be effectively described by a limited set of underlying factors, each representing a distinct pattern of yield curve movement.

\begin{table}[H]
	\centering
	\caption{Explained Variance of the Principal Components}
	\label{tab:pca_variance}
	\begin{tabular}{lcc}
		\toprule
		Principal Component & Explained Variance (\%) & Cumulative Variance (\%) \\
		\midrule
		PC1 (Level)         & 91.08                   & 91.08                    \\
		PC2 (Slope)         & 8.16                    & 99.24                    \\
		PC3 (Curvature)     & 0.59                    & 99.84                    \\
		\bottomrule
	\end{tabular}
\end{table}

The first principal component (PC1) accounts for 91.08\% of the total variance and clearly emerges as the dominant driver of yield curve fluctuations. The loading plot of PC1 reveals positive loadings across all maturities, indicating that variations in this factor correspond to parallel shifts in the yield curve. In other words, changes in PC1 lead to simultaneous increases or decreases in yields across all tenors, representing the so-called level factor. This component captures broad-based movements in the general level of interest rates and is thus associated with macroeconomic influences such as monetary policy shifts and long-term inflation expectations.

The second principal component (PC2), explaining 8.16\% of the total variance, can be interpreted as the slope factor. Its loading structure is characterized by negative values for short maturities and positive values for longer maturities, reflecting an inverse relationship between short- and long-term rates. When this factor increases, the yield curve steepens-short-term yields decline while long-term yields rise. Conversely, a decrease in this component leads to a flattening of the curve. The slope factor therefore captures non-parallel shifts in the term structure and is closely related to expectations regarding future economic growth and central bank policy adjustments.

The third principal component (PC3) explains 0.59\% of the total variance and represents the curvature factor. Its loading pattern exhibits a distinct hump shape, with positive loadings at the short and long ends of the maturity spectrum and negative loadings in the intermediate maturities. This configuration indicates that variations in PC3 alter the concavity of the yield curve, producing so-called butterfly movements. Such changes often occur when medium-term interest rates move differently from short- and long-term rates, providing information about market expectations of medium-term monetary policy and term premia.

The empirical findings are consistent with the theoretical and empirical literature on interest rate modeling, particularly the work of \parencite[pp.~98--107]{Rebonato_2018}. The identification of level, slope, and curvature as the three principal components of the yield curve is a well-established result in fixed-income research. Their hierarchical importance-where the level factor dominates, followed by the slope and curvature factors-reflects a universal characteristic of yield curve dynamics across different markets and time periods.

The concentration of explanatory power within the first three components demonstrates the suitability of PCA for dimensionality reduction in yield curve modeling. Instead of modeling the dynamics of nine highly correlated interest rates, the analysis can be simplified to three orthogonal factors that capture nearly all relevant variation. This dimensionality reduction not only mitigates model complexity and overfitting risk but also enhances interpretability by linking statistical factors to economically meaningful concepts. Furthermore, such a factor-based representation supports practical applications in risk management, hedging, and scenario analysis, as the identified components correspond directly to the main sources of interest rate risk in the term structure.

\subsection{Hyperparameter Tuning}
To identify the most suitable model configuration, a comprehensive hyperparameter search was conducted using the Hyperband algorithm with a total of 1000 trials. Model performance was evaluated based on the unweighted \ac{rmse} computed on the validation dataset (\textit{val\_rmse}). For subsequent analysis, only the top 5\% of all tested configurations were retained, resulting in a subset of 50 models that minimized the validation \ac{rmse}. Within this subset, Pearson correlation coefficients were calculated to quantify the linear dependencies among the numerical hyperparameters, as well as their individual relationships with the validation error. The resulting correlation matrix and correlation vector serve as diagnostic tools for assessing parameter interactions and their effects on model performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/hyperparameters/hyperparameter_correlation_heatmap.png}
	\caption{Correlation between the optimized numerical hyperparameters.}
	\label{fig:hyperparameter_correlation}
\end{figure}

The correlation matrix of numerical hyperparameters in figure \ref{fig:hyperparameter_correlation} reveals that most pairwise correlations are weak, as indicated by predominantly light blue and grey cells in the respective heatmap. This finding suggests that the search space of the best-performing models is not strongly restricted by interdependencies among parameters. Nevertheless, a moderate negative correlation of -0.34 between \textit{neurons\_2} and \textit{neurons\_3} can be observed, indicating that an increase in the capacity of the third hidden layer is often accompanied by a reduction in the fourth layer's size. This pattern may reflect an implicit constraint on the overall capacity distribution across deeper layers. Furthermore, weak positive correlations were detected between \textit{neurons\_0} and both \textit{dropout\_rate} (0.28) and \textit{learning\_rate} (0.26), implying that larger initial layers might benefit from slightly higher regularization and more aggressive learning rates to achieve optimal performance.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/hyperparameters/error_feature_correlation_heatmap.png}
	\caption{Correlation between the optimized numerical hyperparameters and the validation error.}
	\label{fig:error_feature_correlation_heatmap}
\end{figure}

The correlation analysis provided in figure \ref{fig:error_feature_correlation_heatmap} between hyperparameters and model error provides additional insights into performance sensitivity. Since the objective is to minimize \textit{val\_rmse}, a negative correlation indicates an improvement in performance with increasing hyperparameter values, while a positive correlation suggests performance deterioration. The strongest observed relationship is a positive correlation of 0.46 for the \textit{underestimation\_penalty}, indicating that higher values of this penalty consistently degrade performance. Consequently, the optimal configuration for this parameter likely lies near zero. In contrast, a moderate negative correlation of -0.29 between \textit{num\_layers} and error suggests that deeper network architectures tend to yield superior performance within the top-tier models. Similarly, the parameters \textit{neurons\_0} (-0.22) and \textit{neurons\_3} (-0.15) exhibit weak negative correlations with error, indicating a slight preference for wider layers at specific network depths. The regularization term \textit{dropout\_rate} shows a mild positive correlation (0.13) with error, implying that excessive dropout may hinder learning even among the best configurations. Lastly, the parameter \textit{tuner/epochs} is negatively correlated with error (-0.14), confirming that extended training durations within the Hyperband framework generally improve convergence and validation performance.

The architecture of the best-performing model reflects these findings. It consists of five hidden layers with 112, 48, 32, 112, and 48 neurons, respectively. All layers employ the \ac{relu} activation function. Although a dropout rate of 0.4 was specified in the search space, dropout was disabled in this configuration. The model uses a learning rate of approximately 0.0033 and applies an underestimation penalty of 1.5, indicating the use of a customized loss formulation to control prediction asymmetry. The corresponding hyperparameter configuration is summarized in table~\ref{tab:best_hyperparameters}.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Hyperparameter Configuration of the Best-Performing Model}
		\label{tab:best_hyperparameters}
		\begin{tabular}{lcc}
			\toprule
			\textbf{Hyperparameter}  & \textbf{Description}        & \textbf{Value} \\
			\midrule
			num\_layers              & Number of hidden layers     & 5              \\
			neurons\_0               & Neurons in layer 1          & 112            \\
			neurons\_1               & Neurons in layer 2          & 48             \\
			neurons\_2               & Neurons in layer 3          & 32             \\
			neurons\_3               & Neurons in layer 4          & 112            \\
			neurons\_4               & Neurons in layer 5          & 48             \\
			activation               & Activation function         & \ac{relu}      \\
			use\_dropout             & Dropout enabled             & False          \\
			dropout\_rate            & Dropout rate (inactive)     & 0.4            \\
			learning\_rate           & Optimizer learning rate     & 0.0033         \\
			underestimation\_penalty & Penalty for underestimation & 1.5            \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item The table reports the optimal hyperparameter values obtained from the Hyperband search. Dropout was disabled in the final configuration, rendering the specified dropout rate inactive. The underestimation penalty indicates the inclusion of a custom asymmetric loss component.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\subsection{Out-of-Sample Performance}
\subsubsection{Pricing Ability}
Figure~\ref{fig:daily_rmse_comparison} depicts the daily out-of-sample \ac{rmse} for the \ac{nn} and the traditional \ac{lm} calibration methods over the test period from early August to early September 2025. The evaluation is conducted on a hold-out set of swaptions that were excluded from the \ac{lm} optimization on each respective day, ensuring an unbiased comparison between the \ac{nn}'s predictive capability and the \ac{lm}'s in-sample fitting performance as described in section~\ref{subsec:comparison_of_nn_and_lm}.

To provide a metric that reflects the financial significance of the calibration errors, rather than just their statistical magnitude, the subsequent analysis additionally utilizes vega-weighted values. The formal procedure for this vega-based aggregation, which translates volatility errors into their approximate price impact, is specified in section~\ref{subsec:conversion_from_normal_to_lognormal_errors}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot1_daily_rmse_combined.png}
	\caption{Comparison of daily out-of-sample \ac{rmse} between \ac{nn} and \ac{lm} calibration methods over the test period.}
	\label{fig:daily_rmse_comparison}
\end{figure}

Across the entire test horizon, the \ac{nn} consistently achieves superior out-of-sample accuracy relative to the \ac{lm} algorithm. To provide a more nuanced evaluation beyond a simple statistical average, this analysis considers both the standard, unweighted \ac{rmse} and a vega-weighted \ac{rmse}. The latter metric assesses performance by giving more weight to errors on swaptions with higher price sensitivity to volatility, thereby reflecting financial significance. On both metrics, the \ac{nn} remains persistently superior, indicating a more accurate and robust reproduction of market swaption volatilities. Table~\ref{tab:rmse_statistics} summarizes the descriptive statistics for all four daily out-of-sample \ac{rmse} measures.

\begin{table}[htbp]
	\centering
	\begin{threeparttable}
		\caption{Out-of-Sample \ac{rmse} Statistics (Unweighted vs. Vega-Weighted)}
		\label{tab:rmse_statistics}
		\begin{tabular}{lccccc}
			\toprule
			\textbf{Method} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Median} & \textbf{Min} & \textbf{Max} \\
			\midrule
			NN (Unweighted)     & 6.53          & 0.73             & 6.47            & 5.04         & 8.53         \\
			NN (Vega-Weighted)  & 4.89          & 0.68             & 4.87            & 3.67         & 6.80         \\
			\midrule
			LM (Unweighted)     & 10.11         & 1.10             & 9.79            & 7.67         & 12.37        \\
			LM (Vega-Weighted)  & 11.83         & 1.95             & 11.30           & 7.46         & 15.39        \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The table reports summary statistics of daily out-of-sample \ac{rmse} values over the test period for both unweighted and vega-weighted metrics. All values are in basis points.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

A key finding revealed by this dual-metric approach is the relationship between the two error measures for the \ac{nn}. The vega-weighted \ac{rmse} (mean of 4.89~\ac{bps}) is consistently and significantly lower than its unweighted counterpart (mean of 6.53~\ac{bps}). This indicates that the neural network's smallest errors are concentrated on swaptions with the highest vega - typically the most liquid and financially significant instruments. The model has implicitly learned to prioritize the accuracy of the swaptions that have the largest impact on portfolio P\&L, demonstrating a level of financially-aware generalization.

In stark contrast, the \ac{lm} algorithm exhibits the opposite and more problematic behavior. Its vega-weighted \ac{rmse} (mean of 11.83~\ac{bps}) is significantly higher than the unweighted metric (mean of 10.11~\ac{bps}). This reveals a critical weakness of the traditional optimizer: its largest pricing errors are concentrated on the most financially sensitive instruments. While attempting to find a global best-fit, the \ac{lm} method struggles most with the very swaptions where accuracy is paramount, leading to a financially riskier error profile.

Finally, this analysis underscores the superior stability of the \ac{nn}. This is reflected in the standard deviation of its unweighted \ac{rmse} (0.73~\ac{bps}) and its even more stable vega-weighted \ac{rmse} (0.68~\ac{bps}). Conversely, the financial instability of the \ac{lm} method is highlighted by the substantial increase in volatility when moving from its unweighted (1.10~\ac{bps}) to its vega-weighted (1.95~\ac{bps}) error, confirming that its largest and most volatile errors occur on the instruments of highest financial importance.

To formally assess the statistical properties of the daily out-of-sample \ac{rmse} values for both calibration methods, a series of hypothesis tests were conducted. These include normality testing using the Shapiro-Wilk test, a non-parametric Mann-Whitney~U test to examine differences in central tendency, and Levene's test to evaluate equality of variances. The results are summarized in table~\ref{tab:stat_tests}.

\begin{table}[htbp]
	\centering
	\begin{threeparttable}
		\caption{Statistical Tests on Out-of-Sample \ac{rmse} Distributions}
		\label{tab:stat_tests}
		\begin{tabular}{lp{2.5cm}ccp{3.5cm}}
			\toprule
			\textbf{Test}   & \textbf{Method}      & \textbf{Statistic} & \textbf{p-value} & \textbf{Interpretation}                       \\
			\midrule
			Shapiro--Wilk   & \ac{nn}              & 0.9547             & 0.2588           & Errors are normally distributed               \\
			Shapiro--Wilk   & \ac{lm}              & 0.9263             & 0.0496           & Errors deviate from normality                 \\
			Mann--Whitney~U & \ac{nn} vs.\ \ac{lm} & 3.0000             & $<0.0001$        & Significant difference in error distributions \\
			Levene's        & \ac{nn} vs.\ \ac{lm} & 1.9225             & 0.1713           & No significant difference in variances        \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The tests were conducted on the unweighted errors.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

The Shapiro-Wilk test results indicate that the \ac{nn} errors (\(p=0.2588\)) do not significantly deviate from normality at any conventional significance level (\(\alpha \in \{0.01, 0.05, 0.1\}\)). In contrast, the \ac{lm} errors (\(p=0.0496\)) fail the normality assumption at \(\alpha=0.05\) and \(\alpha=0.1\), suggesting slight non-normal behavior.

Given this, the Mann-Whitney~U test was used to evaluate whether the two error distributions differ in central tendency. The test statistic (\(U=3.00\), \(p<0.0001\)) strongly rejects the null hypothesis across all significance levels, confirming that the \ac{nn}'s errors are significantly lower than those of the \ac{lm} method.

Finally, Levene's test (\(p=0.1713\)) indicates no statistically significant difference in the variances of the two \ac{rmse} distributions, implying comparable dispersion. Combined, these results confirm that the \ac{nn} calibration achieves a statistically and economically meaningful improvement in accuracy without introducing greater variability.

The magnitude of the observed performance difference is economically and statistically significant. On average, the \ac{nn} reduces the out-of-sample pricing error by 3.58~\ac{bps} compared to the \ac{lm} calibration. During periods of elevated instability in the \ac{lm} algorithm's performance, this gap widens up to 4.57~\ac{bps}.

The mean prediction errors, defined as the difference between model-implied volatilities and observed market volatilities (\(\text{Model Volatility} - \text{Market Volatility}\)) in basis points (\ac{bps}), were analyzed for the \ac{nn} and \ac{lm} calibration methods across various swaption expiry and tenor bins. The corresponding heatmaps in figure~\ref{fig:error_heatmaps} visualize these unweighted statistical errors, where red cells indicate overestimation and blue cells indicate underestimation.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot6_error_heatmaps.png}
	\caption{Heatmaps of Mean Prediction Errors for \ac{nn} and \ac{lm} Calibration Methods, and Their Difference.}
	\label{fig:error_heatmaps}
\end{figure}

The \ac{lm} calibration exhibits pronounced, systematic pricing biases across the surface. At the short end, errors reach up to +13.75~\ac{bps}, while at the long end, they are as high as +13.93~\ac{bps}. Conversely, the \ac{lm} model underestimates volatilities in the medium-expiry region, with errors down to -9.84 \ac{bps}. This pattern demonstrates that the in-sample optimization struggles to identify a single set of parameters capable of simultaneously fitting the complex curvature of the surface. In contrast, the \ac{nn} model produces a more balanced and less systematic error distribution. Although it slightly overestimates volatilities at the highly volatile short end (+14.51~\ac{bps}), the errors elsewhere are smaller and more evenly distributed.

To deepen this analysis from a financial risk perspective, a second set of heatmaps was generated, as shown in figure~\ref{fig:error_heatmaps_weighted}. These plots visualize the \textit{vega-weighted} mean errors, which represent the financial impact of the calibration inaccuracies. This metric highlights the regions of the volatility surface where the model's errors would cause the largest potential P\&L discrepancies.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot6b_error_heatmaps_weighted.png}
	\caption{Heatmaps of Vega-Weighted Mean Errors for \ac{nn} and \ac{lm} Calibration, and Their Difference.}
	\label{fig:error_heatmaps_weighted}
\end{figure}

The vega-weighted analysis reveals a critical divergence in the models' performance. For the \ac{nn}, the financial impact of its errors is relatively contained and distributed. While the largest unweighted error was at the short end, the largest weighted errors (e.g., 8.55, 11.04~\ac{bps}) are shifted to the mid-to-long tenor and expiry regions. This indicates that the \ac{nn}'s largest statistical mispricings occur on instruments with lower financial sensitivity.

The \ac{lm} model, however, displays a starkly problematic pattern. The vega-weighted heatmap shows an extreme concentration of financial error in the long-expiry, long-tenor portion of the surface, with weighted errors reaching as high as 27.92~\ac{bps}. This demonstrates that the \ac{lm} optimizer's largest statistical errors are systematically located in the region of the surface corresponding to high-vega, financially critical instruments. While its unweighted errors appeared more dispersed, this analysis reveals that its flaws are concentrated where they are most damaging from a risk-management perspective.

The weighted error difference heatmap (\ac{lm} minus \ac{nn}) confirms this conclusion unequivocally. The bright red areas in the long-expiry, long-tenor corner, with differences exceeding 21~\ac{bps}, highlight the \ac{lm} model's profound financial underperformance in this critical region. Conversely, the dark blue areas in the medium-expiry range show where the \ac{lm}'s underestimation bias has a greater financial impact than the \ac{nn}'s errors. Overall, the vega-weighted analysis proves that the \ac{nn} not only achieves superior statistical accuracy but also produces a significantly more benign and robust financial error profile, avoiding the dangerous concentration of risk inherent in the traditional \ac{lm} calibration.

The relationship between model-implied volatilities and observed market volatilities was examined on an instrument-by-instrument basis for all swaptions in the hold-out sets over the entire test period. Scatter plots (figure~\ref{fig:scatter_comparison}) were constructed for both the \ac{nn} and \ac{lm} (\ac{lm}) models, with the dashed \(y=x\) line representing a perfect fit, where model predictions exactly match market values. The proximity of the data points to this line provides a direct measure of each model's out-of-sample accuracy and predictive reliability.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot10_scatter_comparison.png}
	\caption{Scatter Plot Comparison of Model-Implied vs.\ Market Swaption Volatilities for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:scatter_comparison}
\end{figure}

The \ac{nn} model demonstrates a markedly better goodness-of-fit. Its data points are more tightly clustered around the \(y=x\) line, exhibiting a strong positive correlation with market volatilities across the full range of observed values. This indicates that the \ac{nn} consistently produces volatilities that closely align with unseen market data, confirming its ability to generalize beyond the calibration set.

In contrast, the \ac{lm} model exhibits substantial dispersion and a weaker correlation. Its data points form a diffuse cloud with significant vertical spread, meaning that for swaptions with similar market volatilities, the \ac{lm} model can generate widely varying predicted volatilities. This highlights the method's lack of robustness and reduced reliability in out-of-sample prediction.

The \ac{lm} scatter plot visually corroborates the systematic biases observed in the heatmap analysis. A majority of the points lie above the \(y=x\) line, indicating a persistent tendency to overestimate volatility. Conversely, the \ac{nn} errors appear more symmetrically distributed around the perfect fit line, suggesting a more balanced prediction profile with minimal systematic bias.

While the average 3.58~\ac{bps} reduction in out-of-sample RMSE is statistically significant, a complete analysis requires an examination of its economic relevance. The economic significance of this accuracy improvement is amplified by the scale of modern derivatives portfolios and manifests in three critical domains: portfolio valuation, hedging efficiency, and institutional risk management.

First, the most direct monetary impact of a volatility misestimation is transmitted through a portfolio's vega. For institutional-scale trading desks, where aggregate vega is substantial, a systematic pricing error of several basis points can lead to significant daily mark-to-market discrepancies. Such inaccuracies can obscure the true profitability of trading strategies and lead to flawed financial reporting. The superior accuracy of the neural network provides a more reliable basis for daily valuation and profit and loss attribution.

Second, the utility of a calibration extends beyond pricing to the calculation of risk sensitivities for hedging. An inaccurate calibration yields unreliable hedge ratios, leading to hedge slippage and unintended market exposures. Furthermore, the greater parameter stability demonstrated by the neural network translates into more stable hedge ratios over time. This reduces the need for frequent, costly portfolio rebalancing driven by model instability, thereby lowering transaction costs and improving the overall efficiency of the hedging program.

Finally, the improved accuracy has profound implications for firm-wide risk management and regulatory compliance. The parameters derived from calibration are critical inputs for stochastic risk models used to compute measures such as Value-at-Risk and to conduct stress tests. A systematic calibration error distorts the simulated profit and loss distributions, potentially leading to a material underestimation of the firm's true market risk. The observed 3.58 bps difference can be interpreted as a quantifiable measure of model risk, providing crucial information for model validation functions and for the efficient allocation of regulatory capital. In this context, the enhanced accuracy of the neural network approach is not a marginal improvement, but a meaningful contribution to financial stability and operational robustness.

\subsubsection{Parameter Stability}
This section examines the temporal behavior of the calibrated \ac{hw} model parameters-specifically the mean-reversion rate (\(\alpha\)) and the piecewise constant volatility term structure (\(\sigma_i\))-for both the \ac{nn} and \ac{lm} (\ac{lm}) calibration methods. The analysis is based on daily recalibrations (\ac{lm} approach) and the predicted parameters (\ac{nn}) over the test period, with a focus on the stability  of the parameter trajectories. Parameter stability is of central importance, as erratic fluctuations can result in unstable hedging ratios, inconsistent pricing, and unreliable risk metrics.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot3_parameter_evolution.png}
	\caption{Parameter Evolution for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:parameter_evolution}
\end{figure}

At first inspection, both calibration techniques yield parameters that evolve smoothly over time without abrupt jumps or discontinuities. The magnitudes of the parameters appear economically plausible, with no extreme outliers observed. This visual stability suggests that both methods achieve a baseline level of temporal consistency in the estimated \ac{hw} parameters.

A notable divergence arises in the behavior of the mean-reversion parameter. The \ac{lm} method persistently drives \(\alpha\) toward values near zero (mean \(\approx 0.00002\)), with a correspondingly low standard deviation. Although statistically stable, this near-zero value effectively suppresses the mean-reversion mechanism of the model, thereby failing to capture an essential economic feature of interest rate dynamics. In contrast, the \ac{nn} produces a stable and economically meaningful mean-reversion level (mean \(\approx 0.0109\)), reflecting a more realistic adjustment speed of short rates toward their long-term equilibrium.

The volatility term structures (\(\sigma_i\)) estimated by the \ac{nn} demonstrate a high degree of structural coherence. Across time, the piecewise volatilities retain their relative ordering, and the overall shape of the term structure evolves smoothly and predictably. This indicates that the \ac{nn} calibration preserves the internal consistency of the model across maturities and dates. The resulting stability of the \(\sigma_i\) curves suggests that the \ac{nn} effectively captures persistent features of the volatility surface while adapting flexibly to gradual market changes.

Although the \ac{lm} method exhibits slightly lower parameter standard deviations on an absolute scale, this apparent stability does not necessarily imply superior model robustness. The \ac{nn}'s moderately higher parameter variance reflects meaningful responsiveness to evolving market conditions within a coherent and economically interpretable structure. Thus, the \ac{nn}'s stability should be viewed as both statistical and structural-balancing smooth temporal evolution with sensitivity to genuine shifts in the underlying market environment.

\subsubsection{Parameter Sensitivity Analysis}
The sensitivity analysis highlights pronounced differences in the robustness and economic interpretability of the parameters estimated by the \ac{nn} and the traditional \ac{lm} (\ac{lm}) method. Across all examined scenarios, the \ac{nn} exhibits stable and economically consistent adjustments, whereas the \ac{lm} method frequently produces erratic or extreme results, particularly regarding the estimation of the mean-reversion parameter. The following discussion presents a detailed examination of three distinct yield curve perturbations.

\paragraph{Scenario 1: Parallel Yield Curve Shift Up (+50 \ac{bps})}
This scenario reflects a market environment characterized by rising interest rates, such as during monetary tightening or increased inflation expectations. The \ac{nn} responds by reducing the mean-reversion parameter \( a_1 \) by 22.38\%. This behavior is economically intuitive: as interest rates rise, the short rate becomes less strongly attracted to its long-term mean, implying a slower reversion speed. Regarding volatility, the \ac{nn} exhibits a differentiated adjustment across maturities. Short-to-medium term volatilities (\( \sigma_1 \) to \( \sigma_4 \)) increase by 1.46\% to 8.66\%, reflecting greater short-term uncertainty, whereas long-term volatilities (\( \sigma_5 \) to \( \sigma_7 \)) decrease by 3.45\% to 5.47\%. This pattern suggests that the network associates persistent rate hikes with a more predictable long-term environment, consistent with the stabilizing influence of central bank policy.

In contrast, the \ac{lm} method produces unstable and economically implausible results. The mean-reversion parameter \( a_1 \) collapses by 99.997\%, effectively eliminating mean reversion from the model. This outcome does not represent an economic insight but rather a numerical artifact, reflecting the optimizer's convergence to a degenerate local minimum. The volatility response is similarly simplistic: all parameters decrease uniformly by 2.41\% to 5.60\%, indicating that the \ac{lm} optimizer finds a slightly smoother volatility surface that minimizes the fitting error without capturing any meaningful structural information.

\paragraph{Scenario 2: Parallel Yield Curve Shift Down (-50 \ac{bps})}
A parallel downward shift of 50 \ac{bps} captures an environment of monetary easing or a flight-to-quality, where rates decline broadly across maturities. In this context, the \ac{nn} increases the mean-reversion parameter \( a_1 \) by 16.40\%, which is the logical counterpart to the previous scenario. Lower rates induce a stronger pull back toward the long-term mean, consistent with market expectations of normalization. The volatility adjustments are again complex: the short-term volatilities (\( \sigma_1, \sigma_2 \)) decrease, while medium- and long-term volatilities (\( \sigma_3 \) to \( \sigma_7 \)) rise by up to 9.87\%. This asymmetric reaction captures a common market feature, where declining rates heighten duration risk and uncertainty in the longer end of the curve.

The \ac{lm} method fails to produce a meaningful calibration under this scenario. The mean-reversion parameter explodes by 303,251.39\%, indicating a complete loss of numerical stability. This extreme value is economically meaningless and arises from the optimizer's sensitivity to local minima in the parameter space. The volatility adjustments are erratic, lacking any coherent term-structure pattern. While most volatilities increase, the longest-tenor parameters (\( \sigma_6, \sigma_7 \)) decrease sharply by 10.45\% and 17.24\%, respectively. This inconsistency further demonstrates the inability of the \ac{lm} approach to produce economically interpretable parameter dynamics under yield curve perturbations.

\paragraph{Scenario 3: Yield Curve Twist (Steepening)}
The final scenario models a steepening of the yield curve, representing a situation in which long-term rates rise relative to short-term rates due to improved growth or inflation expectations. The \ac{nn} decreases the mean-reversion parameter \( a_1 \) moderately by 11.83\%, reflecting a realistic interpretation that a steeper curve reduces the immediate mean-reverting pressure on the short rate. The network also produces a measured increase in volatility across the term structure, with changes ranging from 0.65\% to 3.67\%. This restrained yet systematic response indicates that the \ac{nn} captures the nuanced relationship between curve shape and rate uncertainty without overfitting to transient fluctuations.

The \ac{lm} method again exhibits numerical instability. The mean-reversion parameter \( a_1 \) collapses entirely, decreasing by 100.00\%, which effectively removes mean reversion from the model. This recurring pattern underscores the optimizer's inability to handle non-parallel yield curve shifts in a stable manner. The volatility parameters uniformly decrease by 2.25\% to 5.08\%, mirroring the behavior observed in the upward shift scenario. Such uniform adjustments suggest that the \ac{lm} method applies a rigid, non-adaptive response that fails to capture the dynamic interactions between curve shape and volatility, in stark contrast to the \ac{nn}'s flexible and economically coherent interpretation.

Overall, these scenario-based analyses demonstrate that the \ac{nn} not only yields stable parameter estimates but also captures economically meaningful dynamics consistent with macro-financial reasoning. The traditional \ac{lm} optimizer, by contrast, repeatedly fails to produce reliable or interpretable results, highlighting the superiority of data-driven calibration in ensuring robustness and stability under varying market conditions.

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\caption{Scenario Analysis of \ac{nn} Model Parameters}
	\label{tab:scenario_analysis_part_nn}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			\toprule
			           & \multicolumn{2}{c}{$a_1$} & \multicolumn{2}{c}{$\sigma_1$} & \multicolumn{2}{c}{$\sigma_2$} & \multicolumn{2}{c}{$\sigma_3$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$               & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                           & 0.01136                        &                                & 0.00021                        &             & 0.00027    &             & 0.00026    \\
			Shift Up   & -22.38                    & 0.00881                        & 1.46                           & 0.00022                        & 2.83        & 0.00028    & 4.43        & 0.00028    \\
			Shift Down & 16.40                     & 0.01322                        & -7.11                          & 0.00020                        & -2.92       & 0.00026    & 6.32        & 0.00028    \\
			Twist      & -11.83                    & 0.01001                        & 0.65                           & 0.00021                        & 1.49        & 0.00027    & 1.20        & 0.00027    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			           & \multicolumn{2}{c}{$\sigma_4$} & \multicolumn{2}{c}{$\sigma_5$} & \multicolumn{2}{c}{$\sigma_6$} & \multicolumn{2}{c}{$\sigma_7$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$                    & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                                & 0.00026                        &                                & 0.00018                        &             & 0.00014    &             & 0.00016    \\
			Shift Up   & 8.66                           & 0.00028                        & -3.45                          & 0.00018                        & -5.47       & 0.00013    & -3.40       & 0.00015    \\
			Shift Down & 2.25                           & 0.00027                        & 9.87                           & 0.00020                        & 4.22        & 0.00014    & 2.25        & 0.00016    \\
			Twist      & 3.67                           & 0.00027                        & -0.48                          & 0.00018                        & -2.17       & 0.00013    & -0.02       & 0.00016    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

% Levenberg-Marquardt
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\caption{Scenario Analysis of \ac{lm} Model Parameters}
	\label{tab:scenario_analysis_part_lm}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			\toprule
			           & \multicolumn{2}{c}{$a_1$} & \multicolumn{2}{c}{$\sigma_1$} & \multicolumn{2}{c}{$\sigma_2$} & \multicolumn{2}{c}{$\sigma_3$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$               & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                           & 0.00009                        &                                & 0.00021                        &             & 0.00021    &             & 0.00021    \\
			Shift Up   & -100.00                   & 0.00000                        & -2.41                          & 0.00020                        & -3.20       & 0.00020    & -3.80       & 0.00020    \\
			Shift Down & 303,251                   & 0.26543                        & -8.52                          & 0.00019                        & -1.85       & 0.00020    & 1.32        & 0.00021    \\
			Twist      & -100.00                   & 0.00000                        & -2.25                          & 0.00020                        & -2.96       & 0.00020    & -3.50       & 0.00020    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{threeparttable}
		\begin{tabular}{l *{4}{rr}}
			           & \multicolumn{2}{c}{$\sigma_4$} & \multicolumn{2}{c}{$\sigma_5$} & \multicolumn{2}{c}{$\sigma_6$} & \multicolumn{2}{c}{$\sigma_7$}                                                       \\
			\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			Scenario   & \% $\delta$                    & Abs. Value                     & \% $\delta$                    & Abs. Value                     & \% $\delta$ & Abs. Value & \% $\delta$ & Abs. Value \\
			\midrule

			Base Case  &                                & 0.00021                        &                                & 0.00022                        &             & 0.00021    &             & 0.00020    \\
			Shift Up   & -3.60                          & 0.00020                        & -5.60                          & 0.00020                        & -2.27       & 0.00020    & -0.51       & 0.00020    \\
			Shift Down & 3.08                           & 0.00022                        & -0.67                          & 0.00021                        & -10.45      & 0.00018    & -17.24      & 0.00017    \\
			Twist      & -3.30                          & 0.00020                        & -5.08                          & 0.00020                        & -2.00       & 0.00020    & -0.36       & 0.00020    \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}


\subsubsection{Analysis of Computational Efficiency and Practical Applicability}
The computational performance of the \ac{nn} and \ac{lm} (\ac{lm}) calibration methods was evaluated to assess their practical suitability for real-world applications. Table~\ref{tab:comp_efficiency} summarizes the measured runtimes and variability for each approach, highlighting the substantial differences in computational efficiency.

\begin{table}[H]
	\centering
	\begin{threeparttable}
		\caption{Computational Efficiency of \ac{nn} and \ac{lm} Calibration Methods}
		\label{tab:comp_efficiency}
		\begin{tabular}{lcc}
			\toprule
			Method  & Mean Time (s) & Std Dev Time (s) \\
			\midrule
			\ac{nn} & 0.0041        & 0.0019           \\
			\ac{lm} & 91.87         & 23.52            \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\footnotesize
			\item \textit{Note:} The table reports average runtime and standard deviation for daily calibrations. The \ac{nn} achieves an approximate speed-up of 22,555x compared to \ac{lm}.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

The \ac{nn} exhibits a dramatic advantage in computational speed for daily calibration tasks. With an average runtime of just 4 milliseconds per calibration, it is approximately 22,555 times faster than the traditional \ac{lm} method, which requires an average of 92 seconds. This multi-order-of-magnitude improvement fundamentally transforms the operational feasibility of high-frequency calibration workflows and is consistent with the findings of \textcite{alaya2021deep}, who demonstrated that their neural network reduced the calibration time of the G2++ model from 480.1 seconds to 0.063 seconds-an acceleration of roughly 7,620 times.

This computational advantage arises from the methodological distinction between the two approaches. The \ac{nn} incurs the majority of its computational cost during an offline training phase, including a one-time hyperparameter tuning (approx. 1 week) and a subsequent model training (approx. 1 hour). Once trained, the \ac{nn} performs calibration via a near-instantaneous forward pass (inference). In contrast, the \ac{lm} algorithm performs a fully iterative numerical optimization online for each new market snapshot, making it substantially slower and more resource-intensive in day-to-day operations.

The initial offline cost of the \ac{nn} is efficiently amortized over its operational lifetime. Optimal hyperparameters remain stable over time, requiring only periodic retraining (e.g., nightly or weekly) in approximately one hour. This allows thousands of real-time calibrations to be executed almost instantaneously, rendering the upfront investment highly cost-effective in a production environment.

The sub-second runtime of the \ac{nn} makes it suitable for applications that are infeasible with the \ac{lm} method, including real-time risk management, pre-trade pricing of extensive derivative portfolios, and intraday recalibration in response to evolving market conditions. The 92-second average runtime of \ac{lm} constitutes a significant bottleneck, restricting its use to end-of-day or batch-processing tasks.

Beyond mean speed, the \ac{nn} demonstrates highly predictable performance with a low standard deviation of execution time (0.0019 s), whereas the \ac{lm} algorithm shows substantial variability (23.52 s) due to sensitivity to market-specific data and optimization paths. This predictability is critical for integration into time-sensitive production workflows, further emphasizing the \ac{nn}'s suitability for operational deployment.

\subsection{Interpretability of the Neural Network via SHAP Values}
The interpretability of the \ac{nn} was examined through \ac{shap} values, which quantify the contribution of each input feature to the model's predictions for the \ac{hw} parameters. This analysis provides insight into the internal decision-making process of the \ac{nn}, allowing for a scientific understanding of how it generates parameter estimates from market data.

The \ac{shap} analysis validates the \ac{nn} as an interpretable model. The feature importance patterns reveal systematic and economically intuitive relationships, demonstrating that the model's predictions are grounded in financially relevant signals rather than being arbitrary or opaque. This dispels the perception of the network as an unexplainable "black box" and confirms that its outputs reflect meaningful market dynamics.

A key finding is the dominant influence of the long-end yield curve shape, represented by the curvature\_10y20y30y feature, across nearly all output parameters, including the mean-reversion parameter (\(a_1\)) and the piecewise volatility terms (\(\sigma_i\)). This indicates that the \ac{nn} has learned that the long-end curvature of the yield curve is critical for determining both the long-term pull of interest rates and the overall volatility term structure. This is consistent with financial intuition, as long-end curvature captures market expectations regarding long-term economic uncertainty and monetary policy.

The model further demonstrates sophistication through parameter-specific feature importance. The mean-reversion parameter (\(a_1\)) is primarily driven by short- and medium-term yield curve slopes (slope\_3m10y, slope\_5y30y), reflecting the theoretical notion that mean-reversion measures the speed at which interest rates revert to their long-term average. In contrast, the long-term volatility parameter (\(\sigma_7\)) is highly sensitive to the \ac{move}\_\ac{vix}\_Ratio, indicating that the network has learned that treasury market-implied volatility is a key determinant of long-horizon rate uncertainty.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/SHAP_summary_sigma_7.png}
	\caption{\ac{shap} Summary Plot for the Long-Term Volatility Parameter \(\sigma_7\).}
	\label{fig:shap_summary_sigma_7}
\end{figure}

Direct interpretation of the \ac{shap} summary plot for \(\sigma_7\) (\ref{fig:shap_summary_sigma_7}) reveals a counter-intuitive financial relationship learned by the model. For the \ac{move}\_\ac{vix}\_Ratio feature, the \ac{nn} has identified an inverse relationship: high values of the ratio (red dots), indicating elevated Treasury volatility relative to equity volatility, lead the model to predict lower long-term volatility (\(\sigma_7\)), whereas low values of the ratio (blue dots) push predictions higher. This pattern may reflect a "flight to quality" effect, where extreme market stress, signaled by a high \ac{move}\_\ac{vix}\_Ratio, is associated with expectations of aggressive central bank interventions aimed at stabilizing long-term rates and suppressing volatility. The network appears to have captured that such stress episodes are often precursors to a calmer, lower-volatility regime over the long term.


This \ac{shap}-based interpretability analysis reinforces confidence in the \ac{nn}'s predictions. It demonstrates that the model captures complex, non-linear relationships consistent with economic theory and market intuition, rather than fitting noise. Consequently, the \ac{nn} emerges not only as a computationally efficient and accurate calibration tool but also as a robust and transparent alternative to traditional \ac{hw} calibration methods.

Note: All other plots regarding the \ac{shap} values can be found at the appendix section.

\subsection{Comparison with \textcite{hernandez2016model}}
\textcite{hernandez2016model} investigated the calibration of a simplified one-factor \ac{hw} model characterized by a single mean-reversion parameter $\alpha$ and a single volatility parameter $\sigma$. His study was based on 156 GBP \ac{atm} swaptions observed between 2013 and 2016. For the estimation task, he employed a feed-forward \ac{nn} with four hidden layers, trained to directly predict the parameters $(\alpha, \sigma)$ of the \ac{hw} model. In contrast, the \ac{nn} developed in the present thesis consists of five layers and utilizes a residual parameterization approach, in which the model learns to predict corrections to an initial guess of the parameters rather than the parameters themselves. This design choice aims to enhance numerical stability and convergence properties, particularly in non-linear calibration problems.

The methodological distinction between both approaches is substantial. \textcite{hernandez2016model} \ac{nn} was trained using pre-calibrated parameter pairs obtained from a \ac{lm} optimization as target variables. The \ac{lm} optimizer represented the traditional calibration benchmark in his study, providing a set of optimal parameters for each observation date. Conversely, the approach presented in this thesis does not rely on such precomputed targets. Instead, the predicted parameters are directly inserted into the QuantLib pricing engine, and the \ac{nn} is trained by minimizing the deviation between the model-implied and market-observed swaption volatilities. Formally, this corresponds to minimizing the loss function
\[
	L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left( \sigma^{\text{model}}_i(\alpha(\theta), \sigma(\theta)) - \sigma^{\text{market}}_i \right)^2,
\]
where $\sigma^{\text{model}}_i$ and $\sigma^{\text{market}}_i$ denote model-implied and market-observed volatilities, respectively, and $\theta$ represents the \ac{nn} parameters (see sections \ref{calibration} and \ref{subsec:loss_function_and_error_metric}). This direct loss formulation allows the network to learn parameter mappings that minimize actual pricing errors rather than reproducing the outcomes of an external optimizer. However, the approach entails higher computational costs because QuantLib's pricing routines are implemented in C++ and do not expose analytical gradients to TensorFlow's automatic differentiation, making backpropagation computationally demanding (see section \ref{subsubsec:algorithmic_and_numerical_efficiency}).

Regarding input representation, \textcite{hernandez2016model} incorporated both yield curve information and swaption data into the \ac{nn}'s feature vector. Specifically, he employed a 6-month tenor \ac{libor} curve discretized at 44 maturity points, ranging from 0 days to 50 years, including tenors of 0, 1, 2, 7, and 14 days; 1--24 months; 3--10 years; and 12, 15, 20, 25, 30, and 50 years. \ac{pca} was subsequently applied to retain 99.5\% of the variance. Additionally, the 156 swaption volatilities formed a major component of the input vector. In contrast, the this thesis did not use swaption volatilities as input features but instead relied on alternative market information to ensure that the model's predictive power stemmed from broader market dynamics rather than direct encoding of the target variable.

Both studies calibrated their respective models to the full swaption volatility surface. To ensure comparability, the results obtained in this thesis, originally expressed in Bachelier (normal) volatilities, were converted to Black (lognormal) volatilities and then weighted by the corresponding vegas using the approach described in section \ref{subsec:conversion_from_normal_to_lognormal_errors}, consistent with the convention used by \textcite{hernandez2016model}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/plot4_daily_rmse_black_vol.png}
	\caption{Average Daily \ac{rmse} in Black Volatilities for \ac{nn} and \ac{lm} Calibration Methods.}
	\label{fig:daily_rmse_black_vol}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{images/calibration_results/hernandez_calibration_results.png}
	\caption{Hernandez's: Average Daily \ac{rmse} in Black Volatilities for \ac{nn} and \ac{lm} Calibration Methods. Source: \parencite[figure~3]{hernandez2016model}}
	\label{fig:daily_rmse_black_vol_herandez}
\end{figure}


The traditional \ac{lm} calibration methods in both studies exhibit comparable average relative errors of approximately 4.5\%. Nevertheless, the results obtained in this thesis indicate slightly lower errors, particularly in periods when \textcite{hernandez2016model} \ac{lm}-based calibration demonstrated performance deterioration, such as from January 2016 onward. This improvement can be attributed to the higher flexibility of the \ac{hw} specification employed here, which allows for piecewise constant volatility parameters $\sigma(t)$ rather than a single constant parameter, thereby enabling a more accurate fit to the observed volatility surface.

The performance difference becomes more pronounced when comparing the \ac{nn}-based calibration results. \textcite{hernandez2016model} \ac{nn} achieved an average calibration error similar to that of his \ac{lm} optimizer, approximately 4.5\%, whereas the model developed in this thesis attained a significantly lower average error of around 2.5\%. Both studies were conducted under relatively stable market conditions, suggesting that the superior performance of the present model is primarily due to the direct optimization of pricing errors rather than the replication of pre-calibrated parameters. By learning the end-to-end mapping from market inputs to pricing-consistent model parameters, the proposed \ac{nn} achieves a better generalization to the true market dynamics underlying the volatility surface.

Beyond this headline improvement, a more granular analysis of the error metric reveals a key aspect of the model's practical performance. It was observed that the vega-weighted portfolio error for the neural network is consistently lower than its simple average error. This indicates that the model's highest accuracy is concentrated on the instruments with the highest vega, which typically correspond to the most liquid, financially significant tenors of the volatility surface. The \ac{nn} has implicitly learned to prioritize the fit where it matters most for risk and P\&L, relegating larger (though still relatively small) errors to the less liquid, lower-vega wings of the surface.

A further difference between the studies lies in the temporal stability of the models. \textcite{hernandez2016model} reported a degradation in performance approximately six to twelve months after training, implying limited temporal generalization. Due to the restricted testing period of only one month in this thesis, no such degradation could be observed, and further analysis over a longer horizon would be required to assess long-term stability.

It is important to note several limitations that constrain the comparability of both studies. First, \textcite{hernandez2016model} employed a simplified version of the \ac{hw} model with only one mean-reversion and one volatility parameter, inherently reducing its flexibility to fit complex swaption surfaces. Second, his \ac{nn} benefited from a substantially larger training dataset, as he synthetically generated approximately 150,000 samples, whereas the present thesis relied exclusively on empirical data, limiting both the temporal coverage and sample size. Furthermore, a crucial distinction lies in the evaluation protocol; this thesis enforced a strict out-of-sample test for both the \ac{nn} and the \ac{lm} optimizer using an intra-day hold-out set, ensuring a direct comparison of their generalization capabilities. In contrast, the benchmark in \textcite{hernandez2016model} work was the in-sample fit of the traditional optimizer, which does not measure predictive performance on unseen data. Consequently, while the lower calibration errors observed in this work indicate improved model fit, they must be interpreted cautiously given the differences in model complexity, data volume, and testing horizon.
